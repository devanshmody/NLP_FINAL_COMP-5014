{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1130532_Textsummarynlpproject_Final_Code.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1AwsyYo2D8msVOnVSbOMz31W9x_jWOMug","authorship_tag":"ABX9TyNsVvwBRYyucIjWFC98CTVf"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qELybmgHs2e1","executionInfo":{"status":"ok","timestamp":1618637420513,"user_tz":-330,"elapsed":7882,"user":{"displayName":"devansh mody","photoUrl":"","userId":"11540078254175805123"}},"outputId":"449857f6-ecab-44f7-8bdf-d0a93dcef70d"},"source":["#step1 import all the required libraries\n","#install this version of transformers and pytorch\n","!pip install transformers==2.8.0\n","!pip install torch==1.4.0\n","from transformers import T5Tokenizer, T5ForConditionalGeneration\n","import tensorflow_datasets as tfds\n","import torch\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow import keras\n","import nltk,spacy,re,string,random,time\n","import matplotlib.pyplot as plt\n","from gensim.parsing.preprocessing import STOPWORDS\n","from spacy.lang.en.stop_words import STOP_WORDS\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from sklearn.model_selection import train_test_split\n","from collections import Counter \n","from keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Input,LSTM,Embedding,Dense,Concatenate,TimeDistributed,Bidirectional\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau\n","from attension import AttentionLayer\n","from keras.initializers import Constant\n","from keras.optimizers import Adam\n","from keras import backend as K \n","from rouge import rouge_n,rouge\n","from bleau import compute_bleu\n","#ignore warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","#stopwords removal list\n","nltk.download('stopwords')\n","#punkt for tokenization\n","nltk.download('punkt')\n","#for tokenaizations\n","nltk.download('wordnet')\n","#combine all the stopwords and create one single list of stopwords\n","s1=stopwords.words('english')\n","s2=list(STOP_WORDS)\n","s3=list(STOPWORDS)\n","#final list of stopwords\n","stop_words = s1+s2+s3\n","#use cuda if available\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","#step2\n","#contraction are used to replace words with their longer meaningfull counter parts \n","contraction = { \n","\"ain't\": \"am not / are not / is not / has not / have not\",\n","\"aren't\": \"are not / am not\",\n","\"can't\": \"cannot\",\n","\"can't've\": \"cannot have\",\n","\"'cause\": \"because\",\n","\"could've\": \"could have\",\n","\"couldn't\": \"could not\",\n","\"couldn't've\": \"could not have\",\n","\"didn't\": \"did not\",\n","\"doesn't\": \"does not\",\n","\"don't\": \"do not\",\n","\"hadn't\": \"had not\",\n","\"hadn't've\": \"had not have\",\n","\"hasn't\": \"has not\",\n","\"haven't\": \"have not\",\n","\"he'd\": \"he had / he would\",\n","\"he'd've\": \"he would have\",\n","\"he'll\": \"he shall / he will\",\n","\"he'll've\": \"he shall have / he will have\",\n","\"he's\": \"he has / he is\",\n","\"how'd\": \"how did\",\n","\"how'd'y\": \"how do you\",\n","\"how'll\": \"how will\",\n","\"how's\": \"how has / how is / how does\",\n","\"I'd\": \"I had / I would\",\n","\"I'd've\": \"I would have\",\n","\"I'll\": \"I shall / I will\",\n","\"I'll've\": \"I shall have / I will have\",\n","\"I'm\": \"I am\",\n","\"I've\": \"I have\",\n","\"isn't\": \"is not\",\n","\"it'd\": \"it had / it would\",\n","\"it'd've\": \"it would have\",\n","\"it'll\": \"it shall / it will\",\n","\"it'll've\": \"it shall have / it will have\",\n","\"it's\": \"it has / it is\",\n","\"let's\": \"let us\",\n","\"ma'am\": \"madam\",\n","\"mayn't\": \"may not\",\n","\"might've\": \"might have\",\n","\"mightn't\": \"might not\",\n","\"mightn't've\": \"might not have\",\n","\"must've\": \"must have\",\n","\"mustn't\": \"must not\",\n","\"mustn't've\": \"must not have\",\n","\"needn't\": \"need not\",\n","\"needn't've\": \"need not have\",\n","\"o'clock\": \"of the clock\",\n","\"oughtn't\": \"ought not\",\n","\"oughtn't've\": \"ought not have\",\n","\"shan't\": \"shall not\",\n","\"sha'n't\": \"shall not\",\n","\"shan't've\": \"shall not have\",\n","\"she'd\": \"she had / she would\",\n","\"she'd've\": \"she would have\",\n","\"she'll\": \"she shall / she will\",\n","\"she'll've\": \"she shall have / she will have\",\n","\"she's\": \"she has / she is\",\n","\"should've\": \"should have\",\n","\"shouldn't\": \"should not\",\n","\"shouldn't've\": \"should not have\",\n","\"so've\": \"so have\",\n","\"so's\": \"so as / so is\",\n","\"that'd\": \"that would / that had\",\n","\"that'd've\": \"that would have\",\n","\"that's\": \"that has / that is\",\n","\"there'd\": \"there had / there would\",\n","\"there'd've\": \"there would have\",\n","\"there's\": \"there has / there is\",\n","\"they'd\": \"they had / they would\",\n","\"they'd've\": \"they would have\",\n","\"they'll\": \"they shall / they will\",\n","\"they'll've\": \"they shall have / they will have\",\n","\"they're\": \"they are\",\n","\"they've\": \"they have\",\n","\"to've\": \"to have\",\n","\"wasn't\": \"was not\",\n","\"we'd\": \"we had / we would\",\n","\"we'd've\": \"we would have\",\n","\"we'll\": \"we will\",\n","\"we'll've\": \"we will have\",\n","\"we're\": \"we are\",\n","\"we've\": \"we have\",\n","\"weren't\": \"were not\",\n","\"what'll\": \"what shall / what will\",\n","\"what'll've\": \"what shall have / what will have\",\n","\"what're\": \"what are\",\n","\"what's\": \"what has / what is\",\n","\"what've\": \"what have\",\n","\"when's\": \"when has / when is\",\n","\"when've\": \"when have\",\n","\"where'd\": \"where did\",\n","\"where's\": \"where has / where is\",\n","\"where've\": \"where have\",\n","\"who'll\": \"who shall / who will\",\n","\"who'll've\": \"who shall have / who will have\",\n","\"who's\": \"who has / who is\",\n","\"who've\": \"who have\",\n","\"why's\": \"why has / why is\",\n","\"why've\": \"why have\",\n","\"will've\": \"will have\",\n","\"won't\": \"will not\",\n","\"won't've\": \"will not have\",\n","\"would've\": \"would have\",\n","\"wouldn't\": \"would not\",\n","\"wouldn't've\": \"would not have\",\n","\"y'all\": \"you all\",\n","\"y'all'd\": \"you all would\",\n","\"y'all'd've\": \"you all would have\",\n","\"y'all're\": \"you all are\",\n","\"y'all've\": \"you all have\",\n","\"you'd\": \"you had / you would\",\n","\"you'd've\": \"you would have\",\n","\"you'll\": \"you shall / you will\",\n","\"you'll've\": \"you shall have / you will have\",\n","\"you're\": \"you are\",\n","\"you've\": \"you have\",\n","\"rec'd\": \"received\"\n","}\n","#rec'd this is my addition to the list of contractions\n","\n","#step3\n","#process_text function is used to remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings\n","def process_text(text,contractions,remove_stopwords = True):\n","  #convert words to lower case\n","  text = text.lower()\n","    \n","  #replace contractions with their longer forms \n","  if True:\n","    text = text.split()\n","    new_text = []\n","    for word in text:\n","      if word in contractions:\n","        new_text.append(contractions[word])\n","      else:\n","        new_text.append(word)\n","    text = \" \".join(new_text)\n","    \n","  #format words and remove unwanted characters\n","  text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE) #remove https string\n","  text = re.sub(r'\\<a href', ' ', text) #remove hyperlink\n","  text = re.sub(r'&amp;', '', text) #remove & in text\n","  text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text) #remove unwanted charecters like puntuation and others\n","  text = re.sub(r'<br />', ' ', text) #remove new line spaces\n","  text = re.sub(r'\\'', ' ', text) #remove slashes\n","  text = \" \".join(text.split()) #remove trailing spaces \n","  #string.printable returns all sets of punctuation, digits, ascii_letters and whitespace.\n","  printable = set(string.printable)\n","  #filter to remove punctuations,digits, ascii_letters and whitespaces\n","  text = \"\".join(list(filter(lambda x: x in printable, text))) \n","  #remove stop words is true then remove stopwords also\n","  if remove_stopwords:\n","    text = text.split()\n","    text = [w for w in text if not w in stop_words]\n","    text = \" \".join(text)\n","\n","  return text\n","\n","#step4\n","#get_data function gets the data from gz file into a dataframe and process the columns\n","#stops are not removed for summary they are only removed from text this is done to get more human like summaries\n","#after processing it returns a dataframe\n","def get_data(contractions):\n","  st=time.time()\n","  #load the data into a dataframe\n","  df = pd.read_json('/content/drive/MyDrive/reviews_Clothing_Shoes_and_Jewelry_5.json.gz', lines=True, compression='gzip')\n","  #drop unwanted columns\n","  df.drop(columns=['reviewerID', 'asin', 'reviewerName', 'helpful','overall','unixReviewTime', 'reviewTime'],inplace=True)\n","  print(\"length of the data\",len(df))\n","  #apply preprocess function on the columns of the dataframe\n","  df['reviewText'] = df['reviewText'].apply(lambda x: process_text(x,contractions,remove_stopwords = True))\n","  df['summary'] = df[ 'summary'].apply(lambda x: process_text(x,contractions,remove_stopwords = False))\n","  #write preprocesssed data to csv file\n","  df.to_csv(\"/content/drive/MyDrive/product_reviews.csv\",index=False)\n","  print(\"total time to generate data and write in csv file \",time.time()-st)\n","\n","\n","#step5\n","#get_embeddings function is used to gett te word embeddings \n","#i am using conceptual number batch word embeddings\n","def get_embeddings():\n","  #get word embeddings\n","  embeddings_index = {}\n","  with open('/content/drive/MyDrive/numberbatch-en-19.08.txt', encoding='utf-8') as f:\n","    for line in f:\n","      values = line.split(' ')\n","      word = values[0]\n","      embedding = np.asarray(values[1:], dtype='float32')\n","      embeddings_index[word] = embedding\n","\n","  print('Word embeddings:', len(embeddings_index))\n","  return embeddings_index\n","\n","#step6\n","#this function is used to build vocabulary\n","def get_vocab(embeddings_index,word_counts,threshold):\n","  #get the number of missing words \n","  missing_words={k:v for k,v in word_counts.items() if v >= threshold if k not in embeddings_index.keys()}\n","  missing_ratio = round(len(missing_words)/len(word_counts),4)*100\n","  print(\"Number of words missing from word_embeddings:\", len(missing_words))\n","  print(\"Percent of words that are missing from our vocabulary: {}%\".format(missing_ratio))\n","\n","  #mapping vocab to index\n","  lr=iter([item for item in range(0,len(word_counts))])\n","  vocab_to_int={k:next(lr) for k,v in word_counts.items() if v >= threshold or k in embeddings_index.keys()}\n","\n","  #mapping index to vocab \n","  lr=iter([item for item in range(0,len(word_counts))])\n","  int_to_vocab={next(lr):k for k,v in word_counts.items() if v >= threshold or k in embeddings_index.keys()}\n","\n","  # Special tokens that will be added to our vocab\n","  codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]\n","\n","  # Add codes to vocab\n","  for code in codes:\n","      vocab_to_int[code] = len(vocab_to_int)\n","      int_to_vocab[len(int_to_vocab)] = code\n","  \n","  #print usage of words in our model and their percent\n","  usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n","  print(\"Total number of unique words:\", len(word_counts))\n","  print(\"Number of words we will use:\", len(vocab_to_int))\n","  print(\"Percent of words we will use: {}%\".format(usage_ratio))\n","  print(\"length vocab_to_int\",len(vocab_to_int))\n","  print(\"length int_to_vocab\",len(int_to_vocab))\n","\n","  return vocab_to_int,int_to_vocab\n","\n","#step7\n","#function to map words with its word embeddings \n","#if embeddings not found for the word then map it with a random number in range(-1.0,1.0)\n","def word_embedding_index(vocab_to_int,embeddings_index):\n","  #using 300 for embedding dimensions to match CN's vectors.\n","  embedding_dim = 300\n","  nb_words = len(vocab_to_int)\n","  \n","  # Create matrix with default values of zero\n","  word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n","  for word, i in vocab_to_int.items():\n","    if word in embeddings_index:\n","      word_embedding_matrix[i] = embeddings_index[word]\n","    else:\n","      # If word not in CN, create a random embedding for it\n","      new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n","      #embeddings_index[word] = new_embedding\n","      word_embedding_matrix[i] = new_embedding\n","\n","  # Check if value matches len(vocab_to_int)\n","  print(\"length of word embedding matrix\",len(word_embedding_matrix))\n","  return word_embedding_matrix\n","\n","#step8\n","#append unk and eos tokens \n","#if eos is equal to true then append go and eos token at begining and end of the summary\n","#add unknown token for word not found in vocabulary\n","def convert_to_ints(text,vocab_to_int,eos=False):\n","  ints = []\n","  for word in text.split():\n","    if word in vocab_to_int:\n","      ints.append(vocab_to_int[word])\n","    else:\n","      ints.append(vocab_to_int[\"<UNK>\"])\n","  if eos:\n","    ints.insert(0,vocab_to_int[\"<GO>\"])\n","    ints.insert(len(ints),vocab_to_int[\"<EOS>\"])\n","  return ints\n","\n","#step9\n","#count unknown tokens\n","def count_unk(text):\n","  unk=0\n","  eos=0\n","  #print(text)\n","  for value in text:\n","    if 41413 in value:\n","      unk+=1\n","  return unk\n","\n","#step10\n","def counts(val):\n","  c=0\n","  for i in val:\n","    try:\n","      if i==41413:\n","        c+=1\n","    except:\n","      pass\n","  return c\n","\n","#step11\n","#remove rows from data frame that dosent staisfy the condition this is done so model is trained with proper data\n","#redundancey is less and input text is accurate\n","def get_refined_output(df,max_rl,max_sl):\n","  unk_rl=1 #unknown token review limit\n","  unk_sl=0 #unknown token summary limit\n","  min_rl=2 #minimum review length\n","  #get the total length of reviewText this is used for sorting\n","  df[\"total_length\"]=df['reviewText'].apply(lambda x: len(x))\n","  #get reviewText whose length is greater then minimum review length \n","  df=df[df['reviewText'].apply(lambda x: len(x)>=min_rl)]\n","  #get reviewText whose length is less than maximum review length\n","  df=df[df['reviewText'].apply(lambda x: len(x)<=max_rl)]\n","  #filter out the unknwon tokens based on unknown token reviewText limit\n","  df=df[df['reviewText'].apply(lambda x: counts(x)<=unk_rl)]\n","  #get summary whose length is less than maximum summary length\n","  df=df[df['summary'].apply(lambda x: len(x)<=max_sl)]\n","  #filter out the unkown tokens based on unkown token summary limit\n","  df=df[df['summary'].apply(lambda x: counts(x)<=unk_sl)]  \n","  #sort the values in ascending order\n","  df.sort_values(by=[\"total_length\"],ascending=True,inplace=True)\n","  #drop unwanted columns\n","  df.drop(columns=[\"total_length\",\"word\"],inplace=True)\n","  #reset index\n","  df.reset_index(drop=True,inplace=True)\n","  return df \n","\n","#step12\n","#function to plot the length of training, validation and testing\n","def plot_tr_tval_tt_len(xtr,xval,xtt):\n","  names = ['Training','Validation','Testing']\n","  values = [len(xtr),len(xval),len(xtt)]\n","  plt.figure(figsize=(10,5))\n","  plt.subplot(131)\n","  plt.bar(names,values,color=['darkorange','coral','coral'],edgecolor='darkblue')\n","  plt.suptitle('Categorical Plotting')\n","  plt.show()\n","\n","#step13\n","#function to plot loss and accuracy curves on training and validation set\n","def plotgraph(history):\n","  plt.figure(figsize=[8,6])\n","  plt.plot(history.history['loss'],'firebrick',linewidth=3.0)\n","  plt.plot(history.history['accuracy'],'turquoise',linewidth=3.0)\n","  plt.plot(history.history['val_loss'],'midnightblue',linewidth=3.0)\n","  plt.legend(['Training loss','Training Accuracy','Validation loss'],fontsize=18)\n","  plt.xlabel('Epochs',fontsize=16)\n","  plt.ylabel('Loss and Accuracy',fontsize=16)\n","  plt.title('Loss Curves and Accuracy Curves for text summarization',fontsize=16)\n","\n","#step14\n","#this function is used to get the preprocessed csv file for our text summarizer\n","def Get_the_data():\n","  #lower the string in contractions and convert it into dict\n","  contractions = dict((k.lower(), v.lower()) for k, v in contraction.items())\n","  #till this step all data is processed and we get our csv file of cleaned texts\n","  get_data(contractions)\n","\n","  #free memory\n","  del contractions\n","\n","#step15 is used to call function Get_the_data which get the preprocessed data and writes it into a csv file\n","#Get_the_data()\n","\n","#step16\n","#this function combines all the above ouput generated by the above function in a proper squence of steps \n","def combining_all_steps():\n","  \n","  st=time.time()\n","  #get the final cleaned data\n","  df=pd.read_csv('/content/drive/MyDrive/product_reviews.csv')[:180000]\n","  print(\"The length of dataset is \",len(df))\n","  #combine reviewText and summary so common vocabulary can be created by finding frequent words \n","  df[\"word\"]=df[['reviewText','summary']].apply(lambda x : '{} {}'.format(x[0],x[1]), axis=1)\n","  #get frequency of words\n","  word_counts=pd.Series(np.concatenate([x.split() for x in df.word])).value_counts()\n","  word_counts=word_counts.to_dict()\n","  #print(type(word_counts))\n","  print(\"vocab length\",len(word_counts))\n","  #set the threshold \n","  threshold = 20\n","  max_rl=80 #maximum review length\n","  max_sl=10 #maximum summary length\n","  #get the embeddings matrix \n","  embeddings_index= get_embeddings()\n","  #get vocab to index and index to vocab mapping of words\n","  vocab_to_int,int_to_vocab=get_vocab(embeddings_index,word_counts,threshold)\n","  #get word embedding for the words in vocab\n","  word_embedding_matrix=word_embedding_index(vocab_to_int,embeddings_index)\n","  #convert words to integers based on their index positions\n","  df['reviewText'] = df['reviewText'].apply(lambda x: convert_to_ints(str(x),vocab_to_int,eos=False))\n","  df['summary'] = df[ 'summary'].apply(lambda x: convert_to_ints(str(x),vocab_to_int,eos=True))\n","  print(\"after word to index for reviewText\",df[\"reviewText\"][0])\n","  print(\"after word to index for summary\",df[\"summary\"][0])\n","  rvunk=count_unk(df[\"reviewText\"])\n","  smunk=count_unk(df[\"summary\"])\n","  print(\"total number of unk token are\",rvunk+smunk)\n","  #apply the filters and get the final preprocessed data\n","  df=get_refined_output(df,max_rl,max_sl)\n","  print(\"length of dataset that will be used\",len(df))\n","  #split data into 75% train, 15% validation and 15% test datasets\n","  x_tr,x_val,y_tr,y_val=train_test_split(df['reviewText'],df['summary'],test_size=0.3,random_state=1,shuffle=True)\n","  x_tt,x_val,y_tt,y_val=train_test_split(x_val,y_val,test_size=0.5,random_state=1,shuffle=True)\n","  print(\"length of split datasets train {}, test {} and validation {}\".format(len(x_tr),len(x_tt),len(x_val)))\n","  print(\"Vocabulary Size: {}\".format(len(vocab_to_int)))\n","  print(\"voc_to_int_\",vocab_to_int['<UNK>'],vocab_to_int['<PAD>'],vocab_to_int['<EOS>'])\n","  #reset index\n","  x_tr=x_tr.reset_index()\n","  y_tr=y_tr.reset_index()\n","  x_tt=x_tt.reset_index()\n","  y_tt=y_tt.reset_index()\n","  x_val=x_val.reset_index()\n","  y_val=y_val.reset_index()\n","  #find max lenght just to verfiy the output of get refined function\n","  #max([len(sentence) for sentence in y_tt[\"summary\"]])\n","  #pad the reviewText and summary to the specified max length\n","  xtr=pad_sequences(x_tr[\"reviewText\"], padding='post',maxlen=max_rl, value=vocab_to_int[\"<PAD>\"])\n","  ytr=pad_sequences(y_tr[\"summary\"], padding='post',maxlen=max_sl, value=vocab_to_int[\"<PAD>\"])\n","  xtt=pad_sequences(x_tt[\"reviewText\"], padding='post',maxlen=max_rl, value=vocab_to_int[\"<PAD>\"])\n","  ytt=pad_sequences(y_tt[\"summary\"], padding='post',maxlen=max_sl, value=vocab_to_int[\"<PAD>\"])\n","  xval=pad_sequences(x_val[\"reviewText\"], padding='post',maxlen=max_rl, value=vocab_to_int[\"<PAD>\"])\n","  yval=pad_sequences(y_val[\"summary\"], padding='post',maxlen=max_sl, value=vocab_to_int[\"<PAD>\"])\n","  #find the number of unique tokens in the list\n","  #flat_list_rt = [item for sublist in df[\"reviewText\"] for item in sublist]\n","  #flat_list_s = [item for sublist in df[\"summary\"] for item in sublist]\n","  #rt=len(np.unique(flat_list_rt))\n","  #st=len(np.unique(flat_list_s))\n","  #print(\"number of unique tokens reviewText {} and summary {}\".format(rt,st))\n","  #plot the length of training, validation and testing\n","  plot_tr_tval_tt_len(xtr,xval,xtt)\n","  print(\"total time to complete all the above steps and get final data \",time.time()-st)\n","  #free memory delete values stored in variables which are not required further\n","  del df,word_counts,embeddings_index,x_tr,x_val,y_tr,y_val,x_tt,y_tt\n","\n","  return xtr,ytr,xtt,ytt,xval,yval,vocab_to_int,int_to_vocab,word_embedding_matrix,max_rl,max_sl  \n","\n","#step17\n","#function to get summary given a sequence\n","def seq_to_summary(seq,vocab_to_int,int_to_vocab):\n","  newstring=''\n","  for i in seq:\n","    if ((i!=0 and i!=vocab_to_int['<GO>']) and i!=vocab_to_int['<EOS>']):\n","      newstring=newstring+int_to_vocab[i]+' '\n","  return newstring\n","  \n","#step18\n","#function to get text given a sequence\n","def seq_to_text(seq,int_to_vocab):\n","  newstring=''\n","  for i in seq:\n","    if (i!=0):\n","      newstring=newstring+int_to_vocab[i]+' '\n","  return newstring\n","\n","#step19\n","#this function get the data for the pretrained model t5small\n","def combining_all_steps_t5():\n","  #get the final cleaned data\n","  df=pd.read_csv('/content/drive/MyDrive/product_reviews.csv')[:147799]\n","  print(\"The length of dataset is \",len(df))\n","  \n","  #set the threshold \n","  threshold = 20\n","  max_rl=80 #maximum review length\n","  max_sl=10 #maximum summary length\n","  \n","  #get reviewText whose length is less than maximum review length\n","  df['reviewText']=df['reviewText'].str.slice(0,max_rl)\n","  \n","  #get summary whose length is less than maximum summary length\n","  df['summary']=df['summary'].str.slice(0,max_rl)\n","\n","  #split data into 75% train, 15% validation and 15% test datasets\n","  x_tr,x_val,y_tr,y_val=train_test_split(df['reviewText'],df['summary'],test_size=0.3,random_state=1,shuffle=True)\n","  x_tt,x_val,y_tt,y_val=train_test_split(x_val,y_val,test_size=0.5,random_state=1,shuffle=True)\n","\n","  #reset index\n","  x_tr=x_tr.reset_index()\n","  y_tr=y_tr.reset_index()\n","  x_tt=x_tt.reset_index()\n","  y_tt=y_tt.reset_index()\n","  x_val=x_val.reset_index()\n","  y_val=y_val.reset_index()\n","  print(\"train {}, val {}, test {}\".format(len(x_tr),len(x_val),len(x_tt)))\n","  return x_tr,y_tr,x_tt,y_tt,x_val,y_val"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers==2.8.0 in /usr/local/lib/python3.7/dist-packages (2.8.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (2.23.0)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (0.1.95)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (3.0.12)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (0.0.44)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (1.17.53)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (1.19.5)\n","Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (0.5.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (4.41.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (3.0.4)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (1.15.0)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from boto3->transformers==2.8.0) (0.3.7)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->transformers==2.8.0) (0.10.0)\n","Requirement already satisfied: botocore<1.21.0,>=1.20.53 in /usr/local/lib/python3.7/dist-packages (from boto3->transformers==2.8.0) (1.20.53)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.53->boto3->transformers==2.8.0) (2.8.1)\n","Requirement already satisfied: torch==1.4.0 in /usr/local/lib/python3.7/dist-packages (1.4.0)\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Z8TyGtX0STcy"},"source":["#step20\n","#function to design and evaluate the model\n","def design_model_fit_eval(xtr,ytr,xval,yval,vocab_to_int,word_embedding_matrix,max_rl):\n","  K.clear_session() \n","  latent_dim = 80\n","  embedding_dim=300\n","  \n","  # Encoder\n","  encoder_inputs = Input(shape=(max_rl,))\n","\n","  #embedding layer\n","  enc_emb =  Embedding(len(vocab_to_int),\n","                        embedding_dim,\n","                        embeddings_initializer=Constant(word_embedding_matrix),\n","                        trainable=False)(encoder_inputs)\n","\n","  \n","  #LSTM 1 \n","  encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True) \n","  encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb) \n","\n","  #LSTM 2 \n","  encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True) \n","  encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1) \n","\n","  #LSTM 3 \n","  encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True) \n","  encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2) \n","\n","  # Set up the decoder, using `encoder_states` as initial state.\n","  decoder_inputs = Input(shape=(None,))\n","\n","  #embedding layer\n","  dec_emb_layer = Embedding(len(vocab_to_int),\n","                            embedding_dim,\n","                            embeddings_initializer=Constant(word_embedding_matrix),\n","                            trainable=False)\n","\n","  #decoder\n","  dec_emb = dec_emb_layer(decoder_inputs)\n","\n","  decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\n","  decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n","\n","  # Attention layer\n","  attn_layer = AttentionLayer(name='attention_layer')\n","  attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n","\n","  # Concat attention input and decoder LSTM output\n","  decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n","\n","  #dense layer\n","  decoder_dense =  TimeDistributed(Dense(len(vocab_to_int), activation='softmax'))\n","  decoder_outputs = decoder_dense(decoder_concat_input)\n","\n","  # Define the model \n","  model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","  \n","  #print model summary\n","  model.summary()\n","\n","  model.compile(optimizer='rmsprop',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n","  #reduce_lr method is used to reduce the learning rate if the learning rate is stagnant or if there are no major improvements in training\n","  reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n","                                patience=5, min_lr=0.001)\n","  #early stopping condition\n","  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n","  \n","  st=time.time()\n","  \n","  #fit te model\n","  history=model.fit([xtr,ytr[:,:-1]], ytr.reshape(ytr.shape[0],ytr.shape[1], 1)[:,1:] ,epochs=100,callbacks=[es],batch_size=512, validation_data=([xval,yval[:,:-1]], yval.reshape(yval.shape[0],yval.shape[1], 1)[:,1:]))\n","                  \n","  #plot loss and accuracy curves\n","  plotgraph(history)\n","  print(\"total time required for training \",time.time()-st)\n","  return encoder_inputs,encoder_outputs, state_h, state_c,decoder_inputs,decoder_lstm,attn_layer,decoder_dense,dec_emb_layer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sYn29_aFd2oG"},"source":["#step21\n","#design of inference function\n","def design_inference(encoder_inputs,encoder_outputs, state_h, state_c,decoder_inputs,decoder_lstm,attn_layer,decoder_dense,max_rl,dec_emb_layer):\n","  #latent dimension\n","  latent_dim = 80\n","  \n","  #encode the input sequence to get the feature vector\n","  encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n","\n","  #decoder setup\n","  #below tensors will hold the states of the previous time step\n","  decoder_state_input_h = Input(shape=(latent_dim,))\n","  decoder_state_input_c = Input(shape=(latent_dim,))\n","  decoder_hidden_state_input = Input(shape=(max_rl,latent_dim))\n","\n","  #get the embeddings of the decoder sequence\n","  dec_emb2= dec_emb_layer(decoder_inputs) \n","  #to predict the next word in the sequence, set the initial states to the states from the previous time step\n","  decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n","\n","  #attention inference\n","  attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n","  decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n","\n","  #a dense softmax layer to generate prob dist. over the target vocabulary\n","  decoder_outputs2 = decoder_dense(decoder_inf_concat) \n","\n","  #final decoder model\n","  decoder_model = Model([decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n","                        [decoder_outputs2] + [state_h2, state_c2])\n","  \n","  return encoder_model,decoder_model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AtygGatZlCWO"},"source":["#step22\n","#fucntion to get the decoded squence for the given review \n","def decode_sequence(input_seq,encoder_model,decoder_model,vocab_to_int,int_to_vocab,max_sl):\n","  # Encode the input as state vectors.\n","  e_out, e_h, e_c = encoder_model.predict(input_seq)\n","    \n","  # Generate empty target sequence of length 1.\n","  target_seq = np.zeros((1,1))\n","    \n","  # Populate the first word of target sequence with the start word.\n","  target_seq[0, 0] = vocab_to_int['<GO>']\n","\n","  stop_condition = False\n","  decoded_sentence = ''\n","  while not stop_condition:\n","    output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n","\n","    # Sample a token\n","    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","    sampled_token = int_to_vocab[sampled_token_index]\n","        \n","    if (sampled_token!=\"<EOS>\"):\n","      decoded_sentence += ' '+sampled_token\n","      \n","      # Exit condition: either hit max length or find stop word.\n","      if (sampled_token == '<EOS>'  or len(decoded_sentence.split()) >= (max_sl-1)):\n","        stop_condition = True\n","\n","    # Update the target sequence (of length 1).\n","    target_seq = np.zeros((1,1))\n","    target_seq[0, 0] = sampled_token_index\n","\n","    # Update internal states\n","    e_h, e_c = h, c\n","\n","  return decoded_sentence\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qEiX6WRESQcn"},"source":["#step23\n","#this function is used to get the score for LSTM scratch model designed and puts output in a txt file\n","def test_scratch(xtt,ytt,int_to_vocab,vocab_to_int,encoder_model,decoder_model,max_sl,max_rl):\n","  st=time.time()\n","  predictions = []\n","  real_og=[]\n","  pred_op=[]\n","  c=0\n","  b=50\n","  for i in range(0,len(xtt)):\n","    #review\n","    review=seq_to_text(xtt[i],int_to_vocab)\n","    review=review.replace(\"<PAD>\",'')\n","    #original summary   \n","    og_summary=seq_to_summary(ytt[i],vocab_to_int,int_to_vocab)\n","    og_summary=og_summary.replace(\"<PAD>\",'')\n","    real_og.append(str(og_summary))\n","    #predicted summary   \n","    predict_summary=decode_sequence(xtt[i].reshape(1,max_rl),encoder_model,decoder_model,vocab_to_int,int_to_vocab,max_sl)\n","    predict_summary=predict_summary.replace(\"<PAD>\",'')\n","    pred_op.append(str(predict_summary))\n","    #write to a text file name review_og_pred.txt\n","    predictions.append(\"review:\"+review+\"\\t\"+\"orignal:\"+og_summary+\"\\t\"+\"predicted:\"+predict_summary+\"\\n\")\n","    #this part is used to print output if the size of c is greater than b \n","    #limited output is print as only 5000 lines can be printed in colab whole output is written in a text file \n","    if c>b:\n","      print(\"Review: {}\".format(review))\n","      print(\"Original Summary: {}\".format(og_summary))\n","      print(\"Predicted Summary: {}\".format(predict_summary))\n","      b+=b\n","    c+=1\n","\n","  print(\"total time to complete {}\".format(time.time()-st))\n","  file = open(\"/content/drive/MyDrive/LSTMscore.txt\",\"w\")\n","  file.writelines(predictions)\n","  file.close()\n","\n","  bleau=compute_bleu(real_og,pred_op, max_order=4,smooth=False)\n","  bscore=nltk.translate.bleu_score.corpus_bleu(real_og,pred_op)\n","  rougen=rouge_n(pred_op, real_og, n=2)\n","  ro=rouge(pred_op, real_og)\n","\n","  print(\"bleu, precisions, bp, ratio, translation_length, reference_length\",bleau)\n","  print(\"bleau score\",bscore)\n","  print(\"rouge2\",rougen)\n","  print(\"rouge\",ro)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lkWUesN6ofHw"},"source":["#step24\n","def lstmmodel():\n","  #this the model designed by me for text summarization \n","  st=time.time()\n","  #get the data\n","  xtr,ytr,xtt,ytt,xval,yval,vocab_to_int,int_to_vocab,word_embedding_matrix,max_rl,max_sl=combining_all_steps()\n","  #call the model\n","  encoder_inputs,encoder_outputs, state_h, state_c,decoder_inputs,decoder_lstm,attn_layer,decoder_dense,dec_emb_layer=design_model_fit_eval(xtr,ytr,xval,yval,vocab_to_int,word_embedding_matrix,max_rl)\n","  #get the inference output\n","  encoder_model,decoder_model=design_inference(encoder_inputs,encoder_outputs, state_h, state_c,decoder_inputs,decoder_lstm,attn_layer,decoder_dense,max_rl,dec_emb_layer)\n","  #call test\n","  test_scratch(xtt,ytt,int_to_vocab,vocab_to_int,encoder_model,decoder_model,max_sl,max_rl)\n","  print(\"total time required for completing whole process \",time.time()-st)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"4VQixJflQJIa","executionInfo":{"status":"ok","timestamp":1618636087994,"user_tz":-330,"elapsed":45693634,"user":{"displayName":"devansh mody","photoUrl":"","userId":"11540078254175805123"}},"outputId":"395e9088-0a2a-4b5a-8b38-c80bde4360e4"},"source":["lstmmodel()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The length of dataset is  180000\n","vocab length 68861\n","Word embeddings: 516783\n","Number of words missing from word_embeddings: 728\n","Percent of words that are missing from our vocabulary: 1.06%\n","Total number of unique words: 68861\n","Number of words we will use: 37429\n","Percent of words we will use: 54.35%\n","length vocab_to_int 37429\n","length int_to_vocab 37429\n","length of word embedding matrix 37429\n","after word to index for reviewText [0, 3920, 0, 17, 12, 119, 278, 209, 79, 905, 3920, 1532]\n","after word to index for summary [37428, 0, 3920, 70, 1154, 565, 37427]\n","total number of unk token are 0\n","length of dataset that will be used 162996\n","length of split datasets train 114097, test 24449 and validation 24450\n","Vocabulary Size: 37429\n","voc_to_int_ 37425 37426 37427\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 720x360 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["total time to complete all the above steps and get final data  55.95032453536987\n","Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 80)]         0                                            \n","__________________________________________________________________________________________________\n","embedding (Embedding)           (None, 80, 300)      11228700    input_1[0][0]                    \n","__________________________________________________________________________________________________\n","lstm (LSTM)                     [(None, 80, 80), (No 121920      embedding[0][0]                  \n","__________________________________________________________________________________________________\n","input_2 (InputLayer)            [(None, None)]       0                                            \n","__________________________________________________________________________________________________\n","lstm_1 (LSTM)                   [(None, 80, 80), (No 51520       lstm[0][0]                       \n","__________________________________________________________________________________________________\n","embedding_1 (Embedding)         (None, None, 300)    11228700    input_2[0][0]                    \n","__________________________________________________________________________________________________\n","lstm_2 (LSTM)                   [(None, 80, 80), (No 51520       lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","lstm_3 (LSTM)                   [(None, None, 80), ( 121920      embedding_1[0][0]                \n","                                                                 lstm_2[0][1]                     \n","                                                                 lstm_2[0][2]                     \n","__________________________________________________________________________________________________\n","attention_layer (AttentionLayer ((None, None, 80), ( 12880       lstm_2[0][0]                     \n","                                                                 lstm_3[0][0]                     \n","__________________________________________________________________________________________________\n","concat_layer (Concatenate)      (None, None, 160)    0           lstm_3[0][0]                     \n","                                                                 attention_layer[0][0]            \n","__________________________________________________________________________________________________\n","time_distributed (TimeDistribut (None, None, 37429)  6026069     concat_layer[0][0]               \n","==================================================================================================\n","Total params: 28,843,229\n","Trainable params: 6,385,829\n","Non-trainable params: 22,457,400\n","__________________________________________________________________________________________________\n","Epoch 1/100\n","223/223 [==============================] - 1248s 6s/step - loss: 4.1427 - accuracy: 0.5584 - val_loss: 2.4180 - val_accuracy: 0.6474\n","Epoch 2/100\n","223/223 [==============================] - 1244s 6s/step - loss: 2.3772 - accuracy: 0.6480 - val_loss: 2.2641 - val_accuracy: 0.6533\n","Epoch 3/100\n","223/223 [==============================] - 1243s 6s/step - loss: 2.2321 - accuracy: 0.6559 - val_loss: 2.1500 - val_accuracy: 0.6627\n","Epoch 4/100\n","223/223 [==============================] - 1239s 6s/step - loss: 2.1236 - accuracy: 0.6631 - val_loss: 2.0704 - val_accuracy: 0.6679\n","Epoch 5/100\n","223/223 [==============================] - 1235s 6s/step - loss: 2.0543 - accuracy: 0.6677 - val_loss: 2.0154 - val_accuracy: 0.6720\n","Epoch 6/100\n","223/223 [==============================] - 1235s 6s/step - loss: 1.9877 - accuracy: 0.6733 - val_loss: 1.9743 - val_accuracy: 0.6753\n","Epoch 7/100\n","223/223 [==============================] - 1245s 6s/step - loss: 1.9583 - accuracy: 0.6751 - val_loss: 1.9404 - val_accuracy: 0.6789\n","Epoch 8/100\n","223/223 [==============================] - 1243s 6s/step - loss: 1.9236 - accuracy: 0.6780 - val_loss: 1.9141 - val_accuracy: 0.6813\n","Epoch 9/100\n","223/223 [==============================] - 1258s 6s/step - loss: 1.8837 - accuracy: 0.6812 - val_loss: 1.8914 - val_accuracy: 0.6834\n","Epoch 10/100\n","223/223 [==============================] - 1260s 6s/step - loss: 1.8665 - accuracy: 0.6826 - val_loss: 1.8726 - val_accuracy: 0.6853\n","Epoch 11/100\n","223/223 [==============================] - 1254s 6s/step - loss: 1.8435 - accuracy: 0.6851 - val_loss: 1.8585 - val_accuracy: 0.6866\n","Epoch 12/100\n","223/223 [==============================] - 1247s 6s/step - loss: 1.8204 - accuracy: 0.6865 - val_loss: 1.8460 - val_accuracy: 0.6876\n","Epoch 13/100\n","223/223 [==============================] - 1253s 6s/step - loss: 1.8093 - accuracy: 0.6872 - val_loss: 1.8321 - val_accuracy: 0.6892\n","Epoch 14/100\n","223/223 [==============================] - 1250s 6s/step - loss: 1.7929 - accuracy: 0.6886 - val_loss: 1.8233 - val_accuracy: 0.6904\n","Epoch 15/100\n","223/223 [==============================] - 1249s 6s/step - loss: 1.7777 - accuracy: 0.6905 - val_loss: 1.8132 - val_accuracy: 0.6914\n","Epoch 16/100\n","223/223 [==============================] - 1251s 6s/step - loss: 1.7641 - accuracy: 0.6912 - val_loss: 1.8061 - val_accuracy: 0.6922\n","Epoch 17/100\n","223/223 [==============================] - 1246s 6s/step - loss: 1.7502 - accuracy: 0.6922 - val_loss: 1.8002 - val_accuracy: 0.6931\n","Epoch 18/100\n","223/223 [==============================] - 1247s 6s/step - loss: 1.7366 - accuracy: 0.6935 - val_loss: 1.7945 - val_accuracy: 0.6936\n","Epoch 19/100\n","223/223 [==============================] - 1251s 6s/step - loss: 1.7273 - accuracy: 0.6936 - val_loss: 1.7857 - val_accuracy: 0.6947\n","Epoch 20/100\n","223/223 [==============================] - 1268s 6s/step - loss: 1.7194 - accuracy: 0.6945 - val_loss: 1.7820 - val_accuracy: 0.6948\n","Epoch 21/100\n","223/223 [==============================] - 1260s 6s/step - loss: 1.7147 - accuracy: 0.6950 - val_loss: 1.7773 - val_accuracy: 0.6956\n","Epoch 22/100\n","223/223 [==============================] - 1257s 6s/step - loss: 1.6994 - accuracy: 0.6964 - val_loss: 1.7747 - val_accuracy: 0.6960\n","Epoch 23/100\n","223/223 [==============================] - 1259s 6s/step - loss: 1.6993 - accuracy: 0.6956 - val_loss: 1.7721 - val_accuracy: 0.6960\n","Epoch 24/100\n","223/223 [==============================] - 1253s 6s/step - loss: 1.6862 - accuracy: 0.6969 - val_loss: 1.7699 - val_accuracy: 0.6964\n","Epoch 25/100\n","223/223 [==============================] - 1253s 6s/step - loss: 1.6839 - accuracy: 0.6964 - val_loss: 1.7665 - val_accuracy: 0.6968\n","Epoch 26/100\n","223/223 [==============================] - 1253s 6s/step - loss: 1.6738 - accuracy: 0.6979 - val_loss: 1.7649 - val_accuracy: 0.6973\n","Epoch 27/100\n","223/223 [==============================] - 1253s 6s/step - loss: 1.6647 - accuracy: 0.6987 - val_loss: 1.7608 - val_accuracy: 0.6972\n","Epoch 28/100\n","223/223 [==============================] - 1255s 6s/step - loss: 1.6560 - accuracy: 0.6996 - val_loss: 1.7578 - val_accuracy: 0.6977\n","Epoch 29/100\n","223/223 [==============================] - 1259s 6s/step - loss: 1.6501 - accuracy: 0.7000 - val_loss: 1.7587 - val_accuracy: 0.6980\n","Epoch 30/100\n","223/223 [==============================] - 1258s 6s/step - loss: 1.6475 - accuracy: 0.7007 - val_loss: 1.7556 - val_accuracy: 0.6981\n","Epoch 31/100\n","223/223 [==============================] - 1257s 6s/step - loss: 1.6468 - accuracy: 0.6999 - val_loss: 1.7581 - val_accuracy: 0.6977\n","Epoch 32/100\n","223/223 [==============================] - 1265s 6s/step - loss: 1.6340 - accuracy: 0.7015 - val_loss: 1.7545 - val_accuracy: 0.6985\n","Epoch 33/100\n","223/223 [==============================] - 1261s 6s/step - loss: 1.6342 - accuracy: 0.7012 - val_loss: 1.7548 - val_accuracy: 0.6984\n","Epoch 34/100\n","223/223 [==============================] - 1268s 6s/step - loss: 1.6290 - accuracy: 0.7014 - val_loss: 1.7529 - val_accuracy: 0.6992\n","Epoch 35/100\n","223/223 [==============================] - 1268s 6s/step - loss: 1.6208 - accuracy: 0.7028 - val_loss: 1.7511 - val_accuracy: 0.6993\n","Epoch 36/100\n","223/223 [==============================] - 1264s 6s/step - loss: 1.6151 - accuracy: 0.7038 - val_loss: 1.7535 - val_accuracy: 0.6984\n","Epoch 37/100\n","223/223 [==============================] - 1259s 6s/step - loss: 1.6168 - accuracy: 0.7028 - val_loss: 1.7499 - val_accuracy: 0.6994\n","Epoch 38/100\n","223/223 [==============================] - 1265s 6s/step - loss: 1.6097 - accuracy: 0.7039 - val_loss: 1.7506 - val_accuracy: 0.6991\n","Epoch 39/100\n","223/223 [==============================] - 1275s 6s/step - loss: 1.6091 - accuracy: 0.7029 - val_loss: 1.7518 - val_accuracy: 0.6990\n","Epoch 40/100\n","223/223 [==============================] - 1262s 6s/step - loss: 1.5974 - accuracy: 0.7042 - val_loss: 1.7494 - val_accuracy: 0.6997\n","Epoch 41/100\n","223/223 [==============================] - 1272s 6s/step - loss: 1.5930 - accuracy: 0.7050 - val_loss: 1.7481 - val_accuracy: 0.6996\n","Epoch 42/100\n","223/223 [==============================] - 1266s 6s/step - loss: 1.5941 - accuracy: 0.7044 - val_loss: 1.7481 - val_accuracy: 0.6996\n","Epoch 43/100\n","223/223 [==============================] - 1272s 6s/step - loss: 1.5894 - accuracy: 0.7047 - val_loss: 1.7507 - val_accuracy: 0.6996\n","Epoch 44/100\n","223/223 [==============================] - 1277s 6s/step - loss: 1.5861 - accuracy: 0.7049 - val_loss: 1.7498 - val_accuracy: 0.7001\n","Epoch 45/100\n","223/223 [==============================] - 1272s 6s/step - loss: 1.5869 - accuracy: 0.7049 - val_loss: 1.7497 - val_accuracy: 0.7003\n","Epoch 46/100\n","223/223 [==============================] - 1281s 6s/step - loss: 1.5802 - accuracy: 0.7056 - val_loss: 1.7491 - val_accuracy: 0.7001\n","Epoch 47/100\n","223/223 [==============================] - 1270s 6s/step - loss: 1.5774 - accuracy: 0.7062 - val_loss: 1.7482 - val_accuracy: 0.7000\n","Epoch 48/100\n","223/223 [==============================] - 1273s 6s/step - loss: 1.5757 - accuracy: 0.7063 - val_loss: 1.7489 - val_accuracy: 0.6999\n","Epoch 49/100\n","223/223 [==============================] - 1275s 6s/step - loss: 1.5671 - accuracy: 0.7072 - val_loss: 1.7504 - val_accuracy: 0.6997\n","Epoch 50/100\n","223/223 [==============================] - 1270s 6s/step - loss: 1.5673 - accuracy: 0.7069 - val_loss: 1.7490 - val_accuracy: 0.7004\n","Epoch 51/100\n","223/223 [==============================] - 1268s 6s/step - loss: 1.5697 - accuracy: 0.7066 - val_loss: 1.7504 - val_accuracy: 0.6997\n","Epoch 00051: early stopping\n","total time required for training  64167.45829248428\n","Review: compared hanes partner company champion hoodie exactly needed cool winter spring fall nights fabric heavy cumbersome pulling head product complaints value compared 34 branded 34 sweats usual service amazon                                                \n","Original Summary: sweat price     \n","Predicted Summary:  great quality       \n","Review: briefs gift feel wear loves looks amazing complaints                                                                         \n","Original Summary: full support in the briefest of briefs  \n","Predicted Summary:  great        \n","Review: took chance shoes match champagne colored dress perfect looking small heel exactly looking quick delivery                                                                  \n","Original Summary: wedding accessories       \n","Predicted Summary:  love these shoes      \n","Review: fit like years ago cheaper quality materials gravity extra weight comfortable socks price                                                                    \n","Original Summary: love them but      \n","Predicted Summary:  good socks       \n","Review: received compliments pair shoes run bit small mind love getting colors                                                                      \n","Original Summary: very cute       \n","Predicted Summary:  great shoes       \n","Review: elegant perfect height beautiful black velvet love necklaces display easy buy necklaces nice good price homework best priced places looked                                                             \n","Original Summary: elegant very nice way to display your necklaces \n","Predicted Summary:  beautiful        \n","Review: styles choose happy got wife said look good block sunlight happy purchase                                                                     \n","Original Summary: cool sunglasses       \n","Predicted Summary:  great        \n","Review: dockers belt quality leather soft touch edging adds extra touch quality attractiveness belt husband happy                                                                  \n","Original Summary: top quality       \n","Predicted Summary:  great belt       \n","Review: boot cold weather sole little stiff need wear minute warm shoe strings look bad tied tie tuck bow tongue shown size runs tad small maybe 1 4 size fleece lining wear 8 5 ordered 9 perfect socks boot ready snow                                        \n","Original Summary: boot for snow fun    \n","Predicted Summary:  great boots       \n","total time to complete 15829.7639939785\n","bleu, precisions, bp, ratio, translation_length, reference_length (0.0, [0.2900551776136539, 0.0, 0.0, 0.0], 1.0, 19.09509591394331, 466856, 24449)\n","bleau score 0.7338717254431542\n","rouge2 (0.06599443828484215, 0.8312236286919831, 0.03436126421544687)\n","rouge {'rouge_1/f_score': 0.36915419958439477, 'rouge_1/r_score': 0.36583216309687744, 'rouge_1/p_score': 0.41662537566051006, 'rouge_2/f_score': 0.2881554948360078, 'rouge_2/r_score': 0.33457293997806903, 'rouge_2/p_score': 0.2751092653511975, 'rouge_l/f_score': 0.6180779054304887, 'rouge_l/r_score': 0.6641052933641588, 'rouge_l/p_score': 0.5875250521493721}\n","total time required for completing whole process  80087.67300844193\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 576x432 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"euct4cIGtr4u"},"source":["#summary using T5small pretrained model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"quKmq4CvdKGL"},"source":["#step26\n","#function is used to return the loss\n","def step(inputs_ids, attention_mask, y, pad_token_id, model):\n","  y_ids = y[:, :-1].contiguous()\n","  lm_labels = y[:, 1:].clone()\n","  lm_labels[y[:, 1:] == pad_token_id] = -100\n","  output = model(inputs_ids, attention_mask=attention_mask, decoder_input_ids=y_ids, lm_labels=lm_labels)\n","  # loss\n","  return output[0] "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"15XD1TxRdPN0"},"source":["#step25\n","#this function is used to train the pretrained t5small model\n","def t5train(train_loader,val_loader,pad_token_id,model,EPOCHS,log_interval):\n","  #initialize empty list for train_loss and val_loss\n","  train_loss = []\n","  val_loss = []\n","  #optimizer\n","  optimizer = torch.optim.Adam(model.parameters(),lr=1e-4, weight_decay=1e-4/25)\n","  #iterate for number of epochs\n","  for epoch in range(EPOCHS):\n","    model.train() \n","    #start time\n","    start_time = time.time()\n","    #for data in train_loader train the model\n","    for i, (inputs_ids, attention_mask, y) in enumerate(train_loader):\n","      inputs_ids = inputs_ids.to(device)\n","      attention_mask = attention_mask.to(device)\n","      y = y.to(device)\n","            \n","      optimizer.zero_grad()\n","      loss = step(inputs_ids, attention_mask, y, pad_token_id, model)\n","      train_loss.append(loss.item())\n","      loss.backward()\n","      torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n","      optimizer.step()\n","            \n","      if (i + 1) % log_interval == 0:\n","        with torch.no_grad():\n","          x, x_mask, y = next(iter(val_loader))\n","          x = x.to(device)\n","          x_mask = x_mask.to(device)\n","          y = y.to(device)\n","                \n","          v_loss = step(x, x_mask, y, pad_token_id, model)\n","          v_loss = v_loss.item()\n","                \n","          elapsed = time.time() - start_time\n","          print('| epoch {:3d} | [{:5d}/{:5d}] | '\n","                'ms/batch {:5.2f} | '\n","                'loss {:5.2f} | val loss {:5.2f}'.format(\n","                  epoch, i, len(train_loader),\n","                  elapsed * 1000 / log_interval,\n","                  loss.item(), v_loss))\n","          start_time = time.time()\n","          val_loss.append(v_loss)\n","\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zYDbQrNxdYdI"},"source":["#step26\n","#function to test the model it writes original and predicted summary in txt file\n","def testT5(model,tokenizer,test_loader):\n","  #intialize the empty lists\n","  predictions = []\n","  real_og=[]\n","  pred_op=[]\n","  c=0\n","  b=1000\n","  #for data in test loader\n","  for i, (input_ids, attention_mask, y) in enumerate(test_loader):\n","    input_ids = input_ids.to(device)\n","    attention_mask = attention_mask.to(device)\n","    y = y.to(device)\n","    #generate summaries \n","    #store real and predicted summary in a list and write in txt file\n","    summaries = model.generate(input_ids=input_ids, attention_mask=attention_mask,max_length=10)\n","    pred = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summaries]\n","    real = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in y]\n","    #this part is used to print output if the size of c is greater than b \n","    #limited output is print as only 5000 lines can be printed in colab whole output is written in a text file \n","    for pred_sent, real_sent in zip(pred, real): \n","      if c>b:\n","        print(\"Original: {}\".format(real_sent))\n","        print(\"Predicted: {}\".format(pred_sent))\n","        print(\"\\n\")\n","        b+=b\n","      real_og.append(real_sent)\n","      pred_op.append(pred_sent)\n","      predictions.append(str(\"pred sentence: \" + pred_sent + \"\\t\\t real sentence: \" + real_sent+\"\\n\"))\n","      c+=1\n","  file1 = open(\"/content/drive/MyDrive/TFIVE.txt\",\"w\")\n","  file1.writelines(predictions)\n","  file1.close()\n","  #calculate scores\n","  bleau=compute_bleu(real_og,pred_op, max_order=4,smooth=False)\n","  bscore=nltk.translate.bleu_score.corpus_bleu(real_og,pred_op)\n","  rougen=rouge_n(pred_op, real_og, n=2)\n","  ro=rouge(pred_op, real_og)\n","\n","  print(\"bleu, precisions, bp, ratio, translation_length, reference_length\",bleau)\n","  print(\"bleau score\",bscore)\n","  print(\"rouge2\",rougen)\n","  print(\"rouge\",ro)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"awxFEtTPbMgZ"},"source":["#step27\n","#fucntion to get the data and call all the functions in a squence\n","def tf5token():\n","  class MyDataset(torch.utils.data.Dataset):\n","    def __init__(self, articles, highlights):\n","      self.x = articles\n","      self.y = highlights\n","  \n","    def __getitem__(self,index):\n","      x = tokenizer.encode_plus(model.config.prefix + str(self.x[index]), max_length=80, return_tensors=\"pt\", pad_to_max_length=True)\n","      y = tokenizer.encode(str(self.y[index]), max_length=10, return_tensors=\"pt\", pad_to_max_length=True)\n","      return x['input_ids'].view(-1), x['attention_mask'].view(-1), y.view(-1)\n","        \n","    def __len__(self):\n","      return len(self.x)\n","\n","  #get the data\n","  x_tr,y_tr,x_tt,y_tt,x_val,y_val=combining_all_steps_t5()\n","  BATCH_SIZE = 128\n","  SHUFFEL_SIZE = 1024\n","  EPOCHS = 150\n","  log_interval = 200\n","  #get the pretrained model t5-small\n","  tokenizer = T5Tokenizer.from_pretrained('t5-small')\n","  model = T5ForConditionalGeneration.from_pretrained('t5-small').to(device)\n","\n","  task_specific_params = model.config.task_specific_params\n","  if task_specific_params is not None:\n","    model.config.update(task_specific_params.get(\"summarization\", {}))\n","  \n","  #create train,test and validation datasets\n","  train_ds = MyDataset(x_tr[\"reviewText\"],y_tr[\"summary\"]) \n","  val_ds = MyDataset(x_val[\"reviewText\"],y_val[\"summary\"])\n","  test_ds = MyDataset(x_tt[\"reviewText\"],y_tt[\"summary\"])\n","\n","  train_loader = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE)\n","  val_loader = torch.utils.data.DataLoader(val_ds, batch_size=BATCH_SIZE)\n","  test_loader = torch.utils.data.DataLoader(test_ds, batch_size=BATCH_SIZE)\n","\n","  x, x_mask, y = next(iter(val_loader))\n","  print(x.shape, x_mask.shape, y.shape)\n","  pad_token_id = tokenizer.pad_token_id\n","\n","  #call the train function\n","  model=t5train(train_loader,val_loader,pad_token_id,model,EPOCHS,log_interval)\n","  #call the test function\n","  testT5(model,tokenizer,test_loader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yQPZ1GzZx5Ej","executionInfo":{"status":"ok","timestamp":1618668312380,"user_tz":-330,"elapsed":27772458,"user":{"displayName":"devansh mody","photoUrl":"","userId":"11540078254175805123"}},"outputId":"3bdcacc7-7bf5-4c40-c8d3-b14d53e07dc3"},"source":["tf5token()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The length of dataset is  147799\n","train 103459, val 22170, test 22170\n","torch.Size([128, 80]) torch.Size([128, 80]) torch.Size([128, 10])\n","| epoch   0 | [  199/  809] | ms/batch 247.76 | loss  4.32 | val loss  4.37\n","| epoch   0 | [  399/  809] | ms/batch 248.13 | loss  3.74 | val loss  4.19\n","| epoch   0 | [  599/  809] | ms/batch 248.59 | loss  4.19 | val loss  4.01\n","| epoch   0 | [  799/  809] | ms/batch 247.97 | loss  3.83 | val loss  3.94\n","| epoch   1 | [  199/  809] | ms/batch 248.90 | loss  3.82 | val loss  3.87\n","| epoch   1 | [  399/  809] | ms/batch 249.66 | loss  3.50 | val loss  3.90\n","| epoch   1 | [  599/  809] | ms/batch 249.99 | loss  3.95 | val loss  3.82\n","| epoch   1 | [  799/  809] | ms/batch 250.72 | loss  3.66 | val loss  3.79\n","| epoch   2 | [  199/  809] | ms/batch 251.27 | loss  3.67 | val loss  3.76\n","| epoch   2 | [  399/  809] | ms/batch 250.84 | loss  3.42 | val loss  3.70\n","| epoch   2 | [  599/  809] | ms/batch 251.13 | loss  3.73 | val loss  3.77\n","| epoch   2 | [  799/  809] | ms/batch 250.01 | loss  3.54 | val loss  3.71\n","| epoch   3 | [  199/  809] | ms/batch 250.65 | loss  3.59 | val loss  3.73\n","| epoch   3 | [  399/  809] | ms/batch 250.62 | loss  3.37 | val loss  3.65\n","| epoch   3 | [  599/  809] | ms/batch 250.53 | loss  3.67 | val loss  3.66\n","| epoch   3 | [  799/  809] | ms/batch 250.37 | loss  3.46 | val loss  3.70\n","| epoch   4 | [  199/  809] | ms/batch 250.74 | loss  3.48 | val loss  3.58\n","| epoch   4 | [  399/  809] | ms/batch 250.33 | loss  3.30 | val loss  3.59\n","| epoch   4 | [  599/  809] | ms/batch 250.53 | loss  3.60 | val loss  3.61\n","| epoch   4 | [  799/  809] | ms/batch 249.99 | loss  3.38 | val loss  3.62\n","| epoch   5 | [  199/  809] | ms/batch 250.36 | loss  3.38 | val loss  3.62\n","| epoch   5 | [  399/  809] | ms/batch 249.20 | loss  3.19 | val loss  3.63\n","| epoch   5 | [  599/  809] | ms/batch 249.51 | loss  3.53 | val loss  3.58\n","| epoch   5 | [  799/  809] | ms/batch 249.32 | loss  3.39 | val loss  3.54\n","| epoch   6 | [  199/  809] | ms/batch 250.17 | loss  3.32 | val loss  3.61\n","| epoch   6 | [  399/  809] | ms/batch 249.12 | loss  3.11 | val loss  3.55\n","| epoch   6 | [  599/  809] | ms/batch 251.31 | loss  3.45 | val loss  3.52\n","| epoch   6 | [  799/  809] | ms/batch 250.64 | loss  3.29 | val loss  3.47\n","| epoch   7 | [  199/  809] | ms/batch 249.94 | loss  3.32 | val loss  3.47\n","| epoch   7 | [  399/  809] | ms/batch 249.94 | loss  3.12 | val loss  3.44\n","| epoch   7 | [  599/  809] | ms/batch 249.65 | loss  3.42 | val loss  3.52\n","| epoch   7 | [  799/  809] | ms/batch 250.05 | loss  3.28 | val loss  3.41\n","| epoch   8 | [  199/  809] | ms/batch 250.14 | loss  3.18 | val loss  3.44\n","| epoch   8 | [  399/  809] | ms/batch 250.49 | loss  3.06 | val loss  3.49\n","| epoch   8 | [  599/  809] | ms/batch 250.25 | loss  3.34 | val loss  3.41\n","| epoch   8 | [  799/  809] | ms/batch 249.93 | loss  3.24 | val loss  3.46\n","| epoch   9 | [  199/  809] | ms/batch 249.50 | loss  3.20 | val loss  3.37\n","| epoch   9 | [  399/  809] | ms/batch 249.73 | loss  3.02 | val loss  3.32\n","| epoch   9 | [  599/  809] | ms/batch 249.70 | loss  3.26 | val loss  3.46\n","| epoch   9 | [  799/  809] | ms/batch 250.00 | loss  3.25 | val loss  3.48\n","| epoch  10 | [  199/  809] | ms/batch 250.51 | loss  3.14 | val loss  3.45\n","| epoch  10 | [  399/  809] | ms/batch 249.99 | loss  2.96 | val loss  3.42\n","| epoch  10 | [  599/  809] | ms/batch 249.54 | loss  3.27 | val loss  3.37\n","| epoch  10 | [  799/  809] | ms/batch 250.41 | loss  3.13 | val loss  3.39\n","| epoch  11 | [  199/  809] | ms/batch 250.59 | loss  3.10 | val loss  3.38\n","| epoch  11 | [  399/  809] | ms/batch 249.91 | loss  2.84 | val loss  3.35\n","| epoch  11 | [  599/  809] | ms/batch 250.52 | loss  3.22 | val loss  3.41\n","| epoch  11 | [  799/  809] | ms/batch 250.03 | loss  3.21 | val loss  3.36\n","| epoch  12 | [  199/  809] | ms/batch 249.50 | loss  3.11 | val loss  3.32\n","| epoch  12 | [  399/  809] | ms/batch 250.16 | loss  2.88 | val loss  3.34\n","| epoch  12 | [  599/  809] | ms/batch 250.44 | loss  3.21 | val loss  3.27\n","| epoch  12 | [  799/  809] | ms/batch 249.76 | loss  3.09 | val loss  3.39\n","| epoch  13 | [  199/  809] | ms/batch 249.93 | loss  3.02 | val loss  3.35\n","| epoch  13 | [  399/  809] | ms/batch 248.74 | loss  2.84 | val loss  3.38\n","| epoch  13 | [  599/  809] | ms/batch 249.41 | loss  3.18 | val loss  3.40\n","| epoch  13 | [  799/  809] | ms/batch 249.34 | loss  3.11 | val loss  3.37\n","| epoch  14 | [  199/  809] | ms/batch 249.34 | loss  3.00 | val loss  3.39\n","| epoch  14 | [  399/  809] | ms/batch 249.09 | loss  2.74 | val loss  3.41\n","| epoch  14 | [  599/  809] | ms/batch 249.66 | loss  3.20 | val loss  3.38\n","| epoch  14 | [  799/  809] | ms/batch 251.43 | loss  3.08 | val loss  3.38\n","| epoch  15 | [  199/  809] | ms/batch 251.38 | loss  2.97 | val loss  3.37\n","| epoch  15 | [  399/  809] | ms/batch 250.15 | loss  2.76 | val loss  3.39\n","| epoch  15 | [  599/  809] | ms/batch 249.78 | loss  3.09 | val loss  3.37\n","| epoch  15 | [  799/  809] | ms/batch 249.48 | loss  3.04 | val loss  3.35\n","| epoch  16 | [  199/  809] | ms/batch 250.30 | loss  2.92 | val loss  3.42\n","| epoch  16 | [  399/  809] | ms/batch 249.65 | loss  2.69 | val loss  3.35\n","| epoch  16 | [  599/  809] | ms/batch 249.92 | loss  3.04 | val loss  3.39\n","| epoch  16 | [  799/  809] | ms/batch 250.85 | loss  3.06 | val loss  3.35\n","| epoch  17 | [  199/  809] | ms/batch 250.13 | loss  2.89 | val loss  3.42\n","| epoch  17 | [  399/  809] | ms/batch 250.30 | loss  2.70 | val loss  3.37\n","| epoch  17 | [  599/  809] | ms/batch 249.76 | loss  3.03 | val loss  3.32\n","| epoch  17 | [  799/  809] | ms/batch 250.98 | loss  2.97 | val loss  3.39\n","| epoch  18 | [  199/  809] | ms/batch 249.68 | loss  2.90 | val loss  3.32\n","| epoch  18 | [  399/  809] | ms/batch 247.22 | loss  2.67 | val loss  3.48\n","| epoch  18 | [  599/  809] | ms/batch 249.25 | loss  3.01 | val loss  3.36\n","| epoch  18 | [  799/  809] | ms/batch 249.10 | loss  3.01 | val loss  3.46\n","| epoch  19 | [  199/  809] | ms/batch 247.67 | loss  2.93 | val loss  3.30\n","| epoch  19 | [  399/  809] | ms/batch 249.34 | loss  2.70 | val loss  3.39\n","| epoch  19 | [  599/  809] | ms/batch 248.97 | loss  3.03 | val loss  3.32\n","| epoch  19 | [  799/  809] | ms/batch 249.18 | loss  2.95 | val loss  3.30\n","| epoch  20 | [  199/  809] | ms/batch 249.53 | loss  2.79 | val loss  3.38\n","| epoch  20 | [  399/  809] | ms/batch 249.33 | loss  2.58 | val loss  3.38\n","| epoch  20 | [  599/  809] | ms/batch 249.66 | loss  2.92 | val loss  3.32\n","| epoch  20 | [  799/  809] | ms/batch 249.57 | loss  2.96 | val loss  3.40\n","| epoch  21 | [  199/  809] | ms/batch 248.95 | loss  2.81 | val loss  3.43\n","| epoch  21 | [  399/  809] | ms/batch 248.85 | loss  2.64 | val loss  3.47\n","| epoch  21 | [  599/  809] | ms/batch 248.73 | loss  2.94 | val loss  3.38\n","| epoch  21 | [  799/  809] | ms/batch 247.84 | loss  2.90 | val loss  3.31\n","| epoch  22 | [  199/  809] | ms/batch 248.49 | loss  2.81 | val loss  3.38\n","| epoch  22 | [  399/  809] | ms/batch 248.16 | loss  2.50 | val loss  3.36\n","| epoch  22 | [  599/  809] | ms/batch 247.28 | loss  2.90 | val loss  3.43\n","| epoch  22 | [  799/  809] | ms/batch 247.45 | loss  2.92 | val loss  3.45\n","| epoch  23 | [  199/  809] | ms/batch 247.13 | loss  2.77 | val loss  3.43\n","| epoch  23 | [  399/  809] | ms/batch 247.20 | loss  2.53 | val loss  3.37\n","| epoch  23 | [  599/  809] | ms/batch 247.71 | loss  2.94 | val loss  3.37\n","| epoch  23 | [  799/  809] | ms/batch 250.64 | loss  2.87 | val loss  3.44\n","| epoch  24 | [  199/  809] | ms/batch 248.72 | loss  2.72 | val loss  3.48\n","| epoch  24 | [  399/  809] | ms/batch 248.19 | loss  2.52 | val loss  3.40\n","| epoch  24 | [  599/  809] | ms/batch 250.84 | loss  2.94 | val loss  3.37\n","| epoch  24 | [  799/  809] | ms/batch 251.39 | loss  2.80 | val loss  3.38\n","| epoch  25 | [  199/  809] | ms/batch 250.62 | loss  2.71 | val loss  3.43\n","| epoch  25 | [  399/  809] | ms/batch 250.08 | loss  2.42 | val loss  3.45\n","| epoch  25 | [  599/  809] | ms/batch 249.37 | loss  2.79 | val loss  3.34\n","| epoch  25 | [  799/  809] | ms/batch 250.09 | loss  2.87 | val loss  3.39\n","| epoch  26 | [  199/  809] | ms/batch 249.45 | loss  2.61 | val loss  3.38\n","| epoch  26 | [  399/  809] | ms/batch 248.94 | loss  2.43 | val loss  3.34\n","| epoch  26 | [  599/  809] | ms/batch 248.98 | loss  2.80 | val loss  3.41\n","| epoch  26 | [  799/  809] | ms/batch 248.05 | loss  2.80 | val loss  3.40\n","| epoch  27 | [  199/  809] | ms/batch 249.69 | loss  2.65 | val loss  3.41\n","| epoch  27 | [  399/  809] | ms/batch 249.15 | loss  2.43 | val loss  3.46\n","| epoch  27 | [  599/  809] | ms/batch 247.87 | loss  2.73 | val loss  3.39\n","| epoch  27 | [  799/  809] | ms/batch 247.51 | loss  2.80 | val loss  3.47\n","| epoch  28 | [  199/  809] | ms/batch 250.18 | loss  2.56 | val loss  3.35\n","| epoch  28 | [  399/  809] | ms/batch 251.69 | loss  2.43 | val loss  3.40\n","| epoch  28 | [  599/  809] | ms/batch 249.91 | loss  2.69 | val loss  3.45\n","| epoch  28 | [  799/  809] | ms/batch 250.29 | loss  2.73 | val loss  3.55\n","| epoch  29 | [  199/  809] | ms/batch 249.72 | loss  2.54 | val loss  3.44\n","| epoch  29 | [  399/  809] | ms/batch 249.77 | loss  2.37 | val loss  3.41\n","| epoch  29 | [  599/  809] | ms/batch 248.94 | loss  2.72 | val loss  3.42\n","| epoch  29 | [  799/  809] | ms/batch 248.99 | loss  2.74 | val loss  3.38\n","| epoch  30 | [  199/  809] | ms/batch 248.36 | loss  2.63 | val loss  3.38\n","| epoch  30 | [  399/  809] | ms/batch 248.94 | loss  2.34 | val loss  3.43\n","| epoch  30 | [  599/  809] | ms/batch 248.14 | loss  2.75 | val loss  3.56\n","| epoch  30 | [  799/  809] | ms/batch 248.70 | loss  2.70 | val loss  3.53\n","| epoch  31 | [  199/  809] | ms/batch 248.19 | loss  2.62 | val loss  3.38\n","| epoch  31 | [  399/  809] | ms/batch 249.17 | loss  2.31 | val loss  3.52\n","| epoch  31 | [  599/  809] | ms/batch 248.82 | loss  2.70 | val loss  3.43\n","| epoch  31 | [  799/  809] | ms/batch 249.40 | loss  2.70 | val loss  3.42\n","| epoch  32 | [  199/  809] | ms/batch 249.37 | loss  2.48 | val loss  3.47\n","| epoch  32 | [  399/  809] | ms/batch 249.09 | loss  2.24 | val loss  3.49\n","| epoch  32 | [  599/  809] | ms/batch 251.04 | loss  2.67 | val loss  3.47\n","| epoch  32 | [  799/  809] | ms/batch 250.67 | loss  2.73 | val loss  3.51\n","| epoch  33 | [  199/  809] | ms/batch 249.22 | loss  2.55 | val loss  3.40\n","| epoch  33 | [  399/  809] | ms/batch 249.69 | loss  2.22 | val loss  3.54\n","| epoch  33 | [  599/  809] | ms/batch 249.25 | loss  2.65 | val loss  3.26\n","| epoch  33 | [  799/  809] | ms/batch 248.94 | loss  2.71 | val loss  3.36\n","| epoch  34 | [  199/  809] | ms/batch 250.17 | loss  2.43 | val loss  3.39\n","| epoch  34 | [  399/  809] | ms/batch 250.01 | loss  2.17 | val loss  3.50\n","| epoch  34 | [  599/  809] | ms/batch 249.54 | loss  2.70 | val loss  3.38\n","| epoch  34 | [  799/  809] | ms/batch 249.44 | loss  2.61 | val loss  3.47\n","| epoch  35 | [  199/  809] | ms/batch 249.45 | loss  2.46 | val loss  3.50\n","| epoch  35 | [  399/  809] | ms/batch 249.14 | loss  2.27 | val loss  3.47\n","| epoch  35 | [  599/  809] | ms/batch 249.26 | loss  2.65 | val loss  3.43\n","| epoch  35 | [  799/  809] | ms/batch 249.05 | loss  2.62 | val loss  3.44\n","| epoch  36 | [  199/  809] | ms/batch 248.89 | loss  2.44 | val loss  3.45\n","| epoch  36 | [  399/  809] | ms/batch 250.10 | loss  2.28 | val loss  3.52\n","| epoch  36 | [  599/  809] | ms/batch 249.18 | loss  2.61 | val loss  3.42\n","| epoch  36 | [  799/  809] | ms/batch 250.16 | loss  2.60 | val loss  3.52\n","| epoch  37 | [  199/  809] | ms/batch 250.17 | loss  2.44 | val loss  3.40\n","| epoch  37 | [  399/  809] | ms/batch 251.38 | loss  2.14 | val loss  3.43\n","| epoch  37 | [  599/  809] | ms/batch 251.87 | loss  2.53 | val loss  3.58\n","| epoch  37 | [  799/  809] | ms/batch 251.01 | loss  2.62 | val loss  3.53\n","| epoch  38 | [  199/  809] | ms/batch 251.74 | loss  2.40 | val loss  3.47\n","| epoch  38 | [  399/  809] | ms/batch 251.53 | loss  2.14 | val loss  3.46\n","| epoch  38 | [  599/  809] | ms/batch 249.59 | loss  2.63 | val loss  3.48\n","| epoch  38 | [  799/  809] | ms/batch 250.49 | loss  2.52 | val loss  3.58\n","| epoch  39 | [  199/  809] | ms/batch 249.21 | loss  2.47 | val loss  3.52\n","| epoch  39 | [  399/  809] | ms/batch 249.33 | loss  2.01 | val loss  3.44\n","| epoch  39 | [  599/  809] | ms/batch 249.59 | loss  2.52 | val loss  3.48\n","| epoch  39 | [  799/  809] | ms/batch 249.37 | loss  2.53 | val loss  3.41\n","| epoch  40 | [  199/  809] | ms/batch 248.87 | loss  2.44 | val loss  3.47\n","| epoch  40 | [  399/  809] | ms/batch 248.40 | loss  2.08 | val loss  3.55\n","| epoch  40 | [  599/  809] | ms/batch 249.23 | loss  2.55 | val loss  3.43\n","| epoch  40 | [  799/  809] | ms/batch 249.45 | loss  2.51 | val loss  3.59\n","| epoch  41 | [  199/  809] | ms/batch 249.50 | loss  2.36 | val loss  3.50\n","| epoch  41 | [  399/  809] | ms/batch 249.26 | loss  2.11 | val loss  3.50\n","| epoch  41 | [  599/  809] | ms/batch 248.74 | loss  2.48 | val loss  3.45\n","| epoch  41 | [  799/  809] | ms/batch 247.65 | loss  2.50 | val loss  3.46\n","| epoch  42 | [  199/  809] | ms/batch 249.37 | loss  2.31 | val loss  3.52\n","| epoch  42 | [  399/  809] | ms/batch 249.26 | loss  2.03 | val loss  3.57\n","| epoch  42 | [  599/  809] | ms/batch 248.05 | loss  2.50 | val loss  3.59\n","| epoch  42 | [  799/  809] | ms/batch 247.18 | loss  2.43 | val loss  3.45\n","| epoch  43 | [  199/  809] | ms/batch 248.37 | loss  2.31 | val loss  3.52\n","| epoch  43 | [  399/  809] | ms/batch 248.68 | loss  1.92 | val loss  3.50\n","| epoch  43 | [  599/  809] | ms/batch 249.33 | loss  2.47 | val loss  3.57\n","| epoch  43 | [  799/  809] | ms/batch 249.54 | loss  2.44 | val loss  3.57\n","| epoch  44 | [  199/  809] | ms/batch 248.11 | loss  2.27 | val loss  3.55\n","| epoch  44 | [  399/  809] | ms/batch 251.18 | loss  2.01 | val loss  3.56\n","| epoch  44 | [  599/  809] | ms/batch 249.54 | loss  2.42 | val loss  3.55\n","| epoch  44 | [  799/  809] | ms/batch 249.55 | loss  2.45 | val loss  3.57\n","| epoch  45 | [  199/  809] | ms/batch 250.87 | loss  2.28 | val loss  3.65\n","| epoch  45 | [  399/  809] | ms/batch 248.98 | loss  2.01 | val loss  3.68\n","| epoch  45 | [  599/  809] | ms/batch 249.72 | loss  2.33 | val loss  3.51\n","| epoch  45 | [  799/  809] | ms/batch 249.42 | loss  2.36 | val loss  3.43\n","| epoch  46 | [  199/  809] | ms/batch 248.63 | loss  2.31 | val loss  3.50\n","| epoch  46 | [  399/  809] | ms/batch 248.87 | loss  1.94 | val loss  3.47\n","| epoch  46 | [  599/  809] | ms/batch 248.78 | loss  2.33 | val loss  3.52\n","| epoch  46 | [  799/  809] | ms/batch 248.61 | loss  2.37 | val loss  3.49\n","| epoch  47 | [  199/  809] | ms/batch 248.30 | loss  2.22 | val loss  3.51\n","| epoch  47 | [  399/  809] | ms/batch 250.30 | loss  1.94 | val loss  3.51\n","| epoch  47 | [  599/  809] | ms/batch 250.47 | loss  2.43 | val loss  3.63\n","| epoch  47 | [  799/  809] | ms/batch 249.65 | loss  2.40 | val loss  3.64\n","| epoch  48 | [  199/  809] | ms/batch 248.59 | loss  2.20 | val loss  3.44\n","| epoch  48 | [  399/  809] | ms/batch 248.36 | loss  1.96 | val loss  3.65\n","| epoch  48 | [  599/  809] | ms/batch 250.04 | loss  2.41 | val loss  3.64\n","| epoch  48 | [  799/  809] | ms/batch 251.69 | loss  2.35 | val loss  3.49\n","| epoch  49 | [  199/  809] | ms/batch 251.22 | loss  2.20 | val loss  3.67\n","| epoch  49 | [  399/  809] | ms/batch 250.87 | loss  1.97 | val loss  3.49\n","| epoch  49 | [  599/  809] | ms/batch 251.03 | loss  2.32 | val loss  3.68\n","| epoch  49 | [  799/  809] | ms/batch 251.03 | loss  2.31 | val loss  3.62\n","| epoch  50 | [  199/  809] | ms/batch 250.60 | loss  2.14 | val loss  3.64\n","| epoch  50 | [  399/  809] | ms/batch 250.77 | loss  1.88 | val loss  3.74\n","| epoch  50 | [  599/  809] | ms/batch 251.67 | loss  2.30 | val loss  3.64\n","| epoch  50 | [  799/  809] | ms/batch 250.49 | loss  2.30 | val loss  3.61\n","| epoch  51 | [  199/  809] | ms/batch 251.49 | loss  2.16 | val loss  3.60\n","| epoch  51 | [  399/  809] | ms/batch 251.12 | loss  1.83 | val loss  3.63\n","| epoch  51 | [  599/  809] | ms/batch 250.99 | loss  2.34 | val loss  3.64\n","| epoch  51 | [  799/  809] | ms/batch 250.68 | loss  2.30 | val loss  3.59\n","| epoch  52 | [  199/  809] | ms/batch 250.85 | loss  2.12 | val loss  3.64\n","| epoch  52 | [  399/  809] | ms/batch 250.44 | loss  1.83 | val loss  3.63\n","| epoch  52 | [  599/  809] | ms/batch 251.36 | loss  2.22 | val loss  3.63\n","| epoch  52 | [  799/  809] | ms/batch 250.84 | loss  2.29 | val loss  3.72\n","| epoch  53 | [  199/  809] | ms/batch 250.80 | loss  2.12 | val loss  3.56\n","| epoch  53 | [  399/  809] | ms/batch 250.53 | loss  1.76 | val loss  3.63\n","| epoch  53 | [  599/  809] | ms/batch 250.97 | loss  2.22 | val loss  3.60\n","| epoch  53 | [  799/  809] | ms/batch 250.99 | loss  2.26 | val loss  3.64\n","| epoch  54 | [  199/  809] | ms/batch 251.39 | loss  2.14 | val loss  3.59\n","| epoch  54 | [  399/  809] | ms/batch 250.82 | loss  1.79 | val loss  3.63\n","| epoch  54 | [  599/  809] | ms/batch 251.09 | loss  2.26 | val loss  3.73\n","| epoch  54 | [  799/  809] | ms/batch 251.81 | loss  2.28 | val loss  3.59\n","| epoch  55 | [  199/  809] | ms/batch 250.94 | loss  2.05 | val loss  3.66\n","| epoch  55 | [  399/  809] | ms/batch 251.10 | loss  1.81 | val loss  3.70\n","| epoch  55 | [  599/  809] | ms/batch 251.43 | loss  2.22 | val loss  3.65\n","| epoch  55 | [  799/  809] | ms/batch 251.10 | loss  2.30 | val loss  3.74\n","| epoch  56 | [  199/  809] | ms/batch 251.16 | loss  2.04 | val loss  3.66\n","| epoch  56 | [  399/  809] | ms/batch 250.41 | loss  1.72 | val loss  3.63\n","| epoch  56 | [  599/  809] | ms/batch 250.69 | loss  2.18 | val loss  3.68\n","| epoch  56 | [  799/  809] | ms/batch 251.34 | loss  2.20 | val loss  3.68\n","| epoch  57 | [  199/  809] | ms/batch 251.36 | loss  2.07 | val loss  3.63\n","| epoch  57 | [  399/  809] | ms/batch 251.73 | loss  1.72 | val loss  3.71\n","| epoch  57 | [  599/  809] | ms/batch 251.72 | loss  2.15 | val loss  3.61\n","| epoch  57 | [  799/  809] | ms/batch 251.35 | loss  2.23 | val loss  3.70\n","| epoch  58 | [  199/  809] | ms/batch 250.90 | loss  1.92 | val loss  3.64\n","| epoch  58 | [  399/  809] | ms/batch 250.57 | loss  1.81 | val loss  3.64\n","| epoch  58 | [  599/  809] | ms/batch 251.80 | loss  2.18 | val loss  3.74\n","| epoch  58 | [  799/  809] | ms/batch 251.72 | loss  2.14 | val loss  3.75\n","| epoch  59 | [  199/  809] | ms/batch 252.41 | loss  2.01 | val loss  3.69\n","| epoch  59 | [  399/  809] | ms/batch 253.04 | loss  1.74 | val loss  3.63\n","| epoch  59 | [  599/  809] | ms/batch 252.82 | loss  2.10 | val loss  3.70\n","| epoch  59 | [  799/  809] | ms/batch 252.12 | loss  2.18 | val loss  3.75\n","| epoch  60 | [  199/  809] | ms/batch 252.27 | loss  2.00 | val loss  3.75\n","| epoch  60 | [  399/  809] | ms/batch 250.79 | loss  1.76 | val loss  3.70\n","| epoch  60 | [  599/  809] | ms/batch 250.54 | loss  2.04 | val loss  3.87\n","| epoch  60 | [  799/  809] | ms/batch 250.77 | loss  2.19 | val loss  3.80\n","| epoch  61 | [  199/  809] | ms/batch 250.78 | loss  1.95 | val loss  3.68\n","| epoch  61 | [  399/  809] | ms/batch 251.30 | loss  1.71 | val loss  3.76\n","| epoch  61 | [  599/  809] | ms/batch 251.03 | loss  2.20 | val loss  3.85\n","| epoch  61 | [  799/  809] | ms/batch 250.74 | loss  2.14 | val loss  3.56\n","| epoch  62 | [  199/  809] | ms/batch 250.92 | loss  1.95 | val loss  3.63\n","| epoch  62 | [  399/  809] | ms/batch 250.84 | loss  1.63 | val loss  3.71\n","| epoch  62 | [  599/  809] | ms/batch 250.54 | loss  2.08 | val loss  3.75\n","| epoch  62 | [  799/  809] | ms/batch 250.86 | loss  2.10 | val loss  3.59\n","| epoch  63 | [  199/  809] | ms/batch 251.04 | loss  2.00 | val loss  3.67\n","| epoch  63 | [  399/  809] | ms/batch 250.26 | loss  1.67 | val loss  3.78\n","| epoch  63 | [  599/  809] | ms/batch 251.14 | loss  2.03 | val loss  3.83\n","| epoch  63 | [  799/  809] | ms/batch 251.67 | loss  2.05 | val loss  3.77\n","| epoch  64 | [  199/  809] | ms/batch 250.88 | loss  1.86 | val loss  3.73\n","| epoch  64 | [  399/  809] | ms/batch 250.49 | loss  1.66 | val loss  3.64\n","| epoch  64 | [  599/  809] | ms/batch 250.24 | loss  2.07 | val loss  3.79\n","| epoch  64 | [  799/  809] | ms/batch 250.18 | loss  1.99 | val loss  3.81\n","| epoch  65 | [  199/  809] | ms/batch 251.22 | loss  1.98 | val loss  3.76\n","| epoch  65 | [  399/  809] | ms/batch 250.88 | loss  1.58 | val loss  3.85\n","| epoch  65 | [  599/  809] | ms/batch 250.88 | loss  2.08 | val loss  3.87\n","| epoch  65 | [  799/  809] | ms/batch 252.70 | loss  2.05 | val loss  3.69\n","| epoch  66 | [  199/  809] | ms/batch 250.68 | loss  1.86 | val loss  3.96\n","| epoch  66 | [  399/  809] | ms/batch 249.50 | loss  1.59 | val loss  3.81\n","| epoch  66 | [  599/  809] | ms/batch 250.48 | loss  1.99 | val loss  3.86\n","| epoch  66 | [  799/  809] | ms/batch 249.53 | loss  2.02 | val loss  3.80\n","| epoch  67 | [  199/  809] | ms/batch 249.97 | loss  1.84 | val loss  3.68\n","| epoch  67 | [  399/  809] | ms/batch 250.17 | loss  1.62 | val loss  3.68\n","| epoch  67 | [  599/  809] | ms/batch 250.41 | loss  1.95 | val loss  3.90\n","| epoch  67 | [  799/  809] | ms/batch 249.55 | loss  1.95 | val loss  3.93\n","| epoch  68 | [  199/  809] | ms/batch 249.51 | loss  1.83 | val loss  3.73\n","| epoch  68 | [  399/  809] | ms/batch 250.31 | loss  1.59 | val loss  3.84\n","| epoch  68 | [  599/  809] | ms/batch 249.36 | loss  1.97 | val loss  3.92\n","| epoch  68 | [  799/  809] | ms/batch 249.80 | loss  1.92 | val loss  3.84\n","| epoch  69 | [  199/  809] | ms/batch 249.87 | loss  1.78 | val loss  3.74\n","| epoch  69 | [  399/  809] | ms/batch 249.25 | loss  1.47 | val loss  3.85\n","| epoch  69 | [  599/  809] | ms/batch 249.53 | loss  1.94 | val loss  3.93\n","| epoch  69 | [  799/  809] | ms/batch 250.05 | loss  1.98 | val loss  3.91\n","| epoch  70 | [  199/  809] | ms/batch 249.67 | loss  1.72 | val loss  3.83\n","| epoch  70 | [  399/  809] | ms/batch 249.57 | loss  1.59 | val loss  3.83\n","| epoch  70 | [  599/  809] | ms/batch 250.20 | loss  1.92 | val loss  3.95\n","| epoch  70 | [  799/  809] | ms/batch 249.19 | loss  1.94 | val loss  3.88\n","| epoch  71 | [  199/  809] | ms/batch 249.53 | loss  1.75 | val loss  3.84\n","| epoch  71 | [  399/  809] | ms/batch 249.53 | loss  1.52 | val loss  3.86\n","| epoch  71 | [  599/  809] | ms/batch 249.23 | loss  1.99 | val loss  3.97\n","| epoch  71 | [  799/  809] | ms/batch 249.74 | loss  1.91 | val loss  3.99\n","| epoch  72 | [  199/  809] | ms/batch 249.62 | loss  1.71 | val loss  3.96\n","| epoch  72 | [  399/  809] | ms/batch 250.31 | loss  1.49 | val loss  4.03\n","| epoch  72 | [  599/  809] | ms/batch 250.12 | loss  1.88 | val loss  3.86\n","| epoch  72 | [  799/  809] | ms/batch 251.47 | loss  1.90 | val loss  3.94\n","| epoch  73 | [  199/  809] | ms/batch 251.42 | loss  1.75 | val loss  3.96\n","| epoch  73 | [  399/  809] | ms/batch 251.33 | loss  1.46 | val loss  3.97\n","| epoch  73 | [  599/  809] | ms/batch 250.63 | loss  1.87 | val loss  3.93\n","| epoch  73 | [  799/  809] | ms/batch 250.89 | loss  1.95 | val loss  4.00\n","| epoch  74 | [  199/  809] | ms/batch 250.82 | loss  1.68 | val loss  3.89\n","| epoch  74 | [  399/  809] | ms/batch 251.37 | loss  1.54 | val loss  3.97\n","| epoch  74 | [  599/  809] | ms/batch 251.59 | loss  1.83 | val loss  3.92\n","| epoch  74 | [  799/  809] | ms/batch 251.05 | loss  1.82 | val loss  3.94\n","| epoch  75 | [  199/  809] | ms/batch 251.75 | loss  1.65 | val loss  3.95\n","| epoch  75 | [  399/  809] | ms/batch 251.72 | loss  1.46 | val loss  3.87\n","| epoch  75 | [  599/  809] | ms/batch 251.00 | loss  1.77 | val loss  3.99\n","| epoch  75 | [  799/  809] | ms/batch 250.83 | loss  1.78 | val loss  4.07\n","| epoch  76 | [  199/  809] | ms/batch 250.92 | loss  1.79 | val loss  3.85\n","| epoch  76 | [  399/  809] | ms/batch 251.62 | loss  1.53 | val loss  3.98\n","| epoch  76 | [  599/  809] | ms/batch 251.40 | loss  1.73 | val loss  3.94\n","| epoch  76 | [  799/  809] | ms/batch 251.58 | loss  1.76 | val loss  3.80\n","| epoch  77 | [  199/  809] | ms/batch 252.01 | loss  1.64 | val loss  3.93\n","| epoch  77 | [  399/  809] | ms/batch 252.35 | loss  1.42 | val loss  3.89\n","| epoch  77 | [  599/  809] | ms/batch 250.78 | loss  1.71 | val loss  4.13\n","| epoch  77 | [  799/  809] | ms/batch 250.63 | loss  1.81 | val loss  4.06\n","| epoch  78 | [  199/  809] | ms/batch 250.29 | loss  1.63 | val loss  3.93\n","| epoch  78 | [  399/  809] | ms/batch 249.55 | loss  1.44 | val loss  4.15\n","| epoch  78 | [  599/  809] | ms/batch 250.80 | loss  1.75 | val loss  4.01\n","| epoch  78 | [  799/  809] | ms/batch 251.55 | loss  1.77 | val loss  4.00\n","| epoch  79 | [  199/  809] | ms/batch 252.15 | loss  1.54 | val loss  4.00\n","| epoch  79 | [  399/  809] | ms/batch 253.33 | loss  1.32 | val loss  3.87\n","| epoch  79 | [  599/  809] | ms/batch 253.50 | loss  1.66 | val loss  3.78\n","| epoch  79 | [  799/  809] | ms/batch 252.37 | loss  1.74 | val loss  3.97\n","| epoch  80 | [  199/  809] | ms/batch 251.57 | loss  1.71 | val loss  3.96\n","| epoch  80 | [  399/  809] | ms/batch 250.72 | loss  1.30 | val loss  3.86\n","| epoch  80 | [  599/  809] | ms/batch 251.33 | loss  1.71 | val loss  4.08\n","| epoch  80 | [  799/  809] | ms/batch 252.12 | loss  1.78 | val loss  4.02\n","| epoch  81 | [  199/  809] | ms/batch 251.33 | loss  1.61 | val loss  4.03\n","| epoch  81 | [  399/  809] | ms/batch 250.66 | loss  1.37 | val loss  4.14\n","| epoch  81 | [  599/  809] | ms/batch 251.60 | loss  1.64 | val loss  3.93\n","| epoch  81 | [  799/  809] | ms/batch 251.08 | loss  1.73 | val loss  4.02\n","| epoch  82 | [  199/  809] | ms/batch 251.74 | loss  1.54 | val loss  4.02\n","| epoch  82 | [  399/  809] | ms/batch 250.84 | loss  1.30 | val loss  4.11\n","| epoch  82 | [  599/  809] | ms/batch 251.03 | loss  1.68 | val loss  4.00\n","| epoch  82 | [  799/  809] | ms/batch 251.12 | loss  1.73 | val loss  3.99\n","| epoch  83 | [  199/  809] | ms/batch 250.99 | loss  1.51 | val loss  4.09\n","| epoch  83 | [  399/  809] | ms/batch 251.23 | loss  1.36 | val loss  3.99\n","| epoch  83 | [  599/  809] | ms/batch 251.55 | loss  1.67 | val loss  4.11\n","| epoch  83 | [  799/  809] | ms/batch 251.49 | loss  1.74 | val loss  4.15\n","| epoch  84 | [  199/  809] | ms/batch 251.66 | loss  1.46 | val loss  4.18\n","| epoch  84 | [  399/  809] | ms/batch 251.31 | loss  1.27 | val loss  4.08\n","| epoch  84 | [  599/  809] | ms/batch 252.29 | loss  1.60 | val loss  4.13\n","| epoch  84 | [  799/  809] | ms/batch 251.58 | loss  1.61 | val loss  4.06\n","| epoch  85 | [  199/  809] | ms/batch 251.81 | loss  1.47 | val loss  4.08\n","| epoch  85 | [  399/  809] | ms/batch 251.35 | loss  1.30 | val loss  4.19\n","| epoch  85 | [  599/  809] | ms/batch 251.59 | loss  1.53 | val loss  4.10\n","| epoch  85 | [  799/  809] | ms/batch 251.28 | loss  1.67 | val loss  4.20\n","| epoch  86 | [  199/  809] | ms/batch 252.41 | loss  1.44 | val loss  4.23\n","| epoch  86 | [  399/  809] | ms/batch 251.66 | loss  1.33 | val loss  4.16\n","| epoch  86 | [  599/  809] | ms/batch 252.09 | loss  1.59 | val loss  4.19\n","| epoch  86 | [  799/  809] | ms/batch 251.43 | loss  1.65 | val loss  4.20\n","| epoch  87 | [  199/  809] | ms/batch 251.34 | loss  1.44 | val loss  4.07\n","| epoch  87 | [  399/  809] | ms/batch 251.70 | loss  1.16 | val loss  4.12\n","| epoch  87 | [  599/  809] | ms/batch 251.70 | loss  1.58 | val loss  4.15\n","| epoch  87 | [  799/  809] | ms/batch 251.56 | loss  1.71 | val loss  4.08\n","| epoch  88 | [  199/  809] | ms/batch 252.05 | loss  1.47 | val loss  4.17\n","| epoch  88 | [  399/  809] | ms/batch 251.66 | loss  1.26 | val loss  4.11\n","| epoch  88 | [  599/  809] | ms/batch 251.84 | loss  1.44 | val loss  4.19\n","| epoch  88 | [  799/  809] | ms/batch 251.90 | loss  1.63 | val loss  4.17\n","| epoch  89 | [  199/  809] | ms/batch 251.65 | loss  1.44 | val loss  4.13\n","| epoch  89 | [  399/  809] | ms/batch 251.53 | loss  1.25 | val loss  4.21\n","| epoch  89 | [  599/  809] | ms/batch 251.30 | loss  1.47 | val loss  4.16\n","| epoch  89 | [  799/  809] | ms/batch 250.90 | loss  1.59 | val loss  4.13\n","| epoch  90 | [  199/  809] | ms/batch 250.36 | loss  1.37 | val loss  4.07\n","| epoch  90 | [  399/  809] | ms/batch 251.18 | loss  1.21 | val loss  4.10\n","| epoch  90 | [  599/  809] | ms/batch 251.68 | loss  1.55 | val loss  4.24\n","| epoch  90 | [  799/  809] | ms/batch 251.56 | loss  1.64 | val loss  4.24\n","| epoch  91 | [  199/  809] | ms/batch 251.86 | loss  1.38 | val loss  4.28\n","| epoch  91 | [  399/  809] | ms/batch 251.44 | loss  1.21 | val loss  4.12\n","| epoch  91 | [  599/  809] | ms/batch 251.26 | loss  1.57 | val loss  4.21\n","| epoch  91 | [  799/  809] | ms/batch 250.69 | loss  1.54 | val loss  4.38\n","| epoch  92 | [  199/  809] | ms/batch 251.13 | loss  1.40 | val loss  4.18\n","| epoch  92 | [  399/  809] | ms/batch 250.88 | loss  1.18 | val loss  4.17\n","| epoch  92 | [  599/  809] | ms/batch 251.72 | loss  1.53 | val loss  4.30\n","| epoch  92 | [  799/  809] | ms/batch 251.44 | loss  1.53 | val loss  4.33\n","| epoch  93 | [  199/  809] | ms/batch 251.50 | loss  1.39 | val loss  4.31\n","| epoch  93 | [  399/  809] | ms/batch 251.31 | loss  1.21 | val loss  4.39\n","| epoch  93 | [  599/  809] | ms/batch 250.86 | loss  1.45 | val loss  4.34\n","| epoch  93 | [  799/  809] | ms/batch 253.53 | loss  1.53 | val loss  4.32\n","| epoch  94 | [  199/  809] | ms/batch 250.98 | loss  1.38 | val loss  4.22\n","| epoch  94 | [  399/  809] | ms/batch 250.44 | loss  1.16 | val loss  4.20\n","| epoch  94 | [  599/  809] | ms/batch 250.51 | loss  1.44 | val loss  4.27\n","| epoch  94 | [  799/  809] | ms/batch 251.86 | loss  1.47 | val loss  4.25\n","| epoch  95 | [  199/  809] | ms/batch 251.20 | loss  1.29 | val loss  4.18\n","| epoch  95 | [  399/  809] | ms/batch 249.93 | loss  1.13 | val loss  4.40\n","| epoch  95 | [  599/  809] | ms/batch 249.58 | loss  1.49 | val loss  4.38\n","| epoch  95 | [  799/  809] | ms/batch 249.34 | loss  1.46 | val loss  4.24\n","| epoch  96 | [  199/  809] | ms/batch 249.30 | loss  1.39 | val loss  4.16\n","| epoch  96 | [  399/  809] | ms/batch 249.10 | loss  1.11 | val loss  4.34\n","| epoch  96 | [  599/  809] | ms/batch 249.12 | loss  1.47 | val loss  4.36\n","| epoch  96 | [  799/  809] | ms/batch 248.94 | loss  1.49 | val loss  4.39\n","| epoch  97 | [  199/  809] | ms/batch 249.67 | loss  1.29 | val loss  4.42\n","| epoch  97 | [  399/  809] | ms/batch 249.54 | loss  1.21 | val loss  4.38\n","| epoch  97 | [  599/  809] | ms/batch 248.91 | loss  1.41 | val loss  4.36\n","| epoch  97 | [  799/  809] | ms/batch 248.61 | loss  1.52 | val loss  4.44\n","| epoch  98 | [  199/  809] | ms/batch 248.58 | loss  1.32 | val loss  4.33\n","| epoch  98 | [  399/  809] | ms/batch 248.54 | loss  1.09 | val loss  4.44\n","| epoch  98 | [  599/  809] | ms/batch 249.36 | loss  1.40 | val loss  4.41\n","| epoch  98 | [  799/  809] | ms/batch 248.79 | loss  1.46 | val loss  4.13\n","| epoch  99 | [  199/  809] | ms/batch 248.92 | loss  1.25 | val loss  4.37\n","| epoch  99 | [  399/  809] | ms/batch 249.64 | loss  1.20 | val loss  4.45\n","| epoch  99 | [  599/  809] | ms/batch 249.12 | loss  1.49 | val loss  4.39\n","| epoch  99 | [  799/  809] | ms/batch 250.05 | loss  1.36 | val loss  4.29\n","| epoch 100 | [  199/  809] | ms/batch 253.89 | loss  1.35 | val loss  4.44\n","| epoch 100 | [  399/  809] | ms/batch 251.82 | loss  1.06 | val loss  4.51\n","| epoch 100 | [  599/  809] | ms/batch 251.18 | loss  1.40 | val loss  4.42\n","| epoch 100 | [  799/  809] | ms/batch 251.25 | loss  1.47 | val loss  4.50\n","| epoch 101 | [  199/  809] | ms/batch 251.93 | loss  1.16 | val loss  4.47\n","| epoch 101 | [  399/  809] | ms/batch 252.32 | loss  1.09 | val loss  4.49\n","| epoch 101 | [  599/  809] | ms/batch 250.29 | loss  1.30 | val loss  4.41\n","| epoch 101 | [  799/  809] | ms/batch 249.74 | loss  1.50 | val loss  4.46\n","| epoch 102 | [  199/  809] | ms/batch 250.10 | loss  1.20 | val loss  4.51\n","| epoch 102 | [  399/  809] | ms/batch 249.39 | loss  1.00 | val loss  4.44\n","| epoch 102 | [  599/  809] | ms/batch 249.42 | loss  1.32 | val loss  4.73\n","| epoch 102 | [  799/  809] | ms/batch 249.96 | loss  1.46 | val loss  4.39\n","| epoch 103 | [  199/  809] | ms/batch 250.28 | loss  1.27 | val loss  4.47\n","| epoch 103 | [  399/  809] | ms/batch 253.46 | loss  1.03 | val loss  4.40\n","| epoch 103 | [  599/  809] | ms/batch 252.96 | loss  1.31 | val loss  4.57\n","| epoch 103 | [  799/  809] | ms/batch 251.83 | loss  1.30 | val loss  4.51\n","| epoch 104 | [  199/  809] | ms/batch 252.81 | loss  1.29 | val loss  4.79\n","| epoch 104 | [  399/  809] | ms/batch 253.54 | loss  1.06 | val loss  4.48\n","| epoch 104 | [  599/  809] | ms/batch 251.45 | loss  1.23 | val loss  4.69\n","| epoch 104 | [  799/  809] | ms/batch 251.20 | loss  1.33 | val loss  4.48\n","| epoch 105 | [  199/  809] | ms/batch 251.92 | loss  1.20 | val loss  4.54\n","| epoch 105 | [  399/  809] | ms/batch 253.12 | loss  1.03 | val loss  4.44\n","| epoch 105 | [  599/  809] | ms/batch 253.23 | loss  1.36 | val loss  4.66\n","| epoch 105 | [  799/  809] | ms/batch 253.36 | loss  1.42 | val loss  4.62\n","| epoch 106 | [  199/  809] | ms/batch 254.20 | loss  1.21 | val loss  4.52\n","| epoch 106 | [  399/  809] | ms/batch 253.41 | loss  1.00 | val loss  4.69\n","| epoch 106 | [  599/  809] | ms/batch 253.41 | loss  1.40 | val loss  4.55\n","| epoch 106 | [  799/  809] | ms/batch 253.21 | loss  1.30 | val loss  4.61\n","| epoch 107 | [  199/  809] | ms/batch 253.14 | loss  1.14 | val loss  4.53\n","| epoch 107 | [  399/  809] | ms/batch 252.68 | loss  0.94 | val loss  4.47\n","| epoch 107 | [  599/  809] | ms/batch 253.15 | loss  1.22 | val loss  4.62\n","| epoch 107 | [  799/  809] | ms/batch 253.20 | loss  1.38 | val loss  4.60\n","| epoch 108 | [  199/  809] | ms/batch 253.96 | loss  1.20 | val loss  4.61\n","| epoch 108 | [  399/  809] | ms/batch 254.74 | loss  0.96 | val loss  4.60\n","| epoch 108 | [  599/  809] | ms/batch 254.39 | loss  1.31 | val loss  4.74\n","| epoch 108 | [  799/  809] | ms/batch 254.70 | loss  1.32 | val loss  4.68\n","| epoch 109 | [  199/  809] | ms/batch 255.47 | loss  1.06 | val loss  4.77\n","| epoch 109 | [  399/  809] | ms/batch 255.64 | loss  0.98 | val loss  4.74\n","| epoch 109 | [  599/  809] | ms/batch 254.73 | loss  1.20 | val loss  4.82\n","| epoch 109 | [  799/  809] | ms/batch 253.91 | loss  1.26 | val loss  4.69\n","| epoch 110 | [  199/  809] | ms/batch 254.37 | loss  1.11 | val loss  4.59\n","| epoch 110 | [  399/  809] | ms/batch 254.40 | loss  0.93 | val loss  4.56\n","| epoch 110 | [  599/  809] | ms/batch 254.44 | loss  1.21 | val loss  4.52\n","| epoch 110 | [  799/  809] | ms/batch 254.45 | loss  1.25 | val loss  4.69\n","| epoch 111 | [  199/  809] | ms/batch 254.01 | loss  1.05 | val loss  4.81\n","| epoch 111 | [  399/  809] | ms/batch 254.18 | loss  0.94 | val loss  4.66\n","| epoch 111 | [  599/  809] | ms/batch 254.13 | loss  1.19 | val loss  4.62\n","| epoch 111 | [  799/  809] | ms/batch 254.31 | loss  1.25 | val loss  4.50\n","| epoch 112 | [  199/  809] | ms/batch 253.69 | loss  1.17 | val loss  4.57\n","| epoch 112 | [  399/  809] | ms/batch 255.82 | loss  0.85 | val loss  4.64\n","| epoch 112 | [  599/  809] | ms/batch 255.28 | loss  1.19 | val loss  4.75\n","| epoch 112 | [  799/  809] | ms/batch 254.37 | loss  1.16 | val loss  4.50\n","| epoch 113 | [  199/  809] | ms/batch 254.04 | loss  0.96 | val loss  4.69\n","| epoch 113 | [  399/  809] | ms/batch 253.47 | loss  0.89 | val loss  4.81\n","| epoch 113 | [  599/  809] | ms/batch 253.47 | loss  1.20 | val loss  4.57\n","| epoch 113 | [  799/  809] | ms/batch 253.57 | loss  1.13 | val loss  4.79\n","| epoch 114 | [  199/  809] | ms/batch 254.16 | loss  1.12 | val loss  4.66\n","| epoch 114 | [  399/  809] | ms/batch 254.66 | loss  0.98 | val loss  4.60\n","| epoch 114 | [  599/  809] | ms/batch 254.98 | loss  1.11 | val loss  4.73\n","| epoch 114 | [  799/  809] | ms/batch 254.46 | loss  1.23 | val loss  4.77\n","| epoch 115 | [  199/  809] | ms/batch 254.21 | loss  1.03 | val loss  4.71\n","| epoch 115 | [  399/  809] | ms/batch 254.50 | loss  0.94 | val loss  4.83\n","| epoch 115 | [  599/  809] | ms/batch 254.78 | loss  1.09 | val loss  4.62\n","| epoch 115 | [  799/  809] | ms/batch 254.55 | loss  1.29 | val loss  4.99\n","| epoch 116 | [  199/  809] | ms/batch 253.96 | loss  1.09 | val loss  4.66\n","| epoch 116 | [  399/  809] | ms/batch 254.55 | loss  0.90 | val loss  4.94\n","| epoch 116 | [  599/  809] | ms/batch 254.53 | loss  1.13 | val loss  4.82\n","| epoch 116 | [  799/  809] | ms/batch 255.17 | loss  1.15 | val loss  4.72\n","| epoch 117 | [  199/  809] | ms/batch 255.06 | loss  1.02 | val loss  4.99\n","| epoch 117 | [  399/  809] | ms/batch 254.73 | loss  0.81 | val loss  4.90\n","| epoch 117 | [  599/  809] | ms/batch 254.55 | loss  1.12 | val loss  4.61\n","| epoch 117 | [  799/  809] | ms/batch 254.72 | loss  1.24 | val loss  4.71\n","| epoch 118 | [  199/  809] | ms/batch 254.26 | loss  1.12 | val loss  4.96\n","| epoch 118 | [  399/  809] | ms/batch 254.32 | loss  0.79 | val loss  4.72\n","| epoch 118 | [  599/  809] | ms/batch 254.68 | loss  1.11 | val loss  4.90\n","| epoch 118 | [  799/  809] | ms/batch 253.77 | loss  1.15 | val loss  4.93\n","| epoch 119 | [  199/  809] | ms/batch 252.35 | loss  0.97 | val loss  4.96\n","| epoch 119 | [  399/  809] | ms/batch 251.46 | loss  0.84 | val loss  5.00\n","| epoch 119 | [  599/  809] | ms/batch 252.37 | loss  1.05 | val loss  4.91\n","| epoch 119 | [  799/  809] | ms/batch 252.33 | loss  1.23 | val loss  4.86\n","| epoch 120 | [  199/  809] | ms/batch 251.89 | loss  1.11 | val loss  4.85\n","| epoch 120 | [  399/  809] | ms/batch 251.59 | loss  0.83 | val loss  4.93\n","| epoch 120 | [  599/  809] | ms/batch 251.96 | loss  1.01 | val loss  5.01\n","| epoch 120 | [  799/  809] | ms/batch 251.72 | loss  1.14 | val loss  4.90\n","| epoch 121 | [  199/  809] | ms/batch 251.70 | loss  0.96 | val loss  4.91\n","| epoch 121 | [  399/  809] | ms/batch 253.58 | loss  0.77 | val loss  4.83\n","| epoch 121 | [  599/  809] | ms/batch 252.04 | loss  1.02 | val loss  4.95\n","| epoch 121 | [  799/  809] | ms/batch 251.62 | loss  1.15 | val loss  4.83\n","| epoch 122 | [  199/  809] | ms/batch 251.83 | loss  1.03 | val loss  4.79\n","| epoch 122 | [  399/  809] | ms/batch 251.84 | loss  0.79 | val loss  5.05\n","| epoch 122 | [  599/  809] | ms/batch 251.83 | loss  1.08 | val loss  4.97\n","| epoch 122 | [  799/  809] | ms/batch 251.10 | loss  1.02 | val loss  4.81\n","| epoch 123 | [  199/  809] | ms/batch 253.25 | loss  0.97 | val loss  4.90\n","| epoch 123 | [  399/  809] | ms/batch 253.74 | loss  0.78 | val loss  5.06\n","| epoch 123 | [  599/  809] | ms/batch 253.01 | loss  0.94 | val loss  4.85\n","| epoch 123 | [  799/  809] | ms/batch 251.54 | loss  1.05 | val loss  4.96\n","| epoch 124 | [  199/  809] | ms/batch 251.79 | loss  0.89 | val loss  4.90\n","| epoch 124 | [  399/  809] | ms/batch 251.67 | loss  0.83 | val loss  5.06\n","| epoch 124 | [  599/  809] | ms/batch 252.75 | loss  0.99 | val loss  5.10\n","| epoch 124 | [  799/  809] | ms/batch 252.40 | loss  1.11 | val loss  4.86\n","| epoch 125 | [  199/  809] | ms/batch 253.63 | loss  0.94 | val loss  4.95\n","| epoch 125 | [  399/  809] | ms/batch 252.45 | loss  0.76 | val loss  4.88\n","| epoch 125 | [  599/  809] | ms/batch 252.12 | loss  1.03 | val loss  4.84\n","| epoch 125 | [  799/  809] | ms/batch 253.25 | loss  1.00 | val loss  4.86\n","| epoch 126 | [  199/  809] | ms/batch 252.14 | loss  0.96 | val loss  4.96\n","| epoch 126 | [  399/  809] | ms/batch 251.44 | loss  0.78 | val loss  4.98\n","| epoch 126 | [  599/  809] | ms/batch 251.48 | loss  1.05 | val loss  4.68\n","| epoch 126 | [  799/  809] | ms/batch 251.87 | loss  1.09 | val loss  4.63\n","| epoch 127 | [  199/  809] | ms/batch 251.56 | loss  0.99 | val loss  5.05\n","| epoch 127 | [  399/  809] | ms/batch 250.74 | loss  0.77 | val loss  5.12\n","| epoch 127 | [  599/  809] | ms/batch 248.91 | loss  0.97 | val loss  4.99\n","| epoch 127 | [  799/  809] | ms/batch 249.92 | loss  0.99 | val loss  4.98\n","| epoch 128 | [  199/  809] | ms/batch 248.81 | loss  0.92 | val loss  4.84\n","| epoch 128 | [  399/  809] | ms/batch 251.36 | loss  0.71 | val loss  4.80\n","| epoch 128 | [  599/  809] | ms/batch 249.71 | loss  0.94 | val loss  5.08\n","| epoch 128 | [  799/  809] | ms/batch 250.72 | loss  1.07 | val loss  5.26\n","| epoch 129 | [  199/  809] | ms/batch 251.70 | loss  0.85 | val loss  4.82\n","| epoch 129 | [  399/  809] | ms/batch 249.34 | loss  0.84 | val loss  5.04\n","| epoch 129 | [  599/  809] | ms/batch 248.55 | loss  0.95 | val loss  5.12\n","| epoch 129 | [  799/  809] | ms/batch 249.89 | loss  1.04 | val loss  4.94\n","| epoch 130 | [  199/  809] | ms/batch 250.70 | loss  0.83 | val loss  5.03\n","| epoch 130 | [  399/  809] | ms/batch 250.10 | loss  0.75 | val loss  5.04\n","| epoch 130 | [  599/  809] | ms/batch 249.77 | loss  0.99 | val loss  5.31\n","| epoch 130 | [  799/  809] | ms/batch 251.69 | loss  0.99 | val loss  5.26\n","| epoch 131 | [  199/  809] | ms/batch 250.53 | loss  0.80 | val loss  5.08\n","| epoch 131 | [  399/  809] | ms/batch 251.14 | loss  0.70 | val loss  5.27\n","| epoch 131 | [  599/  809] | ms/batch 250.77 | loss  0.88 | val loss  4.89\n","| epoch 131 | [  799/  809] | ms/batch 249.82 | loss  0.92 | val loss  5.07\n","| epoch 132 | [  199/  809] | ms/batch 249.98 | loss  0.88 | val loss  4.98\n","| epoch 132 | [  399/  809] | ms/batch 250.55 | loss  0.62 | val loss  5.25\n","| epoch 132 | [  599/  809] | ms/batch 249.97 | loss  0.92 | val loss  5.06\n","| epoch 132 | [  799/  809] | ms/batch 250.20 | loss  0.92 | val loss  4.97\n","| epoch 133 | [  199/  809] | ms/batch 250.59 | loss  0.74 | val loss  5.01\n","| epoch 133 | [  399/  809] | ms/batch 249.84 | loss  0.66 | val loss  5.21\n","| epoch 133 | [  599/  809] | ms/batch 249.92 | loss  0.94 | val loss  5.16\n","| epoch 133 | [  799/  809] | ms/batch 250.18 | loss  0.94 | val loss  5.20\n","| epoch 134 | [  199/  809] | ms/batch 249.95 | loss  0.76 | val loss  5.05\n","| epoch 134 | [  399/  809] | ms/batch 249.90 | loss  0.75 | val loss  5.31\n","| epoch 134 | [  599/  809] | ms/batch 250.97 | loss  0.95 | val loss  5.29\n","| epoch 134 | [  799/  809] | ms/batch 250.33 | loss  1.01 | val loss  4.89\n","| epoch 135 | [  199/  809] | ms/batch 251.11 | loss  1.02 | val loss  5.27\n","| epoch 135 | [  399/  809] | ms/batch 250.68 | loss  0.66 | val loss  5.20\n","| epoch 135 | [  599/  809] | ms/batch 249.81 | loss  0.99 | val loss  5.40\n","| epoch 135 | [  799/  809] | ms/batch 250.57 | loss  0.89 | val loss  5.33\n","| epoch 136 | [  199/  809] | ms/batch 249.57 | loss  0.85 | val loss  5.39\n","| epoch 136 | [  399/  809] | ms/batch 250.12 | loss  0.64 | val loss  5.24\n","| epoch 136 | [  599/  809] | ms/batch 249.52 | loss  0.91 | val loss  5.50\n","| epoch 136 | [  799/  809] | ms/batch 250.59 | loss  0.88 | val loss  5.27\n","| epoch 137 | [  199/  809] | ms/batch 249.46 | loss  0.85 | val loss  5.08\n","| epoch 137 | [  399/  809] | ms/batch 250.02 | loss  0.73 | val loss  5.32\n","| epoch 137 | [  599/  809] | ms/batch 250.31 | loss  0.94 | val loss  5.19\n","| epoch 137 | [  799/  809] | ms/batch 249.81 | loss  0.97 | val loss  5.35\n","| epoch 138 | [  199/  809] | ms/batch 250.03 | loss  0.87 | val loss  5.16\n","| epoch 138 | [  399/  809] | ms/batch 250.17 | loss  0.68 | val loss  5.22\n","| epoch 138 | [  599/  809] | ms/batch 249.58 | loss  0.80 | val loss  5.23\n","| epoch 138 | [  799/  809] | ms/batch 250.02 | loss  0.82 | val loss  5.32\n","| epoch 139 | [  199/  809] | ms/batch 251.16 | loss  0.80 | val loss  5.35\n","| epoch 139 | [  399/  809] | ms/batch 249.76 | loss  0.60 | val loss  5.01\n","| epoch 139 | [  599/  809] | ms/batch 249.84 | loss  0.76 | val loss  5.34\n","| epoch 139 | [  799/  809] | ms/batch 250.25 | loss  0.88 | val loss  5.22\n","| epoch 140 | [  199/  809] | ms/batch 252.25 | loss  0.71 | val loss  5.32\n","| epoch 140 | [  399/  809] | ms/batch 251.51 | loss  0.56 | val loss  5.26\n","| epoch 140 | [  599/  809] | ms/batch 250.28 | loss  0.82 | val loss  5.58\n","| epoch 140 | [  799/  809] | ms/batch 249.29 | loss  0.93 | val loss  5.09\n","| epoch 141 | [  199/  809] | ms/batch 247.35 | loss  0.83 | val loss  5.43\n","| epoch 141 | [  399/  809] | ms/batch 247.52 | loss  0.68 | val loss  5.41\n","| epoch 141 | [  599/  809] | ms/batch 248.28 | loss  0.74 | val loss  5.46\n","| epoch 141 | [  799/  809] | ms/batch 248.08 | loss  0.88 | val loss  5.18\n","| epoch 142 | [  199/  809] | ms/batch 249.82 | loss  0.69 | val loss  5.22\n","| epoch 142 | [  399/  809] | ms/batch 250.52 | loss  0.67 | val loss  5.24\n","| epoch 142 | [  599/  809] | ms/batch 249.80 | loss  0.69 | val loss  5.21\n","| epoch 142 | [  799/  809] | ms/batch 252.28 | loss  0.77 | val loss  5.42\n","| epoch 143 | [  199/  809] | ms/batch 252.01 | loss  0.71 | val loss  5.50\n","| epoch 143 | [  399/  809] | ms/batch 249.89 | loss  0.65 | val loss  5.38\n","| epoch 143 | [  599/  809] | ms/batch 251.49 | loss  0.78 | val loss  5.43\n","| epoch 143 | [  799/  809] | ms/batch 252.72 | loss  0.86 | val loss  5.32\n","| epoch 144 | [  199/  809] | ms/batch 252.56 | loss  0.77 | val loss  5.26\n","| epoch 144 | [  399/  809] | ms/batch 252.21 | loss  0.61 | val loss  5.54\n","| epoch 144 | [  599/  809] | ms/batch 252.36 | loss  0.80 | val loss  5.62\n","| epoch 144 | [  799/  809] | ms/batch 251.27 | loss  0.80 | val loss  5.32\n","| epoch 145 | [  199/  809] | ms/batch 250.99 | loss  0.76 | val loss  5.29\n","| epoch 145 | [  399/  809] | ms/batch 250.10 | loss  0.63 | val loss  5.46\n","| epoch 145 | [  599/  809] | ms/batch 250.40 | loss  0.77 | val loss  5.45\n","| epoch 145 | [  799/  809] | ms/batch 248.92 | loss  0.98 | val loss  5.22\n","| epoch 146 | [  199/  809] | ms/batch 249.29 | loss  0.73 | val loss  5.31\n","| epoch 146 | [  399/  809] | ms/batch 249.52 | loss  0.57 | val loss  5.49\n","| epoch 146 | [  599/  809] | ms/batch 248.87 | loss  0.76 | val loss  5.29\n","| epoch 146 | [  799/  809] | ms/batch 249.39 | loss  0.78 | val loss  5.40\n","| epoch 147 | [  199/  809] | ms/batch 249.74 | loss  0.71 | val loss  5.46\n","| epoch 147 | [  399/  809] | ms/batch 249.19 | loss  0.62 | val loss  5.51\n","| epoch 147 | [  599/  809] | ms/batch 249.66 | loss  0.84 | val loss  5.53\n","| epoch 147 | [  799/  809] | ms/batch 248.74 | loss  0.77 | val loss  5.41\n","| epoch 148 | [  199/  809] | ms/batch 251.35 | loss  0.73 | val loss  5.56\n","| epoch 148 | [  399/  809] | ms/batch 249.05 | loss  0.66 | val loss  5.45\n","| epoch 148 | [  599/  809] | ms/batch 249.06 | loss  0.69 | val loss  5.33\n","| epoch 148 | [  799/  809] | ms/batch 248.31 | loss  0.87 | val loss  5.32\n","| epoch 149 | [  199/  809] | ms/batch 249.31 | loss  0.71 | val loss  5.50\n","| epoch 149 | [  399/  809] | ms/batch 248.14 | loss  0.51 | val loss  5.57\n","| epoch 149 | [  599/  809] | ms/batch 250.74 | loss  0.73 | val loss  5.53\n","| epoch 149 | [  799/  809] | ms/batch 248.44 | loss  0.84 | val loss  5.47\n","Original: snug fit feels great\n","Predicted: will be great for spring rides or\n","\n","\n","Original: cute\n","Predicted: tate hand earrings exactly what i\n","\n","\n","Original: the product description is inaccurate\n","Predicted: senzor senzor senzor vi viper ok but\n","\n","\n","Original: glasses\n","Predicted: linguri linguri linguris are great\n","\n","\n","Original: true to size\n","Predicted: always a great shoe fit is\n","\n","\n","bleu, precisions, bp, ratio, translation_length, reference_length (0.0, [0.22144055961974557, 0.0, 0.0, 0.0], 1.0, 35.20658547586829, 780530, 22170)\n","bleau score 0.6859844838250574\n","rouge2 (0.16848225200975267, 0.15806677562456223, 0.18036717690370083)\n","rouge {'rouge_1/f_score': 0.04740721897690091, 'rouge_1/r_score': 0.06681084477092596, 'rouge_1/p_score': 0.04179674019660488, 'rouge_2/f_score': 0.005587580152682946, 'rouge_2/r_score': 0.009073214912186496, 'rouge_2/p_score': 0.004661490216294005, 'rouge_l/f_score': 0.036532753369628, 'rouge_l/r_score': 0.06430128659492665, 'rouge_l/p_score': 0.03582168095023305}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"M0Yc0HUt-zQK"},"source":["#function to view the output scores of T5 model this can be done for manual checking after txt file is generated\n","def view_t5_op():\n","  #get the final cleaned data\n","  df=pd.read_csv('/content/drive/MyDrive/product_reviews.csv')[:147799]\n","  print(\"The length of dataset is \",len(df))\n","  \n","  #set the threshold \n","  threshold = 20\n","  max_rl=80 #maximum review length\n","  max_sl=10 #maximum summary length\n","  \n","  #get reviewText whose length is less than maximum review length\n","  df['reviewText']=df['reviewText'].str.slice(0,max_rl)\n","  \n","  #get summary whose length is less than maximum summary length\n","  df['summary']=df['summary'].str.slice(0,max_rl)\n","\n","  f = open(\"/content/drive/MyDrive/TFIVE.txt\", \"r\")\n","  text=f.readlines()\n","  text=pd.DataFrame(text,columns=[\"value\"])\n","  text=text[\"value\"].str.split(\"\\t\",expand=True)\n","  text.columns=[\"predicted\",\"value\",\"original\"]\n","  text.drop(columns=[\"value\"],inplace=True)\n","  text[\"predicted\"]=text[\"predicted\"].str.split(\":\").str[1]\n","  text[\"original\"]=text[\"original\"].str.split(\":\").str[1]\n","  text[\"original\"]=text[\"original\"].replace('\\n','', regex=True)\n","  f.close()\n","\n","  bleau=compute_bleu(text[\"original\"],text[\"predicted\"], max_order=4,smooth=False)\n","  bscore=nltk.translate.bleu_score.corpus_bleu(text[\"original\"],text[\"predicted\"])\n","  rougen=rouge_n(text[\"predicted\"], text[\"original\"], n=2)\n","  ro=rouge(text[\"predicted\"],text[\"original\"])\n","\n","  print(\"bleu, precisions, bp, ratio, translation_length, reference_length\",bleau)\n","  print(\"bleau score\",bscore)\n","  print(\"rouge2\",rougen)\n","  print(\"rouge\",ro)\n","  return df,text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KdyLxZeSd0Pc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618668881783,"user_tz":-330,"elapsed":35602,"user":{"displayName":"devansh mody","photoUrl":"","userId":"11540078254175805123"}},"outputId":"9658cc52-a378-4242-f0cd-3b644a845144"},"source":["df,text=view_t5_op()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The length of dataset is  147799\n","bleu, precisions, bp, ratio, translation_length, reference_length (0.0, [0.2185841237246996, 0.0, 0.0, 0.0], 1.0, 36.200090211998194, 802556, 22170)\n","bleau score 0.6837615158224546\n","rouge2 (0.22406421765542467, 0.19950063644374816, 0.2555259445054084)\n","rouge {'rouge_1/f_score': 0.23601519307003388, 'rouge_1/r_score': 0.313584763479892, 'rouge_1/p_score': 0.20924413443019668, 'rouge_2/f_score': 0.0052723712383178, 'rouge_2/r_score': 0.0072777097607950115, 'rouge_2/p_score': 0.00455244825339548, 'rouge_l/f_score': 0.1833967729912532, 'rouge_l/r_score': 0.30963190608319163, 'rouge_l/p_score': 0.1774901912351168}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"V-sqiznJqYQD"},"source":["#function to view LSTM output scores of LSTM model this can be done for manual checking after txt file is generated\n","def view_lstm():\n","  f = open(\"/content/drive/MyDrive/LSTMscore.txt\", \"r\")\n","  text=f.readlines()\n","  text=pd.DataFrame(text,columns=[\"value\"])\n","  text=text[\"value\"].str.split(\"\\t\",expand=True)\n","  text.columns=[\"value\",\"original\",\"predicted\"]\n","  text[\"original\"]=text[\"original\"].str.split(\":\").str[1]\n","  text[\"predicted\"]=text[\"predicted\"].str.split(\":\").str[1]\n","  text[\"predicted\"]=text[\"predicted\"].replace('\\n','', regex=True)\n","  f.close()\n","  bleau=compute_bleu(text[\"original\"],text[\"predicted\"], max_order=4,smooth=False)\n","  bscore=nltk.translate.bleu_score.corpus_bleu(text[\"original\"],text[\"predicted\"])\n","  rougen=rouge_n(text[\"predicted\"], text[\"original\"], n=2)\n","  ro=rouge(text[\"predicted\"],text[\"original\"])\n","\n","  print(\"bleu, precisions, bp, ratio, translation_length, reference_length\",bleau)\n","  print(\"bleau score\",bscore)\n","  print(\"rouge2\",rougen)\n","  print(\"rouge\",ro)\n","  return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h2WAdQc7uJ1y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618668910357,"user_tz":-330,"elapsed":52140,"user":{"displayName":"devansh mody","photoUrl":"","userId":"11540078254175805123"}},"outputId":"8055249b-387a-4544-f57c-99c11d7cb467"},"source":["text=view_lstm()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["bleu, precisions, bp, ratio, translation_length, reference_length (0.0, [0.2900551776136539, 0.0, 0.0, 0.0], 1.0, 19.09509591394331, 466856, 24449)\n","bleau score 0.7338717254431542\n","rouge2 (0.06599443828484215, 0.8312236286919831, 0.03436126421544687)\n","rouge {'rouge_1/f_score': 0.36915419958439477, 'rouge_1/r_score': 0.36583216309687744, 'rouge_1/p_score': 0.41662537566051006, 'rouge_2/f_score': 0.2881554948360078, 'rouge_2/r_score': 0.33457293997806903, 'rouge_2/p_score': 0.2751092653511975, 'rouge_l/f_score': 0.6180779054304887, 'rouge_l/r_score': 0.6641052933641588, 'rouge_l/p_score': 0.5875250521493721}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cV9NzHzALqqQ"},"source":[""],"execution_count":null,"outputs":[]}]}