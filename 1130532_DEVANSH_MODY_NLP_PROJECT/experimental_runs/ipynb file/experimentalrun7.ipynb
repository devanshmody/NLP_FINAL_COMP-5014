{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 1130532_textsummarynlpproject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qELybmgHs2e1",
        "outputId": "9d0beb47-f7a3-4a15-b79a-00b602c04c23"
      },
      "source": [
        "#step1 import all the required libraries\n",
        "#install this version of transformers and pytorch\n",
        "!pip install transformers==2.8.0\n",
        "!pip install torch==1.4.0\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import tensorflow_datasets as tfds\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import nltk,spacy,re,string,random,time\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input,LSTM,Embedding,Dense,Concatenate,TimeDistributed,Bidirectional\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau\n",
        "from attension import AttentionLayer\n",
        "from keras.initializers import Constant\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K \n",
        "from rouge import rouge_n,rouge_l_sentence_level,rouge \n",
        "from bleau import compute_bleu\n",
        "#disable eager execution\n",
        "#tf.compat.v1.disable_eager_execution()\n",
        "#stopwords removal list\n",
        "nltk.download('stopwords')\n",
        "#punkt for tokenization\n",
        "nltk.download('punkt')\n",
        "#for tokenaizations\n",
        "nltk.download('wordnet')\n",
        "#combine all the stopwords and create one single list of stopwords\n",
        "s1=stopwords.words('english')\n",
        "s2=list(STOP_WORDS)\n",
        "s3=list(STOPWORDS)\n",
        "#final list of stopwords\n",
        "stop_words = s1+s2+s3\n",
        "#use cuda if available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#step2\n",
        "#contraction are used to replace words with their longer meaningfull counter parts \n",
        "contraction = { \n",
        "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
        "\"aren't\": \"are not / am not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he had / he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he shall / he will\",\n",
        "\"he'll've\": \"he shall have / he will have\",\n",
        "\"he's\": \"he has / he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how has / how is / how does\",\n",
        "\"I'd\": \"I had / I would\",\n",
        "\"I'd've\": \"I would have\",\n",
        "\"I'll\": \"I shall / I will\",\n",
        "\"I'll've\": \"I shall have / I will have\",\n",
        "\"I'm\": \"I am\",\n",
        "\"I've\": \"I have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it had / it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it shall / it will\",\n",
        "\"it'll've\": \"it shall have / it will have\",\n",
        "\"it's\": \"it has / it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she had / she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she shall / she will\",\n",
        "\"she'll've\": \"she shall have / she will have\",\n",
        "\"she's\": \"she has / she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so as / so is\",\n",
        "\"that'd\": \"that would / that had\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that has / that is\",\n",
        "\"there'd\": \"there had / there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there has / there is\",\n",
        "\"they'd\": \"they had / they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they shall / they will\",\n",
        "\"they'll've\": \"they shall have / they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we had / we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what shall / what will\",\n",
        "\"what'll've\": \"what shall have / what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what has / what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when has / when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where has / where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who shall / who will\",\n",
        "\"who'll've\": \"who shall have / who will have\",\n",
        "\"who's\": \"who has / who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why has / why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you had / you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you shall / you will\",\n",
        "\"you'll've\": \"you shall have / you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\",\n",
        "\"rec'd\": \"received\"\n",
        "}\n",
        "#rec'd this is my addition to the list of contractions\n",
        "\n",
        "#step3\n",
        "#process_text function is used to remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings\n",
        "def process_text(text,contractions,remove_stopwords = True):\n",
        "  #convert words to lower case\n",
        "  text = text.lower()\n",
        "    \n",
        "  #replace contractions with their longer forms \n",
        "  if True:\n",
        "    text = text.split()\n",
        "    new_text = []\n",
        "    for word in text:\n",
        "      if word in contractions:\n",
        "        new_text.append(contractions[word])\n",
        "      else:\n",
        "        new_text.append(word)\n",
        "    text = \" \".join(new_text)\n",
        "    \n",
        "  #format words and remove unwanted characters\n",
        "  text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE) #remove https string\n",
        "  text = re.sub(r'\\<a href', ' ', text) #remove hyperlink\n",
        "  text = re.sub(r'&amp;', '', text) #remove & in text\n",
        "  text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text) #remove unwanted charecters like puntuation and others\n",
        "  text = re.sub(r'<br />', ' ', text) #remove new line spaces\n",
        "  text = re.sub(r'\\'', ' ', text) #remove slashes\n",
        "  text = \" \".join(text.split()) #remove trailing spaces \n",
        "  #string.printable returns all sets of punctuation, digits, ascii_letters and whitespace.\n",
        "  printable = set(string.printable)\n",
        "  #filter to remove punctuations,digits, ascii_letters and whitespaces\n",
        "  text = \"\".join(list(filter(lambda x: x in printable, text))) \n",
        "  #remove stop words is true then remove stopwords also\n",
        "  if remove_stopwords:\n",
        "    text = text.split()\n",
        "    text = [w for w in text if not w in stop_words]\n",
        "    text = \" \".join(text)\n",
        "\n",
        "  return text\n",
        "\n",
        "#step4\n",
        "#get_data function gets the data from gz file into a dataframe and process the columns\n",
        "#stops are not removed for summary they are only removed from text this is done to get more human like summaries\n",
        "#after processing it returns a dataframe\n",
        "def get_data(contractions):\n",
        "  st=time.time()\n",
        "  #load the data into a dataframe\n",
        "  df = pd.read_json('/content/drive/MyDrive/reviews_Clothing_Shoes_and_Jewelry_5.json.gz', lines=True, compression='gzip')\n",
        "  #drop unwanted columns\n",
        "  df.drop(columns=['reviewerID', 'asin', 'reviewerName', 'helpful','overall','unixReviewTime', 'reviewTime'],inplace=True)\n",
        "  print(\"length of the data\",len(df))\n",
        "  #apply preprocess function on the columns of the dataframe\n",
        "  df['reviewText'] = df['reviewText'].apply(lambda x: process_text(x,contractions,remove_stopwords = True))\n",
        "  df['summary'] = df[ 'summary'].apply(lambda x: process_text(x,contractions,remove_stopwords = False))\n",
        "  #write preprocesssed data to csv file\n",
        "  df.to_csv(\"/content/drive/MyDrive/product_reviews.csv\",index=False)\n",
        "  print(\"total time to generate data and write in csv file \",time.time()-st)\n",
        "\n",
        "\n",
        "#step5\n",
        "#get_embeddings function is used to gett te word embeddings \n",
        "#i am using conceptual number batch word embeddings\n",
        "def get_embeddings():\n",
        "  #get word embeddings\n",
        "  embeddings_index = {}\n",
        "  with open('/content/drive/MyDrive/numberbatch-en-19.08.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "      values = line.split(' ')\n",
        "      word = values[0]\n",
        "      embedding = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = embedding\n",
        "\n",
        "  print('Word embeddings:', len(embeddings_index))\n",
        "  return embeddings_index\n",
        "\n",
        "#step6\n",
        "#this function is used to build vocabulary\n",
        "def get_vocab(embeddings_index,word_counts,threshold):\n",
        "  #get the number of missing words \n",
        "  missing_words={k:v for k,v in word_counts.items() if v >= threshold if k not in embeddings_index.keys()}\n",
        "  missing_ratio = round(len(missing_words)/len(word_counts),4)*100\n",
        "  print(\"Number of words missing from word_embeddings:\", len(missing_words))\n",
        "  print(\"Percent of words that are missing from our vocabulary: {}%\".format(missing_ratio))\n",
        "\n",
        "  #mapping vocab to index\n",
        "  lr=iter([item for item in range(0,len(word_counts))])\n",
        "  vocab_to_int={k:next(lr) for k,v in word_counts.items() if v >= threshold or k in embeddings_index.keys()}\n",
        "\n",
        "  #mapping index to vocab \n",
        "  lr=iter([item for item in range(0,len(word_counts))])\n",
        "  int_to_vocab={next(lr):k for k,v in word_counts.items() if v >= threshold or k in embeddings_index.keys()}\n",
        "\n",
        "  # Special tokens that will be added to our vocab\n",
        "  codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]\n",
        "\n",
        "  # Add codes to vocab\n",
        "  for code in codes:\n",
        "      vocab_to_int[code] = len(vocab_to_int)\n",
        "      int_to_vocab[len(int_to_vocab)] = code\n",
        "  \n",
        "  #print usage of words in our model and their percent\n",
        "  usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
        "  print(\"Total number of unique words:\", len(word_counts))\n",
        "  print(\"Number of words we will use:\", len(vocab_to_int))\n",
        "  print(\"Percent of words we will use: {}%\".format(usage_ratio))\n",
        "  print(\"length vocab_to_int\",len(vocab_to_int))\n",
        "  print(\"length int_to_vocab\",len(int_to_vocab))\n",
        "\n",
        "  return vocab_to_int,int_to_vocab\n",
        "\n",
        "#step7\n",
        "#function to map words with its word embeddings \n",
        "#if embeddings not found for the word then map it with a random number in range(-1.0,1.0)\n",
        "def word_embedding_index(vocab_to_int,embeddings_index):\n",
        "  #using 300 for embedding dimensions to match CN's vectors.\n",
        "  embedding_dim = 300\n",
        "  nb_words = len(vocab_to_int)\n",
        "  \n",
        "  # Create matrix with default values of zero\n",
        "  word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
        "  for word, i in vocab_to_int.items():\n",
        "    if word in embeddings_index:\n",
        "      word_embedding_matrix[i] = embeddings_index[word]\n",
        "    else:\n",
        "      # If word not in CN, create a random embedding for it\n",
        "      new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
        "      #embeddings_index[word] = new_embedding\n",
        "      word_embedding_matrix[i] = new_embedding\n",
        "\n",
        "  # Check if value matches len(vocab_to_int)\n",
        "  print(\"length of word embedding matrix\",len(word_embedding_matrix))\n",
        "  return word_embedding_matrix\n",
        "\n",
        "#step8\n",
        "#append unk and eos tokens \n",
        "#if eos is equal to true then append go and eos token at begining and end of the summary\n",
        "#add unknown token for word not found in vocabulary\n",
        "def convert_to_ints(text,vocab_to_int,eos=False):\n",
        "  ints = []\n",
        "  for word in text.split():\n",
        "    if word in vocab_to_int:\n",
        "      ints.append(vocab_to_int[word])\n",
        "    else:\n",
        "      ints.append(vocab_to_int[\"<UNK>\"])\n",
        "  if eos:\n",
        "    ints.insert(0,vocab_to_int[\"<GO>\"])\n",
        "    ints.insert(len(ints),vocab_to_int[\"<EOS>\"])\n",
        "  return ints\n",
        "\n",
        "#step9\n",
        "#count unknown tokens\n",
        "def count_unk(text):\n",
        "  unk=0\n",
        "  eos=0\n",
        "  #print(text)\n",
        "  for value in text:\n",
        "    if 41413 in value:\n",
        "      unk+=1\n",
        "  return unk\n",
        "\n",
        "#step10\n",
        "def counts(val):\n",
        "  c=0\n",
        "  for i in val:\n",
        "    try:\n",
        "      if i==41413:\n",
        "        c+=1\n",
        "    except:\n",
        "      pass\n",
        "  return c\n",
        "\n",
        "#step11\n",
        "#remove rows from data frame that dosent staisfy the condition this is done so model is trained with proper data\n",
        "#redundancey is less and input text is accurate\n",
        "def get_refined_output(df,max_rl,max_sl):\n",
        "  unk_rl=1 #unknown token review limit\n",
        "  unk_sl=0 #unknown token summary limit\n",
        "  min_rl=2 #minimum review length\n",
        "  #get the total length of reviewText this is used for sorting\n",
        "  df[\"total_length\"]=df['reviewText'].apply(lambda x: len(x))\n",
        "  #get reviewText whose length is greater then minimum review length \n",
        "  df=df[df['reviewText'].apply(lambda x: len(x)>=min_rl)]\n",
        "  #get reviewText whose length is less than maximum review length\n",
        "  df=df[df['reviewText'].apply(lambda x: len(x)<=max_rl)]\n",
        "  #filter out the unknwon tokens based on unknown token reviewText limit\n",
        "  df=df[df['reviewText'].apply(lambda x: counts(x)<=unk_rl)]\n",
        "  #get summary whose length is less than maximum summary length\n",
        "  df=df[df['summary'].apply(lambda x: len(x)<=max_sl)]\n",
        "  #filter out the unkown tokens based on unkown token summary limit\n",
        "  df=df[df['summary'].apply(lambda x: counts(x)<=unk_sl)]  \n",
        "  #sort the values in ascending order\n",
        "  df.sort_values(by=[\"total_length\"],ascending=True,inplace=True)\n",
        "  #drop unwanted columns\n",
        "  df.drop(columns=[\"total_length\",\"word\"],inplace=True)\n",
        "  #reset index\n",
        "  df.reset_index(drop=True,inplace=True)\n",
        "  return df \n",
        "\n",
        "#step12\n",
        "#function to plot the length of training, validation and testing\n",
        "def plot_tr_tval_tt_len(xtr,xval,xtt):\n",
        "  names = ['Training','Validation','Testing']\n",
        "  values = [len(xtr),len(xval),len(xtt)]\n",
        "  plt.figure(figsize=(10,5))\n",
        "  plt.subplot(131)\n",
        "  plt.bar(names,values,color=['darkorange','coral','coral'],edgecolor='darkblue')\n",
        "  plt.suptitle('Categorical Plotting')\n",
        "  plt.show()\n",
        "\n",
        "#step13\n",
        "#function to plot loss and accuracy curves on training and validation set\n",
        "def plotgraph(history):\n",
        "  plt.figure(figsize=[8,6])\n",
        "  plt.plot(history.history['loss'],'firebrick',linewidth=3.0)\n",
        "  plt.plot(history.history['accuracy'],'turquoise',linewidth=3.0)\n",
        "  plt.plot(history.history['val_loss'],'midnightblue',linewidth=3.0)\n",
        "  plt.legend(['Training loss','Training Accuracy','Validation loss'],fontsize=18)\n",
        "  plt.xlabel('Epochs',fontsize=16)\n",
        "  plt.ylabel('Loss and Accuracy',fontsize=16)\n",
        "  plt.title('Loss Curves and Accuracy Curves for text summarization',fontsize=16)\n",
        "\n",
        "#step14\n",
        "#this function is used to get the preprocessed csv file for our text summarizer\n",
        "def Get_the_data():\n",
        "  #lower the string in contractions and convert it into dict\n",
        "  contractions = dict((k.lower(), v.lower()) for k, v in contraction.items())\n",
        "  #till this step all data is processed and we get our csv file of cleaned texts\n",
        "  get_data(contractions)\n",
        "\n",
        "  #free memory\n",
        "  del contractions\n",
        "\n",
        "#step15 is used to call function Get_the_data which get the preprocessed data and writes it into a csv file\n",
        "#Get_the_data()\n",
        "\n",
        "#step16\n",
        "#this function combines all the above ouput generated by the above function in a proper squence of steps \n",
        "def combining_all_steps():\n",
        "  \n",
        "  st=time.time()\n",
        "  #get the final cleaned data\n",
        "  df=pd.read_csv('/content/drive/MyDrive/product_reviews.csv')[:180000]\n",
        "  print(\"The length of dataset is \",len(df))\n",
        "  #combine reviewText and summary so common vocabulary can be created by finding frequent words \n",
        "  df[\"word\"]=df[['reviewText','summary']].apply(lambda x : '{} {}'.format(x[0],x[1]), axis=1)\n",
        "  #get frequency of words\n",
        "  word_counts=pd.Series(np.concatenate([x.split() for x in df.word])).value_counts()\n",
        "  word_counts=word_counts.to_dict()\n",
        "  #print(type(word_counts))\n",
        "  print(\"vocab length\",len(word_counts))\n",
        "  #set the threshold \n",
        "  threshold = 20\n",
        "  max_rl=80 #maximum review length\n",
        "  max_sl=10 #maximum summary length\n",
        "  #get the embeddings matrix \n",
        "  embeddings_index= get_embeddings()\n",
        "  #get vocab to index and index to vocab mapping of words\n",
        "  vocab_to_int,int_to_vocab=get_vocab(embeddings_index,word_counts,threshold)\n",
        "  #get word embedding for the words in vocab\n",
        "  word_embedding_matrix=word_embedding_index(vocab_to_int,embeddings_index)\n",
        "  #convert words to integers based on their index positions\n",
        "  df['reviewText'] = df['reviewText'].apply(lambda x: convert_to_ints(str(x),vocab_to_int,eos=False))\n",
        "  df['summary'] = df[ 'summary'].apply(lambda x: convert_to_ints(str(x),vocab_to_int,eos=True))\n",
        "  print(\"after word to index for reviewText\",df[\"reviewText\"][0])\n",
        "  print(\"after word to index for summary\",df[\"summary\"][0])\n",
        "  rvunk=count_unk(df[\"reviewText\"])\n",
        "  smunk=count_unk(df[\"summary\"])\n",
        "  print(\"total number of unk token are\",rvunk+smunk)\n",
        "  #apply the filters and get the final preprocessed data\n",
        "  df=get_refined_output(df,max_rl,max_sl)\n",
        "  print(\"length of dataset that will be used\",len(df))\n",
        "  #split data into 75% train, 15% validation and 15% test datasets\n",
        "  x_tr,x_val,y_tr,y_val=train_test_split(df['reviewText'],df['summary'],test_size=0.3,random_state=1,shuffle=True)\n",
        "  x_tt,x_val,y_tt,y_val=train_test_split(x_val,y_val,test_size=0.5,random_state=1,shuffle=True)\n",
        "  print(\"length of split datasets train {}, test {} and validation {}\".format(len(x_tr),len(x_tt),len(x_val)))\n",
        "  print(\"Vocabulary Size: {}\".format(len(vocab_to_int)))\n",
        "  print(\"voc_to_int_\",vocab_to_int['<UNK>'],vocab_to_int['<PAD>'],vocab_to_int['<EOS>'])\n",
        "  #reset index\n",
        "  x_tr=x_tr.reset_index()\n",
        "  y_tr=y_tr.reset_index()\n",
        "  x_tt=x_tt.reset_index()\n",
        "  y_tt=y_tt.reset_index()\n",
        "  x_val=x_val.reset_index()\n",
        "  y_val=y_val.reset_index()\n",
        "  #find max lenght just to verfiy the output of get refined function\n",
        "  #max([len(sentence) for sentence in y_tt[\"summary\"]])\n",
        "  #pad the reviewText and summary to the specified max length\n",
        "  xtr=pad_sequences(x_tr[\"reviewText\"], padding='post',maxlen=max_rl, value=vocab_to_int[\"<PAD>\"])\n",
        "  ytr=pad_sequences(y_tr[\"summary\"], padding='post',maxlen=max_sl, value=vocab_to_int[\"<PAD>\"])\n",
        "  xtt=pad_sequences(x_tt[\"reviewText\"], padding='post',maxlen=max_rl, value=vocab_to_int[\"<PAD>\"])\n",
        "  ytt=pad_sequences(y_tt[\"summary\"], padding='post',maxlen=max_sl, value=vocab_to_int[\"<PAD>\"])\n",
        "  xval=pad_sequences(x_val[\"reviewText\"], padding='post',maxlen=max_rl, value=vocab_to_int[\"<PAD>\"])\n",
        "  yval=pad_sequences(y_val[\"summary\"], padding='post',maxlen=max_sl, value=vocab_to_int[\"<PAD>\"])\n",
        "  #find the number of unique tokens in the list\n",
        "  #flat_list_rt = [item for sublist in df[\"reviewText\"] for item in sublist]\n",
        "  #flat_list_s = [item for sublist in df[\"summary\"] for item in sublist]\n",
        "  #rt=len(np.unique(flat_list_rt))\n",
        "  #st=len(np.unique(flat_list_s))\n",
        "  #print(\"number of unique tokens reviewText {} and summary {}\".format(rt,st))\n",
        "  #plot the length of training, validation and testing\n",
        "  plot_tr_tval_tt_len(xtr,xval,xtt)\n",
        "  print(\"total time to complete all the above steps and get final data \",time.time()-st)\n",
        "  #free memory delete values stored in variables which are not required further\n",
        "  del df,word_counts,embeddings_index,x_tr,x_val,y_tr,y_val,x_tt,y_tt\n",
        "\n",
        "  return xtr,ytr,xtt,ytt,xval,yval,vocab_to_int,int_to_vocab,word_embedding_matrix,max_rl,max_sl  \n",
        "\n",
        "#step17\n",
        "#function to get summary given a sequence\n",
        "def seq_to_summary(seq,vocab_to_int,int_to_vocab):\n",
        "  newstring=''\n",
        "  for i in seq:\n",
        "    if ((i!=0 and i!=vocab_to_int['<GO>']) and i!=vocab_to_int['<EOS>']):\n",
        "      newstring=newstring+int_to_vocab[i]+' '\n",
        "  return newstring\n",
        "  \n",
        "#step18\n",
        "#function to get text given a sequence\n",
        "def seq_to_text(seq,int_to_vocab):\n",
        "  newstring=''\n",
        "  for i in seq:\n",
        "    if (i!=0):\n",
        "      newstring=newstring+int_to_vocab[i]+' '\n",
        "  return newstring\n",
        "\n",
        "#step19\n",
        "#this function get the data for the pretrained model t5small\n",
        "def combining_all_steps_t5():\n",
        "  #get the final cleaned data\n",
        "  df=pd.read_csv('/content/drive/MyDrive/product_reviews.csv')[:180000]\n",
        "  print(\"The length of dataset is \",len(df))\n",
        "  \n",
        "  #set the threshold \n",
        "  threshold = 20\n",
        "  max_rl=80 #maximum review length\n",
        "  max_sl=10 #maximum summary length\n",
        "  \n",
        "  #get reviewText whose length is less than maximum review length\n",
        "  df['reviewText']=df['reviewText'].str.slice(0,max_rl)\n",
        "  \n",
        "  #get summary whose length is less than maximum summary length\n",
        "  df['summary']=df['summary'].str.slice(0,max_rl)\n",
        "\n",
        "  #split data into 75% train, 15% validation and 15% test datasets\n",
        "  x_tr,x_val,y_tr,y_val=train_test_split(df['reviewText'],df['summary'],test_size=0.3,random_state=1,shuffle=True)\n",
        "  x_tt,x_val,y_tt,y_val=train_test_split(x_val,y_val,test_size=0.5,random_state=1,shuffle=True)\n",
        "\n",
        "  #reset index\n",
        "  x_tr=x_tr.reset_index()\n",
        "  y_tr=y_tr.reset_index()\n",
        "  x_tt=x_tt.reset_index()\n",
        "  y_tt=y_tt.reset_index()\n",
        "  x_val=x_val.reset_index()\n",
        "  y_val=y_val.reset_index()\n",
        "  print(\"train {}, val {}, test {}\".format(len(x_tr),len(x_val),len(x_tt)))\n",
        "  return x_tr,y_tr,x_tt,y_tt,x_val,y_val"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==2.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\r\u001b[K     |▋                               | 10kB 19.3MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 25.2MB/s eta 0:00:01\r\u001b[K     |█▊                              | 30kB 31.1MB/s eta 0:00:01\r\u001b[K     |██▎                             | 40kB 24.8MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 15.6MB/s eta 0:00:01\r\u001b[K     |███▌                            | 61kB 14.6MB/s eta 0:00:01\r\u001b[K     |████                            | 71kB 13.8MB/s eta 0:00:01\r\u001b[K     |████▋                           | 81kB 14.8MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 92kB 15.3MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 102kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 112kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████                         | 122kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 133kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 143kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 153kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 163kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 174kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 184kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 194kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 204kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 215kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 225kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 235kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 245kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 256kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 266kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 276kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 286kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 296kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 307kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 317kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 327kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 337kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 348kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 358kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 368kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 378kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 389kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 399kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 409kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 419kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 430kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 440kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 450kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 460kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 471kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 481kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 491kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 501kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 512kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 522kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 532kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 542kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 552kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 563kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 573kB 12.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (3.0.12)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/e3/5e49e9a83fb605aaa34a1c1173e607302fecae529428c28696fb18f1c2c9/tokenizers-0.5.2-cp37-cp37m-manylinux1_x86_64.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 31.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (2019.12.20)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/60/78919d8b178668aac44b5d5f4fbe660880179ada1e9000cf3ee3bfcb6421/boto3-1.17.50.tar.gz (99kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 10.6MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 45.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (2.23.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 57.5MB/s \n",
            "\u001b[?25hCollecting botocore<1.21.0,>=1.20.50\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/ae/e7e003597f954283f90f21891bda64bab0fc1738951aeb09a7c798ef0a60/botocore-1.20.50-py2.py3-none-any.whl (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 42.2MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/14/0b4be62b65c52d6d1c442f24e02d2a9889a73d3c352002e14c70f84a679f/s3transfer-0.3.6-py2.py3-none-any.whl (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (1.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.50->boto3->transformers==2.8.0) (2.8.1)\n",
            "Building wheels for collected packages: boto3, sacremoses\n",
            "  Building wheel for boto3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for boto3: filename=boto3-1.17.50-py2.py3-none-any.whl size=128779 sha256=35ef9514c3dbfc73a7bcb7154f506887375439e8cf9e6e355afe64b9916aac25\n",
            "  Stored in directory: /root/.cache/pip/wheels/28/e5/43/ef6fc36c3008477a35f9324c0e490c7aa20f7b51993a388267\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=109a8ea2b9b2f8256ec87ff5faddeb10e6f1cff6e4e51038107ad90586f57e59\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n",
            "Successfully built boto3 sacremoses\n",
            "\u001b[31mERROR: botocore 1.20.50 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tokenizers, jmespath, botocore, s3transfer, boto3, sacremoses, sentencepiece, transformers\n",
            "Successfully installed boto3-1.17.50 botocore-1.20.50 jmespath-0.10.0 s3transfer-0.3.6 sacremoses-0.0.44 sentencepiece-0.1.95 tokenizers-0.5.2 transformers-2.8.0\n",
            "Collecting torch==1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/3b/fa92ece1e58a6a48ec598bab327f39d69808133e5b2fb33002ca754e381e/torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4MB 22kB/s \n",
            "\u001b[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "Successfully installed torch-1.4.0\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euct4cIGtr4u"
      },
      "source": [
        "#summary using T5small pretrained model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quKmq4CvdKGL"
      },
      "source": [
        "#step26\n",
        "#function is used to return the loss\n",
        "def step(inputs_ids, attention_mask, y, pad_token_id, model):\n",
        "  y_ids = y[:, :-1].contiguous()\n",
        "  lm_labels = y[:, 1:].clone()\n",
        "  lm_labels[y[:, 1:] == pad_token_id] = -100\n",
        "  output = model(inputs_ids, attention_mask=attention_mask, decoder_input_ids=y_ids, lm_labels=lm_labels)\n",
        "  # loss\n",
        "  return output[0] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15XD1TxRdPN0"
      },
      "source": [
        "#step25\n",
        "#this function is used to train the pretrained t5small model\n",
        "def t5train(train_loader,val_loader,pad_token_id,model,EPOCHS,log_interval):\n",
        "  #initialize empty list for train_loss and val_loss\n",
        "  train_loss = []\n",
        "  val_loss = []\n",
        "  #optimizer\n",
        "  optimizer = torch.optim.Adam(model.parameters(),lr=1e-4, weight_decay=1e-4/25)\n",
        "  #iterate for number of epochs\n",
        "  for epoch in range(EPOCHS):\n",
        "    model.train() \n",
        "    #start time\n",
        "    start_time = time.time()\n",
        "    #for data in train_loader train the model\n",
        "    for i, (inputs_ids, attention_mask, y) in enumerate(train_loader):\n",
        "      inputs_ids = inputs_ids.to(device)\n",
        "      attention_mask = attention_mask.to(device)\n",
        "      y = y.to(device)\n",
        "            \n",
        "      optimizer.zero_grad()\n",
        "      loss = step(inputs_ids, attention_mask, y, pad_token_id, model)\n",
        "      train_loss.append(loss.item())\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "      optimizer.step()\n",
        "            \n",
        "      if (i + 1) % log_interval == 0:\n",
        "        with torch.no_grad():\n",
        "          x, x_mask, y = next(iter(val_loader))\n",
        "          x = x.to(device)\n",
        "          x_mask = x_mask.to(device)\n",
        "          y = y.to(device)\n",
        "                \n",
        "          v_loss = step(x, x_mask, y, pad_token_id, model)\n",
        "          v_loss = v_loss.item()\n",
        "                \n",
        "          elapsed = time.time() - start_time\n",
        "          print('| epoch {:3d} | [{:5d}/{:5d}] | '\n",
        "                'ms/batch {:5.2f} | '\n",
        "                'loss {:5.2f} | val loss {:5.2f}'.format(\n",
        "                  epoch, i, len(train_loader),\n",
        "                  elapsed * 1000 / log_interval,\n",
        "                  loss.item(), v_loss))\n",
        "          start_time = time.time()\n",
        "          val_loss.append(v_loss)\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYDbQrNxdYdI"
      },
      "source": [
        "#step26\n",
        "#function to test the model it writes original and predicted summary in txt file\n",
        "def testT5(model,tokenizer,test_loader):\n",
        "  #intialize the empty lists\n",
        "  predictions = []\n",
        "  real_og=[]\n",
        "  pred_op=[]\n",
        "  c=0\n",
        "  b=1000\n",
        "  #for data in test loader\n",
        "  for i, (input_ids, attention_mask, y) in enumerate(test_loader):\n",
        "    input_ids = input_ids.to(device)\n",
        "    attention_mask = attention_mask.to(device)\n",
        "    y = y.to(device)\n",
        "    #generate summaries \n",
        "    #store real and predicted summary in a list and write in txt file\n",
        "    summaries = model.generate(input_ids=input_ids, attention_mask=attention_mask,max_length=10)\n",
        "    pred = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summaries]\n",
        "    real = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in y]\n",
        "    for pred_sent, real_sent in zip(pred, real): \n",
        "      if c>b:\n",
        "        print(\"Original: {}\".format(real_sent))\n",
        "        print(\"Predicted: {}\".format(pred_sent))\n",
        "        print(\"\\n\")\n",
        "        b+=b\n",
        "      real_og.append(real_sent)\n",
        "      pred_op.append(pred_sent)\n",
        "      predictions.append(str(\"pred sentence: \" + pred_sent + \"\\t\\t real sentence: \" + real_sent+\"\\n\"))\n",
        "      c+=1\n",
        "  file1 = open(\"/content/drive/MyDrive/TFIVE.txt\",\"w\")\n",
        "  file1.writelines(predictions)\n",
        "  file1.close()\n",
        "  #calculate scores\n",
        "  bleau=compute_bleu(real_og,pred_op, max_order=4,smooth=False)\n",
        "  rougen=rouge_n(pred_op, real_og, n=2)\n",
        "  ro=rouge(pred_op, real_og)\n",
        "\n",
        "  print(\"bleu, precisions, bp, ratio, translation_length, reference_length\",bleau)\n",
        "  print(\"rouge2\",rougen)\n",
        "  print(\"rouge\",ro)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awxFEtTPbMgZ"
      },
      "source": [
        "#step27\n",
        "#fucntion to get the data and call all the functions in a squence\n",
        "def tf5token():\n",
        "  class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, articles, highlights):\n",
        "      self.x = articles\n",
        "      self.y = highlights\n",
        "  \n",
        "    def __getitem__(self,index):\n",
        "      x = tokenizer.encode_plus(model.config.prefix + str(self.x[index]), max_length=80, return_tensors=\"pt\", pad_to_max_length=True)\n",
        "      y = tokenizer.encode(str(self.y[index]), max_length=10, return_tensors=\"pt\", pad_to_max_length=True)\n",
        "      return x['input_ids'].view(-1), x['attention_mask'].view(-1), y.view(-1)\n",
        "        \n",
        "    def __len__(self):\n",
        "      return len(self.x)\n",
        "\n",
        "  #get the data\n",
        "  x_tr,y_tr,x_tt,y_tt,x_val,y_val=combining_all_steps_t5()\n",
        "  BATCH_SIZE = 128\n",
        "  SHUFFEL_SIZE = 1024\n",
        "  EPOCHS = 100\n",
        "  log_interval = 200\n",
        "  #get the pretrained model t5-small\n",
        "  tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "  model = T5ForConditionalGeneration.from_pretrained('t5-small').to(device)\n",
        "\n",
        "  task_specific_params = model.config.task_specific_params\n",
        "  if task_specific_params is not None:\n",
        "    model.config.update(task_specific_params.get(\"summarization\", {}))\n",
        "  \n",
        "  #create train,test and validation datasets\n",
        "  train_ds = MyDataset(x_tr[\"reviewText\"],y_tr[\"summary\"]) \n",
        "  val_ds = MyDataset(x_val[\"reviewText\"],y_val[\"summary\"])\n",
        "  test_ds = MyDataset(x_tt[\"reviewText\"],y_tt[\"summary\"])\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE)\n",
        "  val_loader = torch.utils.data.DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
        "  test_loader = torch.utils.data.DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
        "\n",
        "  x, x_mask, y = next(iter(val_loader))\n",
        "  print(x.shape, x_mask.shape, y.shape)\n",
        "  pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "  #call the train function\n",
        "  model=t5train(train_loader,val_loader,pad_token_id,model,EPOCHS,log_interval)\n",
        "  #call the test function\n",
        "  testT5(model,tokenizer,test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQPZ1GzZx5Ej",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e5db2e94259747fcbb6f5f71d405e187",
            "63a370c7757543c4a1a3ed88db496c25",
            "250b2590f657427d8e69873cb0b3bf39"
          ]
        },
        "outputId": "a771b9c4-6e00-4db9-ad37-b714bc5b205d"
      },
      "source": [
        "tf5token()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The length of dataset is  180000\n",
            "train 126000, val 27000, test 27000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e5db2e94259747fcbb6f5f71d405e187",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=791656.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "63a370c7757543c4a1a3ed88db496c25",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1197.0, style=ProgressStyle(description…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "250b2590f657427d8e69873cb0b3bf39",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=242065649.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "torch.Size([128, 80]) torch.Size([128, 80]) torch.Size([128, 10])\n",
            "| epoch   0 | [  199/  985] | ms/batch 247.84 | loss  4.21 | val loss  4.24\n",
            "| epoch   0 | [  399/  985] | ms/batch 246.78 | loss  4.18 | val loss  4.08\n",
            "| epoch   0 | [  599/  985] | ms/batch 247.75 | loss  4.29 | val loss  4.05\n",
            "| epoch   0 | [  799/  985] | ms/batch 249.15 | loss  3.83 | val loss  3.98\n",
            "| epoch   1 | [  199/  985] | ms/batch 250.21 | loss  3.73 | val loss  3.90\n",
            "| epoch   1 | [  399/  985] | ms/batch 248.36 | loss  3.94 | val loss  3.82\n",
            "| epoch   1 | [  599/  985] | ms/batch 248.31 | loss  3.94 | val loss  3.79\n",
            "| epoch   1 | [  799/  985] | ms/batch 247.38 | loss  3.71 | val loss  3.68\n",
            "| epoch   2 | [  199/  985] | ms/batch 248.53 | loss  3.58 | val loss  3.74\n",
            "| epoch   2 | [  399/  985] | ms/batch 246.75 | loss  3.72 | val loss  3.73\n",
            "| epoch   2 | [  599/  985] | ms/batch 246.90 | loss  3.85 | val loss  3.71\n",
            "| epoch   2 | [  799/  985] | ms/batch 246.27 | loss  3.48 | val loss  3.68\n",
            "| epoch   3 | [  199/  985] | ms/batch 246.67 | loss  3.44 | val loss  3.66\n",
            "| epoch   3 | [  399/  985] | ms/batch 246.75 | loss  3.65 | val loss  3.59\n",
            "| epoch   3 | [  599/  985] | ms/batch 247.11 | loss  3.73 | val loss  3.58\n",
            "| epoch   3 | [  799/  985] | ms/batch 246.96 | loss  3.43 | val loss  3.56\n",
            "| epoch   4 | [  199/  985] | ms/batch 246.22 | loss  3.37 | val loss  3.49\n",
            "| epoch   4 | [  399/  985] | ms/batch 246.85 | loss  3.62 | val loss  3.52\n",
            "| epoch   4 | [  599/  985] | ms/batch 246.64 | loss  3.64 | val loss  3.56\n",
            "| epoch   4 | [  799/  985] | ms/batch 248.27 | loss  3.30 | val loss  3.52\n",
            "| epoch   5 | [  199/  985] | ms/batch 247.52 | loss  3.33 | val loss  3.53\n",
            "| epoch   5 | [  399/  985] | ms/batch 245.98 | loss  3.53 | val loss  3.50\n",
            "| epoch   5 | [  599/  985] | ms/batch 246.68 | loss  3.60 | val loss  3.52\n",
            "| epoch   5 | [  799/  985] | ms/batch 246.76 | loss  3.27 | val loss  3.46\n",
            "| epoch   6 | [  199/  985] | ms/batch 246.01 | loss  3.30 | val loss  3.54\n",
            "| epoch   6 | [  399/  985] | ms/batch 247.91 | loss  3.55 | val loss  3.48\n",
            "| epoch   6 | [  599/  985] | ms/batch 248.00 | loss  3.55 | val loss  3.48\n",
            "| epoch   6 | [  799/  985] | ms/batch 247.10 | loss  3.20 | val loss  3.48\n",
            "| epoch   7 | [  199/  985] | ms/batch 247.56 | loss  3.19 | val loss  3.54\n",
            "| epoch   7 | [  399/  985] | ms/batch 247.74 | loss  3.42 | val loss  3.44\n",
            "| epoch   7 | [  599/  985] | ms/batch 246.98 | loss  3.56 | val loss  3.53\n",
            "| epoch   7 | [  799/  985] | ms/batch 246.34 | loss  3.21 | val loss  3.51\n",
            "| epoch   8 | [  199/  985] | ms/batch 246.64 | loss  3.20 | val loss  3.52\n",
            "| epoch   8 | [  399/  985] | ms/batch 247.07 | loss  3.41 | val loss  3.41\n",
            "| epoch   8 | [  599/  985] | ms/batch 249.35 | loss  3.43 | val loss  3.48\n",
            "| epoch   8 | [  799/  985] | ms/batch 247.46 | loss  3.10 | val loss  3.40\n",
            "| epoch   9 | [  199/  985] | ms/batch 246.53 | loss  3.15 | val loss  3.50\n",
            "| epoch   9 | [  399/  985] | ms/batch 246.46 | loss  3.42 | val loss  3.42\n",
            "| epoch   9 | [  599/  985] | ms/batch 246.44 | loss  3.42 | val loss  3.44\n",
            "| epoch   9 | [  799/  985] | ms/batch 247.05 | loss  3.14 | val loss  3.42\n",
            "| epoch  10 | [  199/  985] | ms/batch 246.55 | loss  3.08 | val loss  3.48\n",
            "| epoch  10 | [  399/  985] | ms/batch 246.23 | loss  3.40 | val loss  3.43\n",
            "| epoch  10 | [  599/  985] | ms/batch 246.93 | loss  3.39 | val loss  3.44\n",
            "| epoch  10 | [  799/  985] | ms/batch 250.06 | loss  3.07 | val loss  3.46\n",
            "| epoch  11 | [  199/  985] | ms/batch 246.89 | loss  3.07 | val loss  3.37\n",
            "| epoch  11 | [  399/  985] | ms/batch 247.07 | loss  3.36 | val loss  3.43\n",
            "| epoch  11 | [  599/  985] | ms/batch 246.72 | loss  3.30 | val loss  3.41\n",
            "| epoch  11 | [  799/  985] | ms/batch 246.75 | loss  2.97 | val loss  3.43\n",
            "| epoch  12 | [  199/  985] | ms/batch 249.47 | loss  2.96 | val loss  3.42\n",
            "| epoch  12 | [  399/  985] | ms/batch 250.09 | loss  3.33 | val loss  3.41\n",
            "| epoch  12 | [  599/  985] | ms/batch 247.76 | loss  3.35 | val loss  3.38\n",
            "| epoch  12 | [  799/  985] | ms/batch 248.59 | loss  2.96 | val loss  3.42\n",
            "| epoch  13 | [  199/  985] | ms/batch 247.21 | loss  2.93 | val loss  3.45\n",
            "| epoch  13 | [  399/  985] | ms/batch 247.22 | loss  3.21 | val loss  3.41\n",
            "| epoch  13 | [  599/  985] | ms/batch 247.58 | loss  3.24 | val loss  3.36\n",
            "| epoch  13 | [  799/  985] | ms/batch 247.10 | loss  2.96 | val loss  3.47\n",
            "| epoch  14 | [  199/  985] | ms/batch 246.55 | loss  2.97 | val loss  3.42\n",
            "| epoch  14 | [  399/  985] | ms/batch 246.52 | loss  3.12 | val loss  3.43\n",
            "| epoch  14 | [  599/  985] | ms/batch 246.47 | loss  3.23 | val loss  3.43\n",
            "| epoch  14 | [  799/  985] | ms/batch 247.15 | loss  2.91 | val loss  3.47\n",
            "| epoch  15 | [  199/  985] | ms/batch 246.55 | loss  2.89 | val loss  3.42\n",
            "| epoch  15 | [  399/  985] | ms/batch 246.96 | loss  3.23 | val loss  3.40\n",
            "| epoch  15 | [  599/  985] | ms/batch 247.12 | loss  3.22 | val loss  3.42\n",
            "| epoch  15 | [  799/  985] | ms/batch 247.06 | loss  2.89 | val loss  3.47\n",
            "| epoch  16 | [  199/  985] | ms/batch 248.09 | loss  2.81 | val loss  3.39\n",
            "| epoch  16 | [  399/  985] | ms/batch 247.55 | loss  3.20 | val loss  3.40\n",
            "| epoch  16 | [  599/  985] | ms/batch 246.98 | loss  3.24 | val loss  3.43\n",
            "| epoch  16 | [  799/  985] | ms/batch 246.00 | loss  2.82 | val loss  3.43\n",
            "| epoch  17 | [  199/  985] | ms/batch 249.92 | loss  2.85 | val loss  3.38\n",
            "| epoch  17 | [  399/  985] | ms/batch 248.41 | loss  3.16 | val loss  3.37\n",
            "| epoch  17 | [  599/  985] | ms/batch 247.59 | loss  3.15 | val loss  3.41\n",
            "| epoch  17 | [  799/  985] | ms/batch 248.04 | loss  2.83 | val loss  3.38\n",
            "| epoch  18 | [  199/  985] | ms/batch 247.95 | loss  2.76 | val loss  3.36\n",
            "| epoch  18 | [  399/  985] | ms/batch 248.01 | loss  3.02 | val loss  3.42\n",
            "| epoch  18 | [  599/  985] | ms/batch 250.86 | loss  3.16 | val loss  3.50\n",
            "| epoch  18 | [  799/  985] | ms/batch 247.63 | loss  2.82 | val loss  3.42\n",
            "| epoch  19 | [  199/  985] | ms/batch 248.58 | loss  2.71 | val loss  3.32\n",
            "| epoch  19 | [  399/  985] | ms/batch 246.93 | loss  3.03 | val loss  3.42\n",
            "| epoch  19 | [  599/  985] | ms/batch 244.98 | loss  3.07 | val loss  3.38\n",
            "| epoch  19 | [  799/  985] | ms/batch 245.06 | loss  2.77 | val loss  3.37\n",
            "| epoch  20 | [  199/  985] | ms/batch 244.68 | loss  2.75 | val loss  3.46\n",
            "| epoch  20 | [  399/  985] | ms/batch 245.16 | loss  3.16 | val loss  3.38\n",
            "| epoch  20 | [  599/  985] | ms/batch 245.61 | loss  3.05 | val loss  3.42\n",
            "| epoch  20 | [  799/  985] | ms/batch 245.97 | loss  2.75 | val loss  3.37\n",
            "| epoch  21 | [  199/  985] | ms/batch 245.72 | loss  2.72 | val loss  3.43\n",
            "| epoch  21 | [  399/  985] | ms/batch 244.66 | loss  2.99 | val loss  3.35\n",
            "| epoch  21 | [  599/  985] | ms/batch 245.70 | loss  3.08 | val loss  3.36\n",
            "| epoch  21 | [  799/  985] | ms/batch 245.04 | loss  2.71 | val loss  3.41\n",
            "| epoch  22 | [  199/  985] | ms/batch 247.96 | loss  2.73 | val loss  3.40\n",
            "| epoch  22 | [  399/  985] | ms/batch 245.96 | loss  3.01 | val loss  3.50\n",
            "| epoch  22 | [  599/  985] | ms/batch 244.56 | loss  3.01 | val loss  3.34\n",
            "| epoch  22 | [  799/  985] | ms/batch 245.31 | loss  2.76 | val loss  3.44\n",
            "| epoch  23 | [  199/  985] | ms/batch 246.48 | loss  2.74 | val loss  3.38\n",
            "| epoch  23 | [  399/  985] | ms/batch 248.31 | loss  3.01 | val loss  3.35\n",
            "| epoch  23 | [  599/  985] | ms/batch 246.65 | loss  2.97 | val loss  3.36\n",
            "| epoch  23 | [  799/  985] | ms/batch 245.00 | loss  2.71 | val loss  3.41\n",
            "| epoch  24 | [  199/  985] | ms/batch 244.73 | loss  2.74 | val loss  3.34\n",
            "| epoch  24 | [  399/  985] | ms/batch 244.71 | loss  2.89 | val loss  3.43\n",
            "| epoch  24 | [  599/  985] | ms/batch 244.66 | loss  3.02 | val loss  3.40\n",
            "| epoch  24 | [  799/  985] | ms/batch 244.84 | loss  2.63 | val loss  3.40\n",
            "| epoch  25 | [  199/  985] | ms/batch 244.53 | loss  2.58 | val loss  3.37\n",
            "| epoch  25 | [  399/  985] | ms/batch 244.98 | loss  2.97 | val loss  3.39\n",
            "| epoch  25 | [  599/  985] | ms/batch 244.53 | loss  2.90 | val loss  3.43\n",
            "| epoch  25 | [  799/  985] | ms/batch 244.08 | loss  2.59 | val loss  3.44\n",
            "| epoch  26 | [  199/  985] | ms/batch 244.65 | loss  2.66 | val loss  3.44\n",
            "| epoch  26 | [  399/  985] | ms/batch 244.99 | loss  2.89 | val loss  3.39\n",
            "| epoch  26 | [  599/  985] | ms/batch 243.99 | loss  2.88 | val loss  3.38\n",
            "| epoch  26 | [  799/  985] | ms/batch 244.38 | loss  2.63 | val loss  3.38\n",
            "| epoch  27 | [  199/  985] | ms/batch 243.84 | loss  2.57 | val loss  3.45\n",
            "| epoch  27 | [  399/  985] | ms/batch 244.27 | loss  2.83 | val loss  3.39\n",
            "| epoch  27 | [  599/  985] | ms/batch 245.72 | loss  2.88 | val loss  3.46\n",
            "| epoch  27 | [  799/  985] | ms/batch 245.07 | loss  2.64 | val loss  3.43\n",
            "| epoch  28 | [  199/  985] | ms/batch 245.13 | loss  2.45 | val loss  3.51\n",
            "| epoch  28 | [  399/  985] | ms/batch 246.72 | loss  2.83 | val loss  3.40\n",
            "| epoch  28 | [  599/  985] | ms/batch 244.73 | loss  2.83 | val loss  3.43\n",
            "| epoch  28 | [  799/  985] | ms/batch 245.03 | loss  2.53 | val loss  3.42\n",
            "| epoch  29 | [  199/  985] | ms/batch 244.72 | loss  2.48 | val loss  3.53\n",
            "| epoch  29 | [  399/  985] | ms/batch 244.29 | loss  2.81 | val loss  3.43\n",
            "| epoch  29 | [  599/  985] | ms/batch 244.13 | loss  2.81 | val loss  3.44\n",
            "| epoch  29 | [  799/  985] | ms/batch 243.95 | loss  2.57 | val loss  3.45\n",
            "| epoch  30 | [  199/  985] | ms/batch 244.11 | loss  2.42 | val loss  3.49\n",
            "| epoch  30 | [  399/  985] | ms/batch 244.00 | loss  2.82 | val loss  3.43\n",
            "| epoch  30 | [  599/  985] | ms/batch 243.66 | loss  2.86 | val loss  3.36\n",
            "| epoch  30 | [  799/  985] | ms/batch 243.93 | loss  2.57 | val loss  3.46\n",
            "| epoch  31 | [  199/  985] | ms/batch 244.21 | loss  2.44 | val loss  3.48\n",
            "| epoch  31 | [  399/  985] | ms/batch 243.66 | loss  2.75 | val loss  3.41\n",
            "| epoch  31 | [  599/  985] | ms/batch 244.11 | loss  2.79 | val loss  3.46\n",
            "| epoch  31 | [  799/  985] | ms/batch 243.55 | loss  2.51 | val loss  3.44\n",
            "| epoch  32 | [  199/  985] | ms/batch 243.45 | loss  2.42 | val loss  3.46\n",
            "| epoch  32 | [  399/  985] | ms/batch 243.81 | loss  2.82 | val loss  3.40\n",
            "| epoch  32 | [  599/  985] | ms/batch 243.70 | loss  2.76 | val loss  3.38\n",
            "| epoch  32 | [  799/  985] | ms/batch 243.84 | loss  2.61 | val loss  3.45\n",
            "| epoch  33 | [  199/  985] | ms/batch 244.19 | loss  2.45 | val loss  3.43\n",
            "| epoch  33 | [  399/  985] | ms/batch 243.33 | loss  2.70 | val loss  3.42\n",
            "| epoch  33 | [  599/  985] | ms/batch 242.42 | loss  2.80 | val loss  3.50\n",
            "| epoch  33 | [  799/  985] | ms/batch 243.04 | loss  2.55 | val loss  3.43\n",
            "| epoch  34 | [  199/  985] | ms/batch 242.68 | loss  2.40 | val loss  3.46\n",
            "| epoch  34 | [  399/  985] | ms/batch 242.48 | loss  2.76 | val loss  3.49\n",
            "| epoch  34 | [  599/  985] | ms/batch 242.49 | loss  2.74 | val loss  3.49\n",
            "| epoch  34 | [  799/  985] | ms/batch 242.75 | loss  2.54 | val loss  3.58\n",
            "| epoch  35 | [  199/  985] | ms/batch 243.34 | loss  2.38 | val loss  3.50\n",
            "| epoch  35 | [  399/  985] | ms/batch 242.89 | loss  2.65 | val loss  3.43\n",
            "| epoch  35 | [  599/  985] | ms/batch 242.51 | loss  2.75 | val loss  3.59\n",
            "| epoch  35 | [  799/  985] | ms/batch 242.82 | loss  2.46 | val loss  3.43\n",
            "| epoch  36 | [  199/  985] | ms/batch 242.95 | loss  2.35 | val loss  3.50\n",
            "| epoch  36 | [  399/  985] | ms/batch 242.72 | loss  2.67 | val loss  3.45\n",
            "| epoch  36 | [  599/  985] | ms/batch 243.42 | loss  2.62 | val loss  3.41\n",
            "| epoch  36 | [  799/  985] | ms/batch 242.50 | loss  2.47 | val loss  3.44\n",
            "| epoch  37 | [  199/  985] | ms/batch 242.84 | loss  2.27 | val loss  3.51\n",
            "| epoch  37 | [  399/  985] | ms/batch 242.76 | loss  2.67 | val loss  3.42\n",
            "| epoch  37 | [  599/  985] | ms/batch 243.18 | loss  2.67 | val loss  3.49\n",
            "| epoch  37 | [  799/  985] | ms/batch 242.47 | loss  2.45 | val loss  3.41\n",
            "| epoch  38 | [  199/  985] | ms/batch 242.70 | loss  2.30 | val loss  3.65\n",
            "| epoch  38 | [  399/  985] | ms/batch 243.44 | loss  2.64 | val loss  3.45\n",
            "| epoch  38 | [  599/  985] | ms/batch 243.17 | loss  2.65 | val loss  3.52\n",
            "| epoch  38 | [  799/  985] | ms/batch 242.87 | loss  2.42 | val loss  3.48\n",
            "| epoch  39 | [  199/  985] | ms/batch 243.42 | loss  2.33 | val loss  3.53\n",
            "| epoch  39 | [  399/  985] | ms/batch 243.29 | loss  2.59 | val loss  3.41\n",
            "| epoch  39 | [  599/  985] | ms/batch 243.09 | loss  2.66 | val loss  3.49\n",
            "| epoch  39 | [  799/  985] | ms/batch 243.36 | loss  2.38 | val loss  3.60\n",
            "| epoch  40 | [  199/  985] | ms/batch 243.58 | loss  2.28 | val loss  3.48\n",
            "| epoch  40 | [  399/  985] | ms/batch 243.50 | loss  2.56 | val loss  3.39\n",
            "| epoch  40 | [  599/  985] | ms/batch 244.04 | loss  2.59 | val loss  3.55\n",
            "| epoch  40 | [  799/  985] | ms/batch 243.07 | loss  2.36 | val loss  3.62\n",
            "| epoch  41 | [  199/  985] | ms/batch 243.33 | loss  2.24 | val loss  3.64\n",
            "| epoch  41 | [  399/  985] | ms/batch 243.78 | loss  2.53 | val loss  3.53\n",
            "| epoch  41 | [  599/  985] | ms/batch 243.65 | loss  2.67 | val loss  3.58\n",
            "| epoch  41 | [  799/  985] | ms/batch 243.30 | loss  2.40 | val loss  3.57\n",
            "| epoch  42 | [  199/  985] | ms/batch 243.13 | loss  2.25 | val loss  3.59\n",
            "| epoch  42 | [  399/  985] | ms/batch 243.60 | loss  2.53 | val loss  3.54\n",
            "| epoch  42 | [  599/  985] | ms/batch 243.30 | loss  2.58 | val loss  3.48\n",
            "| epoch  42 | [  799/  985] | ms/batch 244.52 | loss  2.34 | val loss  3.54\n",
            "| epoch  43 | [  199/  985] | ms/batch 244.44 | loss  2.21 | val loss  3.53\n",
            "| epoch  43 | [  399/  985] | ms/batch 246.95 | loss  2.49 | val loss  3.49\n",
            "| epoch  43 | [  599/  985] | ms/batch 245.97 | loss  2.53 | val loss  3.62\n",
            "| epoch  43 | [  799/  985] | ms/batch 245.25 | loss  2.20 | val loss  3.58\n",
            "| epoch  44 | [  199/  985] | ms/batch 244.72 | loss  2.24 | val loss  3.64\n",
            "| epoch  44 | [  399/  985] | ms/batch 244.24 | loss  2.46 | val loss  3.46\n",
            "| epoch  44 | [  599/  985] | ms/batch 245.49 | loss  2.57 | val loss  3.52\n",
            "| epoch  44 | [  799/  985] | ms/batch 247.49 | loss  2.35 | val loss  3.57\n",
            "| epoch  45 | [  199/  985] | ms/batch 247.50 | loss  2.17 | val loss  3.58\n",
            "| epoch  45 | [  399/  985] | ms/batch 245.56 | loss  2.48 | val loss  3.42\n",
            "| epoch  45 | [  599/  985] | ms/batch 244.95 | loss  2.52 | val loss  3.61\n",
            "| epoch  45 | [  799/  985] | ms/batch 245.74 | loss  2.30 | val loss  3.56\n",
            "| epoch  46 | [  199/  985] | ms/batch 246.02 | loss  2.23 | val loss  3.55\n",
            "| epoch  46 | [  399/  985] | ms/batch 245.64 | loss  2.48 | val loss  3.54\n",
            "| epoch  46 | [  599/  985] | ms/batch 246.00 | loss  2.45 | val loss  3.62\n",
            "| epoch  46 | [  799/  985] | ms/batch 246.55 | loss  2.26 | val loss  3.57\n",
            "| epoch  47 | [  199/  985] | ms/batch 247.29 | loss  2.14 | val loss  3.56\n",
            "| epoch  47 | [  399/  985] | ms/batch 246.71 | loss  2.38 | val loss  3.56\n",
            "| epoch  47 | [  599/  985] | ms/batch 247.28 | loss  2.39 | val loss  3.58\n",
            "| epoch  47 | [  799/  985] | ms/batch 247.74 | loss  2.25 | val loss  3.59\n",
            "| epoch  48 | [  199/  985] | ms/batch 247.69 | loss  2.15 | val loss  3.67\n",
            "| epoch  48 | [  399/  985] | ms/batch 247.74 | loss  2.38 | val loss  3.64\n",
            "| epoch  48 | [  599/  985] | ms/batch 246.77 | loss  2.42 | val loss  3.56\n",
            "| epoch  48 | [  799/  985] | ms/batch 247.55 | loss  2.28 | val loss  3.67\n",
            "| epoch  49 | [  199/  985] | ms/batch 247.64 | loss  2.16 | val loss  3.66\n",
            "| epoch  49 | [  399/  985] | ms/batch 248.40 | loss  2.37 | val loss  3.61\n",
            "| epoch  49 | [  599/  985] | ms/batch 248.78 | loss  2.42 | val loss  3.52\n",
            "| epoch  49 | [  799/  985] | ms/batch 248.59 | loss  2.20 | val loss  3.72\n",
            "| epoch  50 | [  199/  985] | ms/batch 248.50 | loss  2.12 | val loss  3.64\n",
            "| epoch  50 | [  399/  985] | ms/batch 248.11 | loss  2.32 | val loss  3.66\n",
            "| epoch  50 | [  599/  985] | ms/batch 247.54 | loss  2.36 | val loss  3.60\n",
            "| epoch  50 | [  799/  985] | ms/batch 247.41 | loss  2.16 | val loss  3.62\n",
            "| epoch  51 | [  199/  985] | ms/batch 245.80 | loss  2.13 | val loss  3.73\n",
            "| epoch  51 | [  399/  985] | ms/batch 245.59 | loss  2.29 | val loss  3.55\n",
            "| epoch  51 | [  599/  985] | ms/batch 246.00 | loss  2.34 | val loss  3.74\n",
            "| epoch  51 | [  799/  985] | ms/batch 245.39 | loss  2.12 | val loss  3.67\n",
            "| epoch  52 | [  199/  985] | ms/batch 246.09 | loss  2.04 | val loss  3.59\n",
            "| epoch  52 | [  399/  985] | ms/batch 245.69 | loss  2.32 | val loss  3.68\n",
            "| epoch  52 | [  599/  985] | ms/batch 246.01 | loss  2.36 | val loss  3.60\n",
            "| epoch  52 | [  799/  985] | ms/batch 245.69 | loss  2.20 | val loss  3.66\n",
            "| epoch  53 | [  199/  985] | ms/batch 245.50 | loss  2.06 | val loss  3.64\n",
            "| epoch  53 | [  399/  985] | ms/batch 245.83 | loss  2.27 | val loss  3.74\n",
            "| epoch  53 | [  599/  985] | ms/batch 245.18 | loss  2.31 | val loss  3.71\n",
            "| epoch  53 | [  799/  985] | ms/batch 245.62 | loss  2.14 | val loss  3.63\n",
            "| epoch  54 | [  199/  985] | ms/batch 245.82 | loss  1.98 | val loss  3.69\n",
            "| epoch  54 | [  399/  985] | ms/batch 244.68 | loss  2.30 | val loss  3.55\n",
            "| epoch  54 | [  599/  985] | ms/batch 244.99 | loss  2.28 | val loss  3.72\n",
            "| epoch  54 | [  799/  985] | ms/batch 245.13 | loss  2.14 | val loss  3.60\n",
            "| epoch  55 | [  199/  985] | ms/batch 245.12 | loss  2.02 | val loss  3.66\n",
            "| epoch  55 | [  399/  985] | ms/batch 245.05 | loss  2.28 | val loss  3.66\n",
            "| epoch  55 | [  599/  985] | ms/batch 244.59 | loss  2.27 | val loss  3.64\n",
            "| epoch  55 | [  799/  985] | ms/batch 245.44 | loss  2.07 | val loss  3.69\n",
            "| epoch  56 | [  199/  985] | ms/batch 245.81 | loss  1.96 | val loss  3.78\n",
            "| epoch  56 | [  399/  985] | ms/batch 246.24 | loss  2.16 | val loss  3.64\n",
            "| epoch  56 | [  599/  985] | ms/batch 245.46 | loss  2.27 | val loss  3.63\n",
            "| epoch  56 | [  799/  985] | ms/batch 245.88 | loss  2.08 | val loss  3.75\n",
            "| epoch  57 | [  199/  985] | ms/batch 245.05 | loss  1.85 | val loss  3.83\n",
            "| epoch  57 | [  399/  985] | ms/batch 245.47 | loss  2.22 | val loss  3.71\n",
            "| epoch  57 | [  599/  985] | ms/batch 248.29 | loss  2.27 | val loss  3.67\n",
            "| epoch  57 | [  799/  985] | ms/batch 248.71 | loss  2.00 | val loss  3.81\n",
            "| epoch  58 | [  199/  985] | ms/batch 246.17 | loss  1.96 | val loss  3.81\n",
            "| epoch  58 | [  399/  985] | ms/batch 246.07 | loss  2.26 | val loss  3.74\n",
            "| epoch  58 | [  599/  985] | ms/batch 246.82 | loss  2.18 | val loss  3.72\n",
            "| epoch  58 | [  799/  985] | ms/batch 246.00 | loss  1.98 | val loss  3.78\n",
            "| epoch  59 | [  199/  985] | ms/batch 246.16 | loss  1.93 | val loss  3.79\n",
            "| epoch  59 | [  399/  985] | ms/batch 245.59 | loss  2.20 | val loss  3.63\n",
            "| epoch  59 | [  599/  985] | ms/batch 245.78 | loss  2.20 | val loss  3.76\n",
            "| epoch  59 | [  799/  985] | ms/batch 246.80 | loss  2.05 | val loss  3.89\n",
            "| epoch  60 | [  199/  985] | ms/batch 247.06 | loss  1.90 | val loss  3.78\n",
            "| epoch  60 | [  399/  985] | ms/batch 247.55 | loss  2.11 | val loss  3.67\n",
            "| epoch  60 | [  599/  985] | ms/batch 246.77 | loss  2.17 | val loss  3.64\n",
            "| epoch  60 | [  799/  985] | ms/batch 247.16 | loss  1.96 | val loss  3.77\n",
            "| epoch  61 | [  199/  985] | ms/batch 247.92 | loss  1.91 | val loss  3.75\n",
            "| epoch  61 | [  399/  985] | ms/batch 247.83 | loss  2.12 | val loss  3.76\n",
            "| epoch  61 | [  599/  985] | ms/batch 247.94 | loss  2.17 | val loss  3.78\n",
            "| epoch  61 | [  799/  985] | ms/batch 247.50 | loss  1.96 | val loss  3.87\n",
            "| epoch  62 | [  199/  985] | ms/batch 246.81 | loss  1.88 | val loss  3.68\n",
            "| epoch  62 | [  399/  985] | ms/batch 247.00 | loss  2.09 | val loss  3.77\n",
            "| epoch  62 | [  599/  985] | ms/batch 246.80 | loss  2.15 | val loss  3.83\n",
            "| epoch  62 | [  799/  985] | ms/batch 247.36 | loss  1.89 | val loss  3.87\n",
            "| epoch  63 | [  199/  985] | ms/batch 246.88 | loss  1.88 | val loss  3.81\n",
            "| epoch  63 | [  399/  985] | ms/batch 247.51 | loss  2.07 | val loss  3.87\n",
            "| epoch  63 | [  599/  985] | ms/batch 246.72 | loss  2.18 | val loss  3.92\n",
            "| epoch  63 | [  799/  985] | ms/batch 247.59 | loss  1.99 | val loss  3.81\n",
            "| epoch  64 | [  199/  985] | ms/batch 247.64 | loss  1.88 | val loss  3.82\n",
            "| epoch  64 | [  399/  985] | ms/batch 248.46 | loss  2.14 | val loss  3.86\n",
            "| epoch  64 | [  599/  985] | ms/batch 248.16 | loss  2.13 | val loss  3.85\n",
            "| epoch  64 | [  799/  985] | ms/batch 248.45 | loss  1.91 | val loss  3.80\n",
            "| epoch  65 | [  199/  985] | ms/batch 247.42 | loss  1.79 | val loss  3.92\n",
            "| epoch  65 | [  399/  985] | ms/batch 247.82 | loss  2.06 | val loss  3.85\n",
            "| epoch  65 | [  599/  985] | ms/batch 247.24 | loss  2.13 | val loss  3.90\n",
            "| epoch  65 | [  799/  985] | ms/batch 248.80 | loss  1.93 | val loss  3.92\n",
            "| epoch  66 | [  199/  985] | ms/batch 248.24 | loss  1.80 | val loss  3.87\n",
            "| epoch  66 | [  399/  985] | ms/batch 248.89 | loss  2.01 | val loss  3.92\n",
            "| epoch  66 | [  599/  985] | ms/batch 248.64 | loss  2.06 | val loss  3.99\n",
            "| epoch  66 | [  799/  985] | ms/batch 249.86 | loss  2.00 | val loss  3.73\n",
            "| epoch  67 | [  199/  985] | ms/batch 248.36 | loss  1.77 | val loss  3.94\n",
            "| epoch  67 | [  399/  985] | ms/batch 248.62 | loss  2.06 | val loss  3.94\n",
            "| epoch  67 | [  599/  985] | ms/batch 249.21 | loss  2.05 | val loss  3.93\n",
            "| epoch  67 | [  799/  985] | ms/batch 248.85 | loss  1.94 | val loss  3.80\n",
            "| epoch  68 | [  199/  985] | ms/batch 247.82 | loss  1.77 | val loss  3.93\n",
            "| epoch  68 | [  399/  985] | ms/batch 247.94 | loss  2.00 | val loss  3.92\n",
            "| epoch  68 | [  599/  985] | ms/batch 248.12 | loss  2.03 | val loss  3.99\n",
            "| epoch  68 | [  799/  985] | ms/batch 248.98 | loss  1.93 | val loss  3.98\n",
            "| epoch  69 | [  199/  985] | ms/batch 248.87 | loss  1.78 | val loss  3.89\n",
            "| epoch  69 | [  399/  985] | ms/batch 248.49 | loss  1.99 | val loss  3.90\n",
            "| epoch  69 | [  599/  985] | ms/batch 249.55 | loss  2.00 | val loss  3.88\n",
            "| epoch  69 | [  799/  985] | ms/batch 247.91 | loss  1.81 | val loss  3.97\n",
            "| epoch  70 | [  199/  985] | ms/batch 246.70 | loss  1.76 | val loss  3.97\n",
            "| epoch  70 | [  399/  985] | ms/batch 247.57 | loss  2.05 | val loss  3.95\n",
            "| epoch  70 | [  599/  985] | ms/batch 247.41 | loss  2.00 | val loss  3.88\n",
            "| epoch  70 | [  799/  985] | ms/batch 247.07 | loss  1.90 | val loss  4.17\n",
            "| epoch  71 | [  199/  985] | ms/batch 248.25 | loss  1.68 | val loss  3.99\n",
            "| epoch  71 | [  399/  985] | ms/batch 249.27 | loss  1.90 | val loss  3.99\n",
            "| epoch  71 | [  599/  985] | ms/batch 249.17 | loss  1.92 | val loss  3.97\n",
            "| epoch  71 | [  799/  985] | ms/batch 248.58 | loss  1.81 | val loss  3.93\n",
            "| epoch  72 | [  199/  985] | ms/batch 248.46 | loss  1.58 | val loss  4.09\n",
            "| epoch  72 | [  399/  985] | ms/batch 248.14 | loss  1.87 | val loss  3.97\n",
            "| epoch  72 | [  599/  985] | ms/batch 248.69 | loss  1.94 | val loss  4.01\n",
            "| epoch  72 | [  799/  985] | ms/batch 248.90 | loss  1.76 | val loss  4.02\n",
            "| epoch  73 | [  199/  985] | ms/batch 248.34 | loss  1.73 | val loss  3.97\n",
            "| epoch  73 | [  399/  985] | ms/batch 247.80 | loss  1.91 | val loss  4.01\n",
            "| epoch  73 | [  599/  985] | ms/batch 248.58 | loss  2.00 | val loss  4.07\n",
            "| epoch  73 | [  799/  985] | ms/batch 248.09 | loss  1.78 | val loss  3.95\n",
            "| epoch  74 | [  199/  985] | ms/batch 248.35 | loss  1.66 | val loss  3.98\n",
            "| epoch  74 | [  399/  985] | ms/batch 248.99 | loss  1.95 | val loss  3.92\n",
            "| epoch  74 | [  599/  985] | ms/batch 247.78 | loss  1.95 | val loss  3.93\n",
            "| epoch  74 | [  799/  985] | ms/batch 247.67 | loss  1.76 | val loss  4.08\n",
            "| epoch  75 | [  199/  985] | ms/batch 249.44 | loss  1.63 | val loss  4.13\n",
            "| epoch  75 | [  399/  985] | ms/batch 248.41 | loss  1.89 | val loss  4.03\n",
            "| epoch  75 | [  599/  985] | ms/batch 248.06 | loss  1.89 | val loss  4.09\n",
            "| epoch  75 | [  799/  985] | ms/batch 248.12 | loss  1.76 | val loss  4.06\n",
            "| epoch  76 | [  199/  985] | ms/batch 248.43 | loss  1.60 | val loss  4.14\n",
            "| epoch  76 | [  399/  985] | ms/batch 248.18 | loss  1.87 | val loss  4.19\n",
            "| epoch  76 | [  599/  985] | ms/batch 248.54 | loss  1.91 | val loss  4.03\n",
            "| epoch  76 | [  799/  985] | ms/batch 248.09 | loss  1.71 | val loss  4.14\n",
            "| epoch  77 | [  199/  985] | ms/batch 247.36 | loss  1.60 | val loss  4.10\n",
            "| epoch  77 | [  399/  985] | ms/batch 248.82 | loss  1.88 | val loss  4.08\n",
            "| epoch  77 | [  599/  985] | ms/batch 247.96 | loss  1.84 | val loss  4.25\n",
            "| epoch  77 | [  799/  985] | ms/batch 248.56 | loss  1.76 | val loss  4.24\n",
            "| epoch  78 | [  199/  985] | ms/batch 252.09 | loss  1.55 | val loss  4.22\n",
            "| epoch  78 | [  399/  985] | ms/batch 248.91 | loss  1.74 | val loss  4.11\n",
            "| epoch  78 | [  599/  985] | ms/batch 248.31 | loss  1.89 | val loss  4.00\n",
            "| epoch  78 | [  799/  985] | ms/batch 248.18 | loss  1.83 | val loss  4.19\n",
            "| epoch  79 | [  199/  985] | ms/batch 251.45 | loss  1.54 | val loss  4.13\n",
            "| epoch  79 | [  399/  985] | ms/batch 250.27 | loss  1.76 | val loss  4.15\n",
            "| epoch  79 | [  599/  985] | ms/batch 250.61 | loss  1.85 | val loss  4.09\n",
            "| epoch  79 | [  799/  985] | ms/batch 251.67 | loss  1.71 | val loss  4.03\n",
            "| epoch  80 | [  199/  985] | ms/batch 250.29 | loss  1.54 | val loss  4.07\n",
            "| epoch  80 | [  399/  985] | ms/batch 250.83 | loss  1.77 | val loss  4.19\n",
            "| epoch  80 | [  599/  985] | ms/batch 251.29 | loss  1.71 | val loss  4.09\n",
            "| epoch  80 | [  799/  985] | ms/batch 248.91 | loss  1.67 | val loss  4.13\n",
            "| epoch  81 | [  199/  985] | ms/batch 249.60 | loss  1.55 | val loss  4.16\n",
            "| epoch  81 | [  399/  985] | ms/batch 248.91 | loss  1.77 | val loss  4.21\n",
            "| epoch  81 | [  599/  985] | ms/batch 249.08 | loss  1.83 | val loss  4.15\n",
            "| epoch  81 | [  799/  985] | ms/batch 248.25 | loss  1.65 | val loss  4.09\n",
            "| epoch  82 | [  199/  985] | ms/batch 248.98 | loss  1.48 | val loss  4.19\n",
            "| epoch  82 | [  399/  985] | ms/batch 248.19 | loss  1.84 | val loss  4.11\n",
            "| epoch  82 | [  599/  985] | ms/batch 248.06 | loss  1.78 | val loss  4.19\n",
            "| epoch  82 | [  799/  985] | ms/batch 248.38 | loss  1.58 | val loss  4.28\n",
            "| epoch  83 | [  199/  985] | ms/batch 247.39 | loss  1.51 | val loss  4.27\n",
            "| epoch  83 | [  399/  985] | ms/batch 246.71 | loss  1.79 | val loss  4.24\n",
            "| epoch  83 | [  599/  985] | ms/batch 247.87 | loss  1.72 | val loss  4.33\n",
            "| epoch  83 | [  799/  985] | ms/batch 246.67 | loss  1.66 | val loss  4.17\n",
            "| epoch  84 | [  199/  985] | ms/batch 246.96 | loss  1.49 | val loss  4.28\n",
            "| epoch  84 | [  399/  985] | ms/batch 247.36 | loss  1.67 | val loss  4.20\n",
            "| epoch  84 | [  599/  985] | ms/batch 246.73 | loss  1.77 | val loss  4.11\n",
            "| epoch  84 | [  799/  985] | ms/batch 246.88 | loss  1.61 | val loss  4.29\n",
            "| epoch  85 | [  199/  985] | ms/batch 247.70 | loss  1.44 | val loss  4.14\n",
            "| epoch  85 | [  399/  985] | ms/batch 247.37 | loss  1.68 | val loss  4.26\n",
            "| epoch  85 | [  599/  985] | ms/batch 248.25 | loss  1.64 | val loss  4.19\n",
            "| epoch  85 | [  799/  985] | ms/batch 249.01 | loss  1.56 | val loss  4.23\n",
            "| epoch  86 | [  199/  985] | ms/batch 248.91 | loss  1.51 | val loss  4.32\n",
            "| epoch  86 | [  399/  985] | ms/batch 248.31 | loss  1.65 | val loss  4.31\n",
            "| epoch  86 | [  599/  985] | ms/batch 249.76 | loss  1.69 | val loss  4.34\n",
            "| epoch  86 | [  799/  985] | ms/batch 248.97 | loss  1.57 | val loss  4.29\n",
            "| epoch  87 | [  199/  985] | ms/batch 248.57 | loss  1.51 | val loss  4.26\n",
            "| epoch  87 | [  399/  985] | ms/batch 249.17 | loss  1.62 | val loss  4.33\n",
            "| epoch  87 | [  599/  985] | ms/batch 246.95 | loss  1.65 | val loss  4.35\n",
            "| epoch  87 | [  799/  985] | ms/batch 246.70 | loss  1.54 | val loss  4.23\n",
            "| epoch  88 | [  199/  985] | ms/batch 247.84 | loss  1.46 | val loss  4.40\n",
            "| epoch  88 | [  399/  985] | ms/batch 248.33 | loss  1.70 | val loss  4.26\n",
            "| epoch  88 | [  599/  985] | ms/batch 248.31 | loss  1.63 | val loss  4.32\n",
            "| epoch  88 | [  799/  985] | ms/batch 248.33 | loss  1.64 | val loss  4.36\n",
            "| epoch  89 | [  199/  985] | ms/batch 248.66 | loss  1.43 | val loss  4.40\n",
            "| epoch  89 | [  399/  985] | ms/batch 248.12 | loss  1.55 | val loss  4.28\n",
            "| epoch  89 | [  599/  985] | ms/batch 245.73 | loss  1.70 | val loss  4.26\n",
            "| epoch  89 | [  799/  985] | ms/batch 246.46 | loss  1.53 | val loss  4.32\n",
            "| epoch  90 | [  199/  985] | ms/batch 246.01 | loss  1.34 | val loss  4.34\n",
            "| epoch  90 | [  399/  985] | ms/batch 243.69 | loss  1.60 | val loss  4.31\n",
            "| epoch  90 | [  599/  985] | ms/batch 244.97 | loss  1.64 | val loss  4.34\n",
            "| epoch  90 | [  799/  985] | ms/batch 242.83 | loss  1.48 | val loss  4.42\n",
            "| epoch  91 | [  199/  985] | ms/batch 242.54 | loss  1.43 | val loss  4.22\n",
            "| epoch  91 | [  399/  985] | ms/batch 243.09 | loss  1.65 | val loss  4.22\n",
            "| epoch  91 | [  599/  985] | ms/batch 243.47 | loss  1.74 | val loss  4.17\n",
            "| epoch  91 | [  799/  985] | ms/batch 243.63 | loss  1.50 | val loss  4.34\n",
            "| epoch  92 | [  199/  985] | ms/batch 244.74 | loss  1.41 | val loss  4.51\n",
            "| epoch  92 | [  399/  985] | ms/batch 244.68 | loss  1.49 | val loss  4.34\n",
            "| epoch  92 | [  599/  985] | ms/batch 244.42 | loss  1.64 | val loss  4.52\n",
            "| epoch  92 | [  799/  985] | ms/batch 244.45 | loss  1.57 | val loss  4.40\n",
            "| epoch  93 | [  199/  985] | ms/batch 243.07 | loss  1.31 | val loss  4.26\n",
            "| epoch  93 | [  399/  985] | ms/batch 242.90 | loss  1.59 | val loss  4.29\n",
            "| epoch  93 | [  599/  985] | ms/batch 243.91 | loss  1.59 | val loss  4.49\n",
            "| epoch  93 | [  799/  985] | ms/batch 242.87 | loss  1.50 | val loss  4.36\n",
            "| epoch  94 | [  199/  985] | ms/batch 243.11 | loss  1.31 | val loss  4.56\n",
            "| epoch  94 | [  399/  985] | ms/batch 244.01 | loss  1.44 | val loss  4.48\n",
            "| epoch  94 | [  599/  985] | ms/batch 243.27 | loss  1.56 | val loss  4.37\n",
            "| epoch  94 | [  799/  985] | ms/batch 243.22 | loss  1.46 | val loss  4.44\n",
            "| epoch  95 | [  199/  985] | ms/batch 244.01 | loss  1.30 | val loss  4.34\n",
            "| epoch  95 | [  399/  985] | ms/batch 243.47 | loss  1.62 | val loss  4.23\n",
            "| epoch  95 | [  599/  985] | ms/batch 243.21 | loss  1.65 | val loss  4.54\n",
            "| epoch  95 | [  799/  985] | ms/batch 242.98 | loss  1.46 | val loss  4.53\n",
            "| epoch  96 | [  199/  985] | ms/batch 243.32 | loss  1.30 | val loss  4.55\n",
            "| epoch  96 | [  399/  985] | ms/batch 242.68 | loss  1.46 | val loss  4.43\n",
            "| epoch  96 | [  599/  985] | ms/batch 243.85 | loss  1.43 | val loss  4.34\n",
            "| epoch  96 | [  799/  985] | ms/batch 245.39 | loss  1.46 | val loss  4.55\n",
            "| epoch  97 | [  199/  985] | ms/batch 244.68 | loss  1.24 | val loss  4.49\n",
            "| epoch  97 | [  399/  985] | ms/batch 244.74 | loss  1.46 | val loss  4.36\n",
            "| epoch  97 | [  599/  985] | ms/batch 245.59 | loss  1.63 | val loss  4.46\n",
            "| epoch  97 | [  799/  985] | ms/batch 245.00 | loss  1.38 | val loss  4.61\n",
            "| epoch  98 | [  199/  985] | ms/batch 245.03 | loss  1.32 | val loss  4.57\n",
            "| epoch  98 | [  399/  985] | ms/batch 245.50 | loss  1.63 | val loss  4.53\n",
            "| epoch  98 | [  599/  985] | ms/batch 242.46 | loss  1.58 | val loss  4.28\n",
            "| epoch  98 | [  799/  985] | ms/batch 243.23 | loss  1.38 | val loss  4.54\n",
            "| epoch  99 | [  199/  985] | ms/batch 245.77 | loss  1.34 | val loss  4.62\n",
            "| epoch  99 | [  399/  985] | ms/batch 243.09 | loss  1.53 | val loss  4.49\n",
            "| epoch  99 | [  599/  985] | ms/batch 242.85 | loss  1.58 | val loss  4.81\n",
            "| epoch  99 | [  799/  985] | ms/batch 244.12 | loss  1.39 | val loss  4.49\n",
            "Original: love this shirt\n",
            "Predicted: s cotton t shirt rugged long \n",
            "\n",
            "\n",
            "Original: daughter loves it\n",
            "Predicted: capezio women s socks are special\n",
            "\n",
            "\n",
            "Original: i bought for my daughter\n",
            "Predicted: linguri linguri linguri is true to\n",
            "\n",
            "\n",
            "Original: very nice sleeveless shirts\n",
            "Predicted: s sexy and quality good\n",
            "\n",
            "\n",
            "Original: such a cool belt\n",
            "Predicted: a little bit cheap looking but it has\n",
            "\n",
            "\n",
            "bleu, precisions, bp, ratio, translation_length, reference_length (0.0, [0.22711127204147336, 0.0, 0.0, 0.0], 1.0, 33.85662962962963, 914129, 27000)\n",
            "rouge2 (0.1780322698616376, 0.172686230248307, 0.18371990140934083)\n",
            "rouge {'rouge_1/f_score': 0.05149547597277218, 'rouge_1/r_score': 0.07128905055849499, 'rouge_1/p_score': 0.045626734273956485, 'rouge_2/f_score': 0.005826158524531902, 'rouge_2/r_score': 0.00920049970605526, 'rouge_2/p_score': 0.00494594356261023, 'rouge_l/f_score': 0.0401753149587903, 'rouge_l/r_score': 0.06868816872427982, 'rouge_l/p_score': 0.03976701940035273}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0Yc0HUt-zQK"
      },
      "source": [
        "#get the final cleaned data\n",
        "df=pd.read_csv('/content/drive/MyDrive/product_reviews.csv')[:117799]\n",
        "print(\"The length of dataset is \",len(df))\n",
        "  \n",
        "#set the threshold \n",
        "threshold = 20\n",
        "max_rl=80 #maximum review length\n",
        "max_sl=10 #maximum summary length\n",
        "  \n",
        "#get reviewText whose length is less than maximum review length\n",
        "df['reviewText']=df['reviewText'].str.slice(0,max_rl)\n",
        "  \n",
        "#get summary whose length is less than maximum summary length\n",
        "df['summary']=df['summary'].str.slice(0,max_rl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdyLxZeSd0Pc"
      },
      "source": [
        "'''\n",
        "f = open(\"/content/drive/MyDrive/TFIVE.txt\", \"r\")\n",
        "text=f.readlines()\n",
        "text=pd.DataFrame(text,columns=[\"value\"])\n",
        "text=text[\"value\"].str.split(\"\\t\",expand=True)\n",
        "text.columns=[\"predicted\",\"value\",\"original\"]\n",
        "text.drop(columns=[\"value\"],inplace=True)\n",
        "text[\"predicted\"]=text[\"predicted\"].str.split(\":\").str[1]\n",
        "text[\"original\"]=text[\"original\"].str.split(\":\").str[1]\n",
        "text[\"original\"]=text[\"original\"].replace('\\n','', regex=True)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xxGTrOobUY0"
      },
      "source": [
        "df[df[\"summary\"]=='best birthday gift ever']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Awltwub5pjsq"
      },
      "source": [
        "df[\"reviewText\"][21619]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQyLzrwxptGE"
      },
      "source": [
        "df[\"reviewText\"][86599]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-sqiznJqYQD"
      },
      "source": [
        "df[\"original\"][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DniBOgWfqepc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}